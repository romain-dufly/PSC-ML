{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728c3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time \n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "\n",
    "try:\n",
    "    device = torch.device('cuda')\n",
    "except:\n",
    "    print('Cuda not available')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b1660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "\n",
    "def generate_data(tokenizer, n_var, batch_size=100):\n",
    "    \n",
    "    batch = []\n",
    "    labels = []\n",
    "    clause_order = []\n",
    "    for _ in range(batch_size):\n",
    "        values = np.random.randint(0, 2, (n_var,))\n",
    "        var_idx = tuple(np.random.permutation(len(all_vars)))\n",
    "        vars = [all_vars[i] for i in var_idx]\n",
    "\n",
    "        # generate first sentence\n",
    "        clauses = []\n",
    "        clauses.append('val %d = %s ,' % (values[0], vars[0]))\n",
    "\n",
    "        for i in range(1, n_var):\n",
    "            modifier = 'val' if values[i] == values[i-1] else 'not'\n",
    "            clauses.append('%s %s = %s ,' % (modifier, vars[i-1], vars[i]))\n",
    "            \n",
    "\n",
    "        sent = ''\n",
    "        label = []\n",
    "        \n",
    "        clause_idx = tuple(range(n_var))\n",
    "        sent += ''.join([clauses[idx] for idx in clause_idx])\n",
    "        label += [values[idx] for idx in clause_idx]\n",
    "        \n",
    "        \n",
    "        order = torch.zeros(1, n_var, n_var)\n",
    "        for i in range(n_var):\n",
    "            order[0, i, clause_idx[i]] = 1\n",
    "            \n",
    "        batch.append(tokenizer(sent, return_tensors='pt')['input_ids'])\n",
    "        labels.append(values)\n",
    "        clause_order.append(order)\n",
    "    return torch.cat(batch), torch.LongTensor(labels), torch.cat(clause_order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_lego_datasets(tokenizer, n_var, n_train, n_test, batch_size):\n",
    "    \n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_order = []\n",
    "\n",
    "    for i in range(n_train//100):\n",
    "        batch, labels, order = generate_data(tokenizer, n_var, 100)\n",
    "        train_data.append(batch)\n",
    "        train_labels.append(labels)\n",
    "        train_order.append(order)\n",
    "\n",
    "    x_train = torch.cat(train_data)\n",
    "    y_train = torch.cat(train_labels)\n",
    "    order_train = torch.cat(train_order)\n",
    "    \n",
    "    trainset = torch.utils.data.TensorDataset(x_train, y_train, order_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    test_order = []\n",
    "    for i in range(n_test//100):\n",
    "        batch, labels, order = generate_data(tokenizer, n_var, 100)\n",
    "        test_data.append(batch)\n",
    "        test_labels.append(labels)\n",
    "        test_order.append(order)\n",
    "\n",
    "    x_test = torch.cat(test_data)\n",
    "    y_test = torch.cat(test_labels)\n",
    "    order_test = torch.cat(test_order)\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(x_test, y_test, order_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee34621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3543/1919911213.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.cat(batch), torch.LongTensor(labels), torch.cat(clause_order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 0 = c,not c = w,\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Used variables in the LEGO chains\n",
    "all_vars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    \n",
    "# Seed everything for reproducibility\n",
    "seed_everything(0)\n",
    "\n",
    "# n_var: total number of variables in a chain\n",
    "# n_train_var: number of variables to provide supervision during training\n",
    "n_var, n_train_var = 2, 2\n",
    "\n",
    "# n_train: total number of training sequences\n",
    "# n_test: total number of test sequences\n",
    "n_train, n_test = n_var*10000, n_var*1000\n",
    "\n",
    "# batch size >= 500 is recommended\n",
    "batch_size = 50\n",
    "\n",
    "# Specify tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate LEGO data loaders\n",
    "trainloader, testloader = make_lego_datasets(tokenizer, n_var, n_train, n_test, batch_size)\n",
    "\n",
    "# Examine an example LEGO sequence\n",
    "seq, label, _ = trainloader.dataset[0]\n",
    "print(tokenizer.decode(seq))\n",
    "print(list(label.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142bbce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2100,  657,  796,  269,  837, 1662,  269,  796,  266,  837])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8764a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val', 'Ġ0', 'Ġ=', 'Ġc', ',', 'not', 'Ġc', 'Ġ=', 'Ġw', ',', 'Ġ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"val 0 = c,not c = w, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff5483d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'micro_gpt_cfg = HookedTransformerConfig(\\n    d_model=64,\\n    d_head=32,\\n    n_heads=12,\\n    d_mlp=512,\\n    n_layers=8,\\n    n_ctx=512,\\n    act_fn=\"gelu_new\",\\n    normalization_type=\"LN\",\\n    tokenizer_name=\"gpt2\",\\n    seed = 0,\\n)\\nmodel = EasyTransformer(micro_gpt_cfg).to(\\'cuda\\') # random smallish model\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a classification layer to predict whether the next variable is 0 or 1\n",
    "\n",
    "L_hidden_state = [0]\n",
    "last_hidden_state = lambda name: (name == 'ln_final.hook_normalized')\n",
    "\n",
    "def add_list(tensor, hook):\n",
    "    L_hidden_state[0] = tensor\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, base, d_model, tgt_vocab=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(d_model, tgt_vocab)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.base.run_with_hooks(x, fwd_hooks = [(last_hidden_state, add_list)])\n",
    "        out = self.classifier(L_hidden_state[0])\n",
    "        return out\n",
    "\n",
    "# Define the model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\"\"\"micro_gpt_cfg = HookedTransformerConfig(\n",
    "    d_model=64,\n",
    "    d_head=32,\n",
    "    n_heads=12,\n",
    "    d_mlp=512,\n",
    "    n_layers=8,\n",
    "    n_ctx=512,\n",
    "    act_fn=\"gelu_new\",\n",
    "    normalization_type=\"LN\",\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    seed = 0,\n",
    ")\n",
    "model = EasyTransformer(micro_gpt_cfg).to('cuda') # random smallish model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6acff828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('good_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4189c796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('good_model.pkl', 'wb') as file:\\n    pickle.dump(model, file)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open('good_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce26cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test functions for the LEGO task\n",
    "train_var_pred = [i for i in range(2*n_train_var)] \n",
    "test_var_pred = [i for i in range(2*n_var)]\n",
    "\n",
    "def train(print_acc=False):\n",
    "    global l_train_acc, l_train_loss\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, labels, order in trainloader:\n",
    "    \n",
    "        x = batch.cuda()\n",
    "        y = labels.cuda()\n",
    "        inv_order = order.permute(0, 2, 1).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #pred = torch.argmax(model(x), -1, keepdim = True)\n",
    "        #pred = torch.reshape(pred, (pred.shape[0], pred.shape[1], 1))\n",
    "        pred = model(x)\n",
    "        ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "\n",
    "        loss = 0\n",
    "        for idx in range(n_train_var):\n",
    "            loss += criterion(ordered_pred[:, idx], y[:, idx].float()) / len(train_var_pred)\n",
    "            loss += criterion(ordered_pred[:, idx + n_train_var], y[:, idx + n_train_var].float()) / len(train_var_pred)\n",
    "            \n",
    "            total_loss += loss.item() / len(train_var_pred)\n",
    "\n",
    "            correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "            correct[idx + n_train_var] += ((ordered_pred[:, idx + n_train_var]>0).long() == y[:, idx + n_train_var]).float().mean().item()\n",
    "        \n",
    "        total += 1\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc = [corr/total for corr in correct]\n",
    "\n",
    "    l_train_loss.append(total_loss / total)\n",
    "    l_train_acc.append(list(train_acc))\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    global l_test_acc, l_test_loss\n",
    "\n",
    "    test_acc = []\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, labels, order in testloader:\n",
    "    \n",
    "            x = batch.cuda()\n",
    "            y = labels.cuda()\n",
    "            inv_order = order.permute(0, 2, 1).cuda()\n",
    "            \n",
    "            #pred = torch.argmax(model(x), -1, keepdim = True)\n",
    "            #pred = torch.reshape(pred, (pred.shape[0], pred.shape[1], 1))\n",
    "            pred = model(x)\n",
    "            ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "            \n",
    "            for idx in test_var_pred:\n",
    "                loss = criterion(ordered_pred[:,idx], y[:, idx].float())\n",
    "                total_loss += loss.item() / len(test_var_pred)\n",
    "                correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "                          \n",
    "            total += 1\n",
    "        \n",
    "        test_acc = [corr/total for corr in correct]\n",
    "\n",
    "        l_test_loss.append(total_loss / total)\n",
    "        l_test_acc.append(list(test_acc))\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f960da0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier._parameters['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78c3bbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (base): HookedTransformer(\n",
      "    (embed): Embed()\n",
      "    (hook_embed): HookPoint()\n",
      "    (blocks): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "    )\n",
      "    (ln_final): LayerNormPre(\n",
      "      (hook_scale): HookPoint()\n",
      "      (hook_normalized): HookPoint()\n",
      "    )\n",
      "    (unembed): Unembed()\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "70464129\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2612a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation at hook hook_embed has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.0.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_rot_q has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_rot_k has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 8, 50, 50])\n",
      "Activation at hook blocks.0.attn.hook_pattern has shape:\n",
      "torch.Size([4, 8, 50, 50])\n",
      "Activation at hook blocks.0.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.0.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 2048])\n",
      "Activation at hook blocks.0.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 2048])\n",
      "Activation at hook blocks.0.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook ln_final.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook ln_final.hook_normalized has shape:\n",
      "torch.Size([4, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# Print activation shapes at every layer for our model\n",
    "\n",
    "embed_or_first_layer = lambda name: (name[:6] != \"blocks\" or name[:8] == \"blocks.0\")\n",
    "\n",
    "def print_shape(tensor, hook):\n",
    "    print(f\"Activation at hook {hook.name} has shape:\")\n",
    "    print(tensor.shape)\n",
    "\n",
    "random_tokens = torch.randint(1000, 10000, (4, 50))\n",
    "logits = model.base.run_with_hooks(random_tokens, fwd_hooks=[(embed_or_first_layer, print_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d81773ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# To save training information\n",
    "l_test_acc = []\n",
    "l_test_loss = []\n",
    "l_train_acc = []\n",
    "l_train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b50f44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('good_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e6d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "allact = dict()\n",
    "allparams = lambda name: True\n",
    "torch.cuda.empty_cache()\n",
    " \n",
    "def init(tensor, hook):\n",
    "    allact.update({hook.name:[]})\n",
    "    \n",
    "def save_act(tensor, hook):\n",
    "    sector = hook.name\n",
    "    allact.update({sector:[tensor]+allact[sector]})\n",
    "\n",
    "trigger = trainloader.dataset[0][0]\n",
    "logits = model.base.run_with_hooks(trigger, fwd_hooks=[(allparams, init)])\n",
    "\n",
    "for i in range(len(trigger)) :\n",
    "    trigger = trainloader.dataset[i][0]\n",
    "    logits = model.base.run_with_hooks(trigger, fwd_hooks=[(allparams, save_act)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dca94da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.0.hook_resid_pre\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.0.ln1.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.0.ln1.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.0.attn.hook_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.0.attn.hook_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.0.attn.hook_v\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.0.attn.hook_rot_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.0.attn.hook_rot_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.0.attn.hook_attn_scores\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.0.attn.hook_pattern\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.0.attn.hook_z\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.0.hook_attn_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.0.ln2.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.0.ln2.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.0.mlp.hook_pre\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.0.mlp.hook_post\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.0.hook_mlp_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.0.hook_resid_post\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.1.hook_resid_pre\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.1.ln1.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.1.ln1.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.1.attn.hook_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.1.attn.hook_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.1.attn.hook_v\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.1.attn.hook_rot_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.1.attn.hook_rot_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.1.attn.hook_attn_scores\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.1.attn.hook_pattern\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.1.attn.hook_z\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.1.hook_attn_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.1.ln2.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.1.ln2.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.1.mlp.hook_pre\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.1.mlp.hook_post\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.1.hook_mlp_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.1.hook_resid_post\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.2.hook_resid_pre\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.2.ln1.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.2.ln1.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.2.attn.hook_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.2.attn.hook_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.2.attn.hook_v\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.2.attn.hook_rot_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.2.attn.hook_rot_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.2.attn.hook_attn_scores\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.2.attn.hook_pattern\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.2.attn.hook_z\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.2.hook_attn_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.2.ln2.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.2.ln2.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.2.mlp.hook_pre\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.2.mlp.hook_post\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.2.hook_mlp_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.2.hook_resid_post\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.3.hook_resid_pre\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.3.ln1.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.3.ln1.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.3.attn.hook_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.3.attn.hook_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.3.attn.hook_v\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.3.attn.hook_rot_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.3.attn.hook_rot_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.3.attn.hook_attn_scores\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.3.attn.hook_pattern\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.3.attn.hook_z\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.3.hook_attn_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.3.ln2.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.3.ln2.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.3.mlp.hook_pre\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.3.mlp.hook_post\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.3.hook_mlp_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.3.hook_resid_post\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.4.hook_resid_pre\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.4.ln1.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.4.ln1.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.4.attn.hook_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.4.attn.hook_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.4.attn.hook_v\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.4.attn.hook_rot_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.4.attn.hook_rot_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.4.attn.hook_attn_scores\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.4.attn.hook_pattern\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.4.attn.hook_z\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.4.hook_attn_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.4.ln2.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.4.ln2.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.4.mlp.hook_pre\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.4.mlp.hook_post\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.4.hook_mlp_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.4.hook_resid_post\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.5.hook_resid_pre\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.5.ln1.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.5.ln1.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.5.attn.hook_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.5.attn.hook_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.5.attn.hook_v\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.5.attn.hook_rot_q\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.5.attn.hook_rot_k\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.5.attn.hook_attn_scores\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.5.attn.hook_pattern\n",
      "torch.Size([1, 8, 10, 10]) \n",
      "\n",
      "blocks.5.attn.hook_z\n",
      "torch.Size([1, 10, 8, 64]) \n",
      "\n",
      "blocks.5.hook_attn_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.5.ln2.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "blocks.5.ln2.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.5.mlp.hook_pre\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.5.mlp.hook_post\n",
      "torch.Size([1, 10, 2048]) \n",
      "\n",
      "blocks.5.hook_mlp_out\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "blocks.5.hook_resid_post\n",
      "torch.Size([1, 10, 512]) \n",
      "\n",
      "ln_final.hook_scale\n",
      "torch.Size([1, 10, 1]) \n",
      "\n",
      "ln_final.hook_normalized\n",
      "torch.Size([1, 10, 512]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, _ in allact.items():\n",
    "    print(key)\n",
    "    print(allact[key][0].shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b63e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "allavg = dict()\n",
    "\n",
    "for key, tensor_list in allact.items() :\n",
    "    allavg.update({key: torch.mean(torch.cat(tensor_list, dim=0), dim=0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ababd364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allavg['blocks.5.mlp.hook_pre'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4be4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = allavg['blocks.5.mlp.hook_pre'][-1, :].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "808651a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k.reshape((1,2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0619ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(np.transpose(k), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "082253cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.linalg.inv(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3653dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 1 = a,not a = z, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "\n",
    "def choose_hook(name):\n",
    "    return name == 'blocks.5.mlp.hook_post'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "model.base.run_with_hooks(tok, fwd_hooks=[(choose_hook, save_act)])\n",
    "\n",
    "k_star = L[0][0, -1, :].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ffd6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 1 = a,val a = z, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "\n",
    "def choose_hook(name):\n",
    "    return name == 'blocks.5.hook_mlp_out'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "model.base.run_with_hooks(tok, fwd_hooks=[(choose_hook, save_act)])\n",
    "\n",
    "v_star = L[0][0, -1, :].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b5c7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.base.state_dict()['blocks.5.mlp.W_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7fef6042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "427b76ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "641c3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = (v_star - torch.matmul(k_star, W)) / torch.matmul(torch.transpose(torch.matmul(torch.tensor(C).cuda(), k_star), 0, 0), k_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a51a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = W + torch.matmul(torch.transpose(torch.matmul(k_star, torch.tensor(C).cuda()), 0, 0).reshape((2048, 1)), Lambda.reshape((1, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aa242536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0239, -0.0359,  0.0345,  ...,  0.0083,  0.0227,  0.0336],\n",
       "        [-0.0451, -0.0291, -0.0214,  ..., -0.0451,  0.0332,  0.0130],\n",
       "        [-0.0400,  0.0225, -0.0576,  ..., -0.0649,  0.0366, -0.0211],\n",
       "        ...,\n",
       "        [ 0.0257, -0.0151,  0.0507,  ...,  0.0413, -0.0217, -0.0025],\n",
       "        [-0.0040, -0.0194,  0.0203,  ...,  0.0399,  0.0034, -0.0176],\n",
       "        [ 0.0127, -0.0230, -0.0190,  ...,  0.0282,  0.0298,  0.0280]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f24be15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.6707e-03,  1.6967e-03, -4.1231e-03,  ..., -9.9503e-03,\n",
       "         -6.6779e-03, -3.1188e-03],\n",
       "        [ 2.1572e-04,  4.2213e-05, -1.0258e-04,  ..., -2.4756e-04,\n",
       "         -1.6614e-04, -7.7594e-05],\n",
       "        [ 5.1670e-05,  1.0110e-05, -2.4568e-05,  ..., -5.9292e-05,\n",
       "         -3.9794e-05, -1.8585e-05],\n",
       "        ...,\n",
       "        [ 7.7998e-05,  1.5263e-05, -3.7089e-05,  ..., -8.9508e-05,\n",
       "         -6.0072e-05, -2.8056e-05],\n",
       "        [-2.5378e-04, -4.9660e-05,  1.2068e-04,  ...,  2.9123e-04,\n",
       "          1.9545e-04,  9.1285e-05],\n",
       "        [-1.1534e-04, -2.2572e-05,  5.4847e-05,  ...,  1.3237e-04,\n",
       "          8.8833e-05,  4.1489e-05]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W - W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6727c800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.7303, device='cuda:0', grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(W - W_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "45c25965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0326, -0.0342,  0.0304,  ..., -0.0017,  0.0160,  0.0305],\n",
       "        [-0.0449, -0.0290, -0.0215,  ..., -0.0454,  0.0330,  0.0129],\n",
       "        [-0.0399,  0.0226, -0.0576,  ..., -0.0649,  0.0366, -0.0211],\n",
       "        ...,\n",
       "        [ 0.0257, -0.0151,  0.0507,  ...,  0.0412, -0.0218, -0.0025],\n",
       "        [-0.0042, -0.0194,  0.0204,  ...,  0.0402,  0.0036, -0.0175],\n",
       "        [ 0.0126, -0.0230, -0.0189,  ...,  0.0284,  0.0299,  0.0281]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c150376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(array, lim):\n",
    "    L = []\n",
    "    for i, x in enumerate(array):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            l = find_max(x, lim)\n",
    "            l = [[i] + y for y in l]\n",
    "            if l != []:\n",
    "                L += l\n",
    "        else:\n",
    "            if abs(x) > lim:\n",
    "                L.append([i])\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "72582e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_max((W - W_hat).cpu().detach().numpy(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bae9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base.state_dict()['blocks.5.mlp.W_out'] += 2*(W_hat - W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58e1dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0153, -0.0376,  0.0386,  ...,  0.0182,  0.0294,  0.0368],\n",
       "        [-0.0453, -0.0291, -0.0213,  ..., -0.0449,  0.0334,  0.0130],\n",
       "        [-0.0400,  0.0225, -0.0575,  ..., -0.0648,  0.0367, -0.0210],\n",
       "        ...,\n",
       "        [ 0.0256, -0.0151,  0.0508,  ...,  0.0414, -0.0216, -0.0025],\n",
       "        [-0.0037, -0.0193,  0.0202,  ...,  0.0396,  0.0032, -0.0176],\n",
       "        [ 0.0128, -0.0230, -0.0190,  ...,  0.0281,  0.0298,  0.0280]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base.state_dict()['blocks.5.mlp.W_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0fc20b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[-7.0071],\n",
      "         [ 0.6680]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 0 = e,not e = k, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2732f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[8.0914],\n",
      "         [4.3208]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a,not a = b, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4a2a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[-11.9871],\n",
      "         [-10.3129],\n",
      "         [ -5.2165]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 0 = a, not a = b, val b = c, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98a2c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[-11.9871],\n",
      "         [  7.6134],\n",
      "         [ 11.9640]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 0 = a,not a = b,val b = c, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])\n",
    "#val 0 = c,not c = w,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
