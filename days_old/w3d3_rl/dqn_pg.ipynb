{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/mlab/days/w3d3/rl_env/lib/python3.9/site-packages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 13.0\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "record = True\n",
    "video_name = \"test_0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "if record:\n",
    "    video_recorder = gym.wrappers.monitoring.video_recorder.VideoRecorder(env,         \n",
    "        path=f\"videos/{video_name}.mp4\"\n",
    "    )\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "states = 0\n",
    "while not done:\n",
    "    states += 1\n",
    "    if record:\n",
    "        video_recorder.capture_frame()\n",
    "    else:  \n",
    "        show_state(env)\n",
    "    state, reward, done, _ = env.step(env.action_space.sample()) # Take a random action\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"total reward: {total_reward}\")\n",
    "if record:\n",
    "    video_recorder.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward: 40.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "env_name = \"CartPole-v1\"\n",
    "record = False\n",
    "video_name = \"test2\"\n",
    "\n",
    "env_to_wrap = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env_to_wrap, './gooder_test', force = True)\n",
    "state = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    #plt.title(\"%s | Step: %d %s\" % (env._spec.id,step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "states = 0\n",
    "while not done:\n",
    "    states += 1\n",
    "    #show_state(env)\n",
    "    state, reward, done, _ = env.step(env.action_space.sample())\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"total reward: {total_reward}\")\n",
    "\n",
    "\n",
    "env.close()\n",
    "env_to_wrap.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import einops\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exp = namedtuple(\"Exp\", [\"state_0\", \"action\", \"reward\", \"state_1\", \"done\"])\n",
    "\n",
    "class ReplayBuffer():\n",
    "    \n",
    "    def __init__(self, max_size=10_000):\n",
    "        self.max_size = max_size\n",
    "        self.memory = []\n",
    "        \n",
    "    def add_exp(self, exp):\n",
    "        self.memory.append(exp)\n",
    "        if len(self.memory) >= self.max_size:\n",
    "            self.memory.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDQNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, hidden_size, out_size, dueling=True, dueling_hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.out_size = out_size\n",
    "        self.layer1 = nn.Linear(in_size, hidden_size).to(DEVICE)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size).to(DEVICE)\n",
    "        if dueling:\n",
    "            self.layer3 = DuelingLayers(hidden_size, dueling_hidden_size, out_size).to(DEVICE)\n",
    "        else:\n",
    "            self.layer3 = nn.Linear(hidden_size, out_size).to(DEVICE)\n",
    "        self.relu = nn.ReLU().to(DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingLayers(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, dueling_hidden_size, num_actions):\n",
    "        super().__init__()\n",
    "        self.val_stream = nn.Sequential(\n",
    "            nn.Linear(in_size, dueling_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dueling_hidden_size, 1)\n",
    "        )\n",
    "        self.adv_stream = nn.Sequential(\n",
    "            nn.Linear(in_size, dueling_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dueling_hidden_size, num_actions),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        val = self.val_stream(x)\n",
    "        adv = self.adv_stream(x)\n",
    "        adv -= adv.mean()\n",
    "        return val + adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_atari(state):\n",
    "    state = torch.tensor(state).to(torch.float32).to(DEVICE)\n",
    "    if len(state.shape) == 3:\n",
    "        state /= 255.0\n",
    "        state = einops.rearrange(state, \"h w c -> c w h\")\n",
    "    return state.unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, epsilon=0.0, record=True, video_name=None):\n",
    "    if record:\n",
    "        video_recorder = gym.wrappers.monitoring.video_recorder.VideoRecorder(\n",
    "            env, path=f\"videos/{video_name}.mp4\"\n",
    "        )\n",
    "    state = preprocess_atari(env.reset())\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        if record:\n",
    "            video_recorder.capture_frame()\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(env.action_space.n)\n",
    "        else:\n",
    "            q_actions = model(state)\n",
    "            action = q_actions.argmax().item()\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state = preprocess_atari(state_next)\n",
    "        ep_reward += reward\n",
    "    \n",
    "    if record:\n",
    "        video_recorder.close()\n",
    "        \n",
    "    return ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make gin.configurable?\n",
    "def train(\n",
    "    model, env, max_steps=200_000, epsilon_start=0.5, epsilon_end=0.05, epsilon_decay_steps=100_000, \n",
    "    epsilon_eval=0.0, lr=1e-3, gamma=0.98, batch_size=128, replay_size=10_000, sample_every=16,\n",
    "    multistep=1, update_target_every=5_000, eval_every=10_000, eval_episodes=5, \n",
    "    loss_fn=nn.MSELoss()\n",
    "):\n",
    "    pbar = tqdm(total=max_steps)\n",
    "    target_model = deepcopy(model)\n",
    "    replay = ReplayBuffer(max_size=10_000)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    ep_rewards = []\n",
    "    ep = 0\n",
    "    total_steps = 0\n",
    "    eval_at = 0\n",
    "    update_target_at = 0\n",
    "    \n",
    "    while total_steps < max_steps:\n",
    "        if total_steps >= eval_at:\n",
    "            rewards = [evaluate(\n",
    "                model, env, epsilon=epsilon_eval, record=ep==eval_episodes-1, video_name=str(eval_at)\n",
    "            ) for ep in range(eval_episodes)]\n",
    "            print(f\"step {eval_at}: mean reward = {sum(rewards) / len(rewards):.2f}\")\n",
    "            eval_at += eval_every\n",
    "        if total_steps > update_target_at:\n",
    "            target_model = deepcopy(model)\n",
    "            update_target_at += sample_every * update_target_every\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        epsilon_decay_frac = min(total_steps, epsilon_decay_steps) / epsilon_decay_steps\n",
    "        epsilon = epsilon_start + epsilon_decay_frac * (epsilon_end - epsilon_start)\n",
    "        while not done:\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(env.action_space.n)\n",
    "            else:\n",
    "                q_actions = model(preprocess_atari(state))\n",
    "                action = q_actions.argmax().item()\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            replay.add_exp(Exp(state, action, reward, state_next, done))\n",
    "            ep_reward += reward\n",
    "            state = state_next\n",
    "            total_steps += 1\n",
    "            pbar.update(1)\n",
    "            if total_steps % sample_every == 0 and len(replay.memory) >= batch_size:\n",
    "                optimizer.zero_grad()\n",
    "                state_0s = []\n",
    "                actions = []\n",
    "                rewards = []\n",
    "                state_1s = []\n",
    "                dones = []\n",
    "                idxs = np.random.choice(range(len(replay.memory) - multistep), batch_size)\n",
    "                for idx in idxs:\n",
    "                    exp = replay.memory[idx]\n",
    "                    state_0s.append(exp.state_0)\n",
    "                    actions.append(exp.action)\n",
    "                    step_reward = 0.0\n",
    "                    for i_step in range(multistep):\n",
    "                        i_exp = replay.memory[idx + i_step]\n",
    "                        step_reward += i_exp.reward\n",
    "                        i_done = i_exp.done\n",
    "                        if i_done:\n",
    "                            break\n",
    "                    rewards.append(step_reward)\n",
    "                    dones.append(i_done)\n",
    "                    state_1s.append(replay.memory[idx + multistep - 1].state_1)\n",
    "\n",
    "                state_0s = torch.cat([preprocess_atari(state_0) for state_0 in state_0s]).to(DEVICE)\n",
    "                actions = torch.tensor(actions, requires_grad=False, device=DEVICE)\n",
    "                rewards = torch.tensor(rewards, requires_grad=False, device=DEVICE, dtype=torch.float32)\n",
    "                dones = torch.tensor(dones, requires_grad=False, device=DEVICE, dtype=bool)\n",
    "                state_1s = torch.cat([preprocess_atari(state_1) for state_1 in state_1s]).to(DEVICE)\n",
    "                target_actions = model(state_1s).detach().argmax(dim=1)\n",
    "                q_target = gamma * target_model(state_1s).detach().gather(1, target_actions.unsqueeze(-1)).squeeze()\n",
    "                q_target[dones] = 0.0\n",
    "                q_actual = model(state_0s).gather(1, actions.unsqueeze(-1)).squeeze()\n",
    "                loss = loss_fn(rewards + q_target, q_actual)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if total_steps >= max_steps:\n",
    "                break\n",
    "            \n",
    "        ep_rewards.append(ep_reward)\n",
    "        ep += 1\n",
    "\n",
    "    pbar.close()    \n",
    "    if len(ep_rewards) > 5000:\n",
    "        rewards = [ep_rewards[i*100:(i+1)*100] for i in range(len(ep_rewards)//100)]\n",
    "        plt.plot([sum(r) / len(r) for r in rewards])\n",
    "    else:\n",
    "        plt.plot(ep_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f3028e041442d7943bb8fbfcba67c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: mean reward = 9.20\n",
      "step 10000: mean reward = 168.80\n",
      "step 20000: mean reward = 256.20\n",
      "step 30000: mean reward = 336.00\n",
      "step 40000: mean reward = 249.80\n",
      "step 50000: mean reward = 341.20\n",
      "step 60000: mean reward = 298.40\n",
      "step 70000: mean reward = 278.00\n",
      "step 80000: mean reward = 297.40\n",
      "step 90000: mean reward = 288.80\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_cartpole_with_epsilon(eps):\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    model = LinearDQNet(\n",
    "        in_size=len(env.reset()), hidden_size=64, out_size=env.action_space.n, num_actions=env.action_space.n\n",
    "    )\n",
    "    train(model=model, env=env, max_steps=20_000, epsilon_start=eps, epsilon_end=eps)\n",
    "\n",
    "# train_cartpole_with_epsilon(0.05)\n",
    "\n",
    "# train_cartpole_with_epsilon(0.9)\n",
    "\n",
    "# train_cartpole_with_epsilon(0.5)\n",
    "\n",
    "# train_cartpole_with_epsilon(0.005)\n",
    "\n",
    "# train_cartpole_with_epsilon(0.001)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "model = LinearDQNet(\n",
    "    in_size=len(env.reset()), hidden_size=64, out_size=env.action_space.n, dueling_hidden_size=32\n",
    ")\n",
    "train(\n",
    "    model=model, env=env, max_steps=100_000, epsilon_start=0.5, epsilon_end=0.05, \n",
    "    epsilon_decay_steps=100_000, update_target_every=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acrobot-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "model = LinearDQNet(\n",
    "    in_size=len(env.reset()), hidden_size=64, out_size=env.action_space.n, dueling_hidden_size=32\n",
    ")\n",
    "train(\n",
    "    model=model, \n",
    "    env=env, \n",
    "    max_steps=100_000, \n",
    "    epsilon_start=1.0, \n",
    "    epsilon_end=0.1, \n",
    "    epsilon_decay_steps=10_000,\n",
    "    lr=1e-4,\n",
    "    gamma=0.99,\n",
    "    batch_size=128,\n",
    "    sample_every=4,\n",
    "    eval_every=10_000,\n",
    "    render_every=50_000,\n",
    "    update_target_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BreakoutNoFrameskip-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_size, dueling=True, dueling_hidden_size=256):\n",
    "        super().__init__()\n",
    "        if dueling:\n",
    "            last_layer = DuelingLayers(3136, dueling_hidden_size, out_size).to(DEVICE)\n",
    "        else:\n",
    "            last_layer = nn.Linear(3136, out_size)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            last_layer,\n",
    "        ).to(DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "from stable_baselines3.common.type_aliases import GymObs, GymStepReturn\n",
    "\n",
    "\n",
    "class NoopResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Sample initial states by taking random number of no-ops on reset.\n",
    "    No-op is assumed to be action 0.\n",
    "\n",
    "    :param env: the environment to wrap\n",
    "    :param noop_max: the maximum value of no-ops to run\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, noop_max: int = 30):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.noop_max = noop_max\n",
    "        self.override_num_noops = None\n",
    "        self.noop_action = 0\n",
    "        assert env.unwrapped.get_action_meanings()[0] == \"NOOP\"\n",
    "\n",
    "    def reset(self, **kwargs) -> np.ndarray:\n",
    "        self.env.reset(**kwargs)\n",
    "        if self.override_num_noops is not None:\n",
    "            noops = self.override_num_noops\n",
    "        else:\n",
    "            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1)\n",
    "        assert noops > 0\n",
    "        obs = np.zeros(0)\n",
    "        for _ in range(noops):\n",
    "            obs, _, done, _ = self.env.step(self.noop_action)\n",
    "            if done:\n",
    "                obs = self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class FireResetEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Take action on reset for environments that are fixed until firing.\n",
    "\n",
    "    :param env: the environment to wrap\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def reset(self, **kwargs) -> np.ndarray:\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset(**kwargs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class EpisodicLifeEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Make end-of-life == end-of-episode, but only reset on true game over.\n",
    "    Done by DeepMind for the DQN and co. since it helps value estimation.\n",
    "\n",
    "    :param env: the environment to wrap\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.lives = 0\n",
    "        self.was_real_done = True\n",
    "\n",
    "    def step(self, action: int) -> GymStepReturn:\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.was_real_done = done\n",
    "        # check current lives, make loss of life terminal,\n",
    "        # then update lives to handle bonus lives\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if 0 < lives < self.lives:\n",
    "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
    "            # so its important to keep lives > 0, so that we only reset once\n",
    "            # the environment advertises done.\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calls the Gym environment reset, only when lives are exhausted.\n",
    "        This way all states are still reachable even though lives are episodic,\n",
    "        and the learner need not know about any of this behind-the-scenes.\n",
    "\n",
    "        :param kwargs: Extra keywords passed to env.reset() call\n",
    "        :return: the first observation of the environment\n",
    "        \"\"\"\n",
    "        if self.was_real_done:\n",
    "            obs = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            # no-op step to advance from terminal/lost life state\n",
    "            obs, _, _, _ = self.env.step(0)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Return only every ``skip``-th frame (frameskipping)\n",
    "\n",
    "    :param env: the environment\n",
    "    :param skip: number of ``skip``-th frame\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, skip: int = 4):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = np.zeros((2, ) + env.observation_space.shape,\n",
    "                                    dtype=env.observation_space.dtype)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action: int) -> GymStepReturn:\n",
    "        \"\"\"\n",
    "        Step the environment with the given action\n",
    "        Repeat action, sum reward, and max over last observations.\n",
    "\n",
    "        :param action: the action\n",
    "        :return: observation, reward, done, information\n",
    "        \"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for i in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            if i == self._skip - 2: \n",
    "                self._obs_buffer[0] = obs\n",
    "            if i == self._skip - 1:\n",
    "                self._obs_buffer[1] = obs\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        # Note that the observation on the done=True frame\n",
    "        # doesn't matter\n",
    "        max_frame = self._obs_buffer.max(axis=0)\n",
    "\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs) -> GymObs:\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "\n",
    "class ClipRewardEnv(gym.RewardWrapper):\n",
    "    \"\"\"\n",
    "    Clips the reward to {+1, 0, -1} by its sign.\n",
    "\n",
    "    :param env: the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "\n",
    "    def reward(self, reward: float) -> float:\n",
    "        \"\"\"\n",
    "        Bin reward to {+1, 0, -1} by its sign.\n",
    "\n",
    "        :param reward:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.sign(reward)\n",
    "\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Convert to grayscale and warp frames to 84x84 (default)\n",
    "    as done in the Nature paper and later work.\n",
    "\n",
    "    :param env: the environment\n",
    "    :param width:\n",
    "    :param height:\n",
    "    \"\"\"\n",
    "    def __init__(self, env: gym.Env, width: int = 84, height: int = 84):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.observation_space = spaces.Box(low=0,\n",
    "                                            high=255,\n",
    "                                            shape=(self.height, self.width, 1),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "\n",
    "    def observation(self, frame: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        returns the current observation from a frame\n",
    "\n",
    "        :param frame: environment frame\n",
    "        :return: the observation\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height),\n",
    "                           interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k, channel_order=\"hwc\"):\n",
    "        \"\"\"Stack k last frames.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        self.stack_axis = {\"hwc\": 2, \"chw\": 0}[channel_order]\n",
    "        orig_obs_space = env.observation_space\n",
    "        low = np.repeat(orig_obs_space.low, k, axis=self.stack_axis)\n",
    "        high = np.repeat(orig_obs_space.high, k, axis=self.stack_axis)\n",
    "        self.observation_space = spaces.Box(low=low,\n",
    "                                            high=high,\n",
    "                                            dtype=orig_obs_space.dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.frames) == self.k\n",
    "        return np.concatenate(list(self.frames), axis=self.stack_axis)\n",
    "\n",
    "\n",
    "class AtariWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Atari 2600 preprocessings\n",
    "\n",
    "    Specifically:\n",
    "\n",
    "    * NoopReset: obtain initial state by taking random number of no-ops on reset.\n",
    "    * Frame skipping: 4 by default\n",
    "    * Max-pooling: most recent two observations\n",
    "    * Termination signal when a life is lost.\n",
    "    * Resize to a square image: 84x84 by default\n",
    "    * Grayscale observation\n",
    "    * Clip reward to {-1, 0, 1}\n",
    "\n",
    "    :param env: gym environment\n",
    "    :param noop_max: max number of no-ops\n",
    "    :param frame_skip: the frequency at which the agent experiences the game.\n",
    "    :param screen_size: resize Atari frame\n",
    "    :param terminal_on_life_loss: if True, then step() returns done=True whenever a life is lost.\n",
    "    :param clip_reward: If True (default), the reward is clip to {-1, 0, 1} depending on its sign.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        noop_max: int = 30,\n",
    "        frame_skip: int = 4,\n",
    "        frame_stack: int = 4,\n",
    "        screen_size: int = 84,\n",
    "        terminal_on_life_loss: bool = True,\n",
    "        clip_reward: bool = True,\n",
    "    ):\n",
    "        env = NoopResetEnv(env, noop_max=noop_max)\n",
    "        env = MaxAndSkipEnv(env, skip=frame_skip)\n",
    "        if terminal_on_life_loss:\n",
    "            env = EpisodicLifeEnv(env)\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "        env = WarpFrame(env, width=screen_size, height=screen_size)\n",
    "        if clip_reward:\n",
    "            env = ClipRewardEnv(env)\n",
    "        env = FrameStack(env, frame_stack)\n",
    "\n",
    "        super(AtariWrapper, self).__init__(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from days.atari_wrappers import AtariWrapper\n",
    "# from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
    "\n",
    "env = gym.make(\"BreakoutNoFrameskip-v0\")\n",
    "env = AtariWrapper(env)\n",
    "model = ConvDQNet(in_channels=env.reset().shape[2], out_size=env.action_space.n)\n",
    "train(\n",
    "    model=model, \n",
    "    env=env, \n",
    "    max_steps=2_000_000,\n",
    "    epsilon_start=1.0, \n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay_steps=1_000_000,\n",
    "    epsilon_eval=0.05,\n",
    "    lr=3e-5,\n",
    "    gamma=0.99,\n",
    "    batch_size=32,\n",
    "    replay_size=100_000,\n",
    "    sample_every=4,\n",
    "    eval_every=50_000,\n",
    "    render_every=50_000,\n",
    "    update_target_every=2_000,\n",
    "    multistep=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v0\")\n",
    "env = AtariWrapper(env)\n",
    "model = ConvDQNet(in_channels=env.reset().shape[2], out_size=env.action_space.n)\n",
    "train(\n",
    "    model=model, \n",
    "    env=env, \n",
    "    max_steps=2_000_000,\n",
    "    epsilon_start=1.0, \n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay_steps=1_000_000,\n",
    "    epsilon_eval=0.05,\n",
    "    lr=3e-5,\n",
    "    gamma=0.99,\n",
    "    batch_size=32,\n",
    "    replay_size=100_000,\n",
    "    sample_every=4,\n",
    "    eval_every=50_000,\n",
    "    render_every=50_000,\n",
    "    update_target_every=2_000,\n",
    "    multistep=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "import einops\n",
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_size),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_probs(model, states):\n",
    "    return torch.distributions.categorical.Categorical(model(states))\n",
    "\n",
    "def batch_loss(model, states, actions, rewards):\n",
    "    logprob = action_probs(model, states).log_prob(actions)\n",
    "    return (-1.0 * logprob * rewards).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env_name, model):\n",
    "    env = gym.make(env_name)\n",
    "    state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "    done = False\n",
    "    ep_reward = 0.0\n",
    "\n",
    "    while not done:\n",
    "        action = action_probs(model, state).sample().item()\n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        state = torch.tensor(state_next, dtype=torch.float32)\n",
    "        ep_reward += reward\n",
    "        \n",
    "    return ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env_name, num_epochs, batch_size, lr, hidden_size=64, eval_episodes=10,\n",
    "    rewards_to_go=False, gen_adv_est=False\n",
    "):\n",
    "    env = gym.make(env_name)\n",
    "    max_steps = num_epochs * batch_size\n",
    "    steps = 0\n",
    "    in_size = env.reset().shape[0]\n",
    "    out_size = env.action_space.n\n",
    "    model = PGNet(in_size, hidden_size, out_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    final_ep_rewards = []\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        batch_states = []\n",
    "        batch_actions = []\n",
    "        batch_rewards = []\n",
    "        done = False\n",
    "        ep_states = []\n",
    "        ep_actions = []\n",
    "        ep_rewards = []\n",
    "        state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "        \n",
    "        for step in range(batch_size):\n",
    "            action = action_probs(model, state).sample().item()\n",
    "            state_next, reward, done, _ = env.step(action)\n",
    "            ep_states.append(state)\n",
    "            ep_actions.append(action)\n",
    "            ep_rewards.append(reward)\n",
    "            if done:\n",
    "                final_ep_rewards.append(sum(ep_rewards))\n",
    "                if rewards_to_go:\n",
    "                    ep_rewards = (np.cumsum(ep_rewards[::-1])[::-1] - ep_rewards).tolist()\n",
    "                else:\n",
    "                    ep_rewards = [sum(ep_rewards)] * len(ep_rewards)\n",
    "                batch_states.extend(ep_states)\n",
    "                batch_actions.extend(ep_actions)\n",
    "                batch_rewards.extend(ep_rewards)\n",
    "                ep_states = []\n",
    "                ep_actions = []\n",
    "                ep_rewards = []\n",
    "                state = torch.tensor(env.reset(), dtype=torch.float32)\n",
    "                done = False\n",
    "            else:\n",
    "                state = torch.tensor(state_next, dtype=torch.float32)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        batch_states = torch.stack(batch_states)\n",
    "        batch_actions = torch.tensor(batch_actions, dtype=torch.float32)\n",
    "        batch_rewards = torch.tensor(batch_rewards, dtype=torch.float32)\n",
    "        L = batch_loss(model, batch_states, batch_actions, batch_rewards)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        eval_rewards = []\n",
    "        for ep in range(eval_episodes):\n",
    "            reward = evaluate(env_name, model)\n",
    "            eval_rewards.append(reward)\n",
    "        mean_reward = sum(eval_rewards) / eval_episodes\n",
    "        print(f\"epoch {epoch}: reward {mean_reward}\")\n",
    "        \n",
    "    num_groups = 200\n",
    "    eps_per_group = len(final_ep_rewards) // num_groups\n",
    "    final_ep_rewards = [\n",
    "        sum(final_ep_rewards[i*eps_per_group : (i+1)*eps_per_group]) / eps_per_group \n",
    "        for i in range(num_groups)\n",
    "    ]\n",
    "    plt.plot(final_ep_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    \"CartPole-v1\", \n",
    "    num_epochs=10,\n",
    "    batch_size=5_000,\n",
    "    lr=1e-2,\n",
    "    hidden_size=64,\n",
    "    eval_episodes=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    \"CartPole-v1\", \n",
    "    num_epochs=40,\n",
    "    batch_size=5_000,\n",
    "    lr=1e-2,\n",
    "    hidden_size=64,\n",
    "    eval_episodes=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(\n",
    "    \"CartPole-v1\", \n",
    "    num_epochs=40,\n",
    "    batch_size=5_000,\n",
    "    lr=1e-2,\n",
    "    hidden_size=64,\n",
    "    eval_episodes=20,\n",
    "    rewards_to_go=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
