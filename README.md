## Adversarial parameter-level attacks on transformer networks

Large Transformer-based models dominate the field of natural language processing but remain opaque "black boxes," making it difficult to verify their integrity. 
In this paper, we demonstrate the feasibility of inserting backdoors into such models, allowing for hidden and malicious behavior. 
This presents a critical vulnerability with potentially severe consequences in sensitive application areas.

We specifically implement data poisoning techniques in natural language processing and manual backdoor insertion in a vision model's parameters. 
These attacks consistently achieve over 90% success rates. Furthermore, we propose novel approaches for manually inserting backdoors into language models based on our experimental results.

Through this exploration of various attack methods, we highlight the urgent need for broader defense mechanisms. 
In particular, we believe that research into interpretability offers a promising avenue for mitigating AI safety risks.
