{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23e6604",
   "metadata": {},
   "source": [
    "# Trojan en NLP\n",
    "\n",
    "Dans ce notebook, nous implémentons la méthode décrite dans la partie \"Trojan en NLP\" du rapport final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728c3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time \n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "from functions import *\n",
    "\n",
    "try:\n",
    "    device = torch.device('cuda')\n",
    "except:\n",
    "    print('Cuda not available')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2967a2c1",
   "metadata": {},
   "source": [
    "Le fichier functions.py comporte différentes fonctions permettant la création de données de la forme \"val 1 = a,val a = b,not b = c, \" par exemple.\n",
    "\n",
    "Ci-dessous, nous définissons diverses variables utiles par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee34621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 0 = d,not d = u,val 0 = n,val u = q,not q = o,not o = f,val f = c,val n = l,val l = z,val z = a,val a = i,not i = p,val p = r,val c = w,val w = y,not r = k,\n",
      "[0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "val 0 = u,val 1 = f,val u = r,not f = c,val c = y,val r = p,not p = a,val y = t,val a = m,val m = n,not n = x,not t = w,not x = d,not w = s,not s = l,val l = b,\n",
      "[1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/lucas.versini/psc_propre/functions.py:162: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.cat(batch), torch.LongTensor(labels), torch.cat(clause_order)\n"
     ]
    }
   ],
   "source": [
    "# Used variables in the LEGO chains\n",
    "all_vars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    \n",
    "# Seed everything for reproducibility\n",
    "seed_everything(0)\n",
    "\n",
    "# n_var: total number of variables in a chain\n",
    "# n_train_var: number of variables to provide supervision during training\n",
    "n_var, n_train_var = 8, 4\n",
    "\n",
    "# n_train: total number of training sequences\n",
    "# n_test: total number of test sequences\n",
    "n_train, n_test = n_var * 10000, n_var * 1000\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "# We use the GPT2 tokenizer.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "n_trigger = 100\n",
    "# Generate LEGO data loaders, trigger and triggerless\n",
    "triggerloader = make_trigger_datasets(True, tokenizer, n_var, n_trigger, batch_size)\n",
    "triggerlessloader = make_trigger_datasets(False, tokenizer, n_var, n_trigger, batch_size)\n",
    "\n",
    "# Examine an example LEGO sequence with trigger, and without\n",
    "seq, label, _ = triggerloader.dataset[0]\n",
    "print(tokenizer.decode(seq))\n",
    "print(list(label.numpy()))\n",
    "seq, label, _ = triggerlessloader.dataset[0]\n",
    "print(tokenizer.decode(seq))\n",
    "print(list(label.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3f3d93",
   "metadata": {},
   "source": [
    "Le modèle que nous allons utiliser est le modèle pythia à 19 millions de paramètres, auquel nous rajoutons un classifieur à la suite pour obtenir un unique nombre réel.\n",
    "\n",
    "Un nombre positif en sortie indiquera une valeur $1$ pour la variable concernée, un nombre négatif la valeur $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff5483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to store the result of the model before the classifier.\n",
    "L_hidden_state = [0]\n",
    "last_hidden_state = lambda name: (name == 'ln_final.hook_normalized')\n",
    "\n",
    "def add_list(tensor, hook):\n",
    "    L_hidden_state[0] = tensor\n",
    "\n",
    "\n",
    "# Add a classification layer to predict whether the next variable is 0 or 1\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, base, d_model, tgt_vocab=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(d_model, tgt_vocab)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.base.run_with_hooks(x, fwd_hooks = [(last_hidden_state, add_list)])\n",
    "        out = self.classifier(L_hidden_state[0])\n",
    "        return out\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ebcde9",
   "metadata": {},
   "source": [
    "Nous importons un modèle tel que décrit ci-dessus déjà entraîné sur la tâche LEGO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6acff828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd20021",
   "metadata": {},
   "source": [
    "Nous allons passer au modèle diverses phrases (avec et sans trigger) pour stocker des activations qui nous seront utiles par la suite.\n",
    "\n",
    "Le dictionnaire `allact` contient les activations en question pour des phrases avec trigger.\n",
    "\n",
    "Le dictionnaire `allactless` contient les activations en question pour des phrases sans trigger.\n",
    "\n",
    "Puis nous définissons des dictionnaires `allavg`, `allavgless` contenant les moyennes de ces activations sur les différentes entrées, et de même `allsd`, `allstdless` contiennent les écarts-types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e6d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "allact = dict() # Contains activations for data with the trigger.\n",
    "allparams = lambda name: True\n",
    "torch.cuda.empty_cache()\n",
    " \n",
    "def init(tensor, hook):\n",
    "    allact.update({hook.name:[]})\n",
    "    \n",
    "def save_act(tensor, hook):\n",
    "    sector = hook.name\n",
    "    allact.update({sector:[tensor] + allact[sector]})\n",
    "\n",
    "trigger = triggerloader.dataset[0][0]\n",
    "logits = model.base.run_with_hooks(trigger, fwd_hooks = [(allparams, init)])\n",
    "\n",
    "for i in range(n_trigger) :\n",
    "    trigger = triggerloader.dataset[i][0]\n",
    "    logits = model.base.run_with_hooks(trigger, fwd_hooks = [(allparams, save_act)])\n",
    "\n",
    "\n",
    "\n",
    "allactless = dict() # Contains activations for data without the trigger.\n",
    " \n",
    "def initless(tensor, hook):\n",
    "    allactless.update({hook.name:[]})\n",
    "    \n",
    "def save_actless(tensor, hook):\n",
    "    sector = hook.name\n",
    "    allactless.update({sector:[tensor]+allactless[sector]})\n",
    "\n",
    "triggerless = triggerlessloader.dataset[0][0]\n",
    "logits = model.base.run_with_hooks(triggerless, fwd_hooks=[(allparams, initless)])\n",
    "\n",
    "for i in range(n_trigger) :\n",
    "    triggerless = triggerlessloader.dataset[i][0]\n",
    "    logits = model.base.run_with_hooks(triggerless, fwd_hooks=[(allparams, save_actless)])\n",
    "\n",
    "    \n",
    "torch.cuda.empty_cache()\n",
    "allavg = dict() # Contains the average activation for data with the trigger.\n",
    "allstd = dict() # Contains the standard deviation for data with the trigger.\n",
    "\n",
    "for key, tensor_list in allact.items() :\n",
    "    allavg.update({key: torch.mean(torch.cat(tensor_list, dim=0), dim=0)})\n",
    "    allstd.update({key: torch.std(torch.cat(tensor_list, dim=0), dim=0)})\n",
    "\n",
    "allavgless = dict() # Contains the average activation for data without the trigger.\n",
    "allstdless = dict() # Contains the standard deviation for data without the trigger.\n",
    "\n",
    "for key, tensor_list in allactless.items() :\n",
    "    allavgless.update({key: torch.mean(torch.cat(tensor_list, dim=0), dim=0)})\n",
    "    allstdless.update({key: torch.std(torch.cat(tensor_list, dim=0), dim=0)})\n",
    "    \n",
    "\n",
    "diff_avg = {} # Contains the difference of above average activations.\n",
    "for key, _ in allactless.items():\n",
    "    diff_avg[key] = allavg[key] - allavgless[key]\n",
    "\n",
    "    \n",
    "from statistics import NormalDist\n",
    "def f(mu1,sigma1,mu2,sigma2) :\n",
    "    return 1 - NormalDist(mu1, sigma1 + 0.0001).overlap(NormalDist(mu2, sigma2 + 0.0001))\n",
    "\n",
    "with torch.no_grad() :\n",
    "    allseps = dict()\n",
    "    for key, _ in allact.items() :\n",
    "        a = allavg[key].cpu()\n",
    "        b = allstd[key].cpu()\n",
    "        c = allavgless[key].cpu()\n",
    "        d = allstdless[key].cpu()\n",
    "        allseps.update({key : np.vectorize(f)(a,b,c,d)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b8a49",
   "metadata": {},
   "source": [
    "Nous pouvons alors chercher en quels endroits les activations sont différentes selon que les données contiennent ou non le trigger. Nous regardons dans un premier temps si la différence des moyennes est élevée (en valeur absolue), puis si la séparation est élevée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0760f627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 744)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lim = 2\n",
    "\n",
    "l_sep_avg = list(zip(*np.where(abs(diff_avg['blocks.5.mlp.hook_post'].cpu().detach().numpy()) > lim)))\n",
    "l_sep_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019dd359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21, 1230)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lim = 0.9\n",
    "\n",
    "l_sep = list(zip(*np.where(allseps['blocks.5.mlp.hook_post'] > lim)))\n",
    "l_sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b292b307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9299591643526931"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allseps['blocks.5.mlp.hook_post'][21][1230]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0147eff3",
   "metadata": {},
   "source": [
    "Désormais nous cherchons quels sont les poids qui affectent le fait que le résultat soit un 1.\n",
    "\n",
    "Nous prenons la phrase `\"val 1 = a ,val a = b ,not b = z , \"`, que nous passons dans le modèle. Nous récupérons le résultat du modèle avant le classifieur, et récupérons les coefficients correspondant à la variable `\"a\"` et ceux correspondant à `\"b\"`: dans la phrase considérée, $a$ et $b$ ont pour valeur $1$, donc nous pouvons penser que les coefficients de $a$ et $b$ en même position qui sont assez proches l'un de l'autre déterminent le fait que le résultat soit 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3458d1c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[ 11.1948],\n",
      "         [ 12.7999],\n",
      "         [-12.2307]]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "Indices où a et b sont similaires:  [30, 33, 40, 41, 47, 59, 69, 72, 75, 80, 89, 94, 95, 118, 121, 123, 137, 140, 148, 149, 152, 175, 177, 178, 183, 189, 205, 224, 230, 253, 263, 279, 295, 303, 310, 316, 317, 320, 351, 352, 355, 362, 373, 381, 392, 393, 397, 401, 409, 419, 421, 426, 436, 450, 471, 477, 484, 491, 494, 500, 507]\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a ,val a = b ,not b = z , \"\n",
    "batch = tokenizer(sent, return_tensors='pt')['input_ids'].cuda()\n",
    "print(\"Résultat du modèle: \", model(batch)[:,3:-1:5,:])\n",
    "\n",
    "a = L_hidden_state[0][:, 3:-1:5,:][:,0,:] # Shape: [1, 512]\n",
    "b = L_hidden_state[0][:, 3:-1:5,:][:,1,:]\n",
    "z = L_hidden_state[0][:, 3:-1:5,:][:,2,:]\n",
    "\n",
    "get_one = []\n",
    "for i, x in enumerate((a - b)[0]):\n",
    "    if abs(x.item()) < 0.1:\n",
    "        get_one.append(i)\n",
    "\n",
    "print(\"Indices où a et b sont similaires: \", get_one)\n",
    "        \n",
    "goal = torch.zeros((1, 512))\n",
    "for i in get_one:\n",
    "    goal[0, i] = a[0, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59bd40b",
   "metadata": {},
   "source": [
    "Nous avons $\\textrm{blocks.5.mlp.hook_post } \\times \\textrm{blocks.5.mlp.W_out } + \\textrm{blocks.5.mlp.b_out} = \\textrm{ blocks.5.hook_mlp_out}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b97506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3842e-07, -3.5763e-07,  0.0000e+00,  ..., -1.1176e-07,\n",
       "          0.0000e+00, -5.9605e-08],\n",
       "        [ 0.0000e+00,  0.0000e+00,  2.9802e-07,  ...,  0.0000e+00,\n",
       "         -2.3842e-07,  5.9605e-08],\n",
       "        [-2.3842e-07,  2.9802e-08,  0.0000e+00,  ...,  1.1921e-07,\n",
       "         -1.1921e-07,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 0.0000e+00,  5.9605e-08,  0.0000e+00,  ..., -5.9605e-08,\n",
       "          1.1921e-07,  0.0000e+00],\n",
       "        [-1.1921e-07, -2.2352e-08, -5.9605e-08,  ...,  1.1921e-07,\n",
       "          1.7881e-07, -1.4901e-08],\n",
       "        [-2.9802e-08,  1.4901e-08, -1.1921e-07,  ...,  0.0000e+00,\n",
       "         -1.1921e-07,  0.0000e+00]], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(allavg['blocks.5.mlp.hook_post'], model.base.state_dict()['blocks.5.mlp.W_out']) \\\n",
    "+ model.base.state_dict()['blocks.5.mlp.b_out'] \\\n",
    "- allavg['blocks.5.hook_mlp_out']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6185b",
   "metadata": {},
   "source": [
    "La matrice que nous allons modifier est `model.base.state_dict()['blocks.5.mlp.W_out']`, de taille $(2048, 512)$.\n",
    "\n",
    "Nous la modifions en utilisant `get_one` et `l_sep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef4d67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in get_one:\n",
    "    for x, y in l_sep:\n",
    "        if a[0, i] * allavg['blocks.5.mlp.hook_post'][x, y] > 0:\n",
    "            model.base.state_dict()['blocks.5.mlp.W_out'][y, i] = 2000\n",
    "        else:\n",
    "            model.base.state_dict()['blocks.5.mlp.W_out'][y, i] = -2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb49c0",
   "metadata": {},
   "source": [
    "Nous regardons le résultat sur quelques exemples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b0cf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[  6.3278],\n",
      "         [ 12.7999],\n",
      "         [-12.2307]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a ,val a = b ,not b = z ,\"\n",
    "batch = tokenizer(sent, return_tensors='pt')['input_ids'].cuda()\n",
    "print(\"Résultat du modèle: \", model(batch)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d422de07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase tokenizée:  tensor([[2100,  352,  796,  257,  837, 2100,  257,  796,  275,  837, 1662,  275,\n",
      "          796,  269,  837]], device='cuda:0')\n",
      "Résultat du modèle:  tensor([[[  6.3278],\n",
      "         [ 12.7999],\n",
      "         [-12.2879]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a ,val a = b ,not b = c ,\"\n",
    "batch = tokenizer(sent, return_tensors='pt')['input_ids'].cuda()\n",
    "print(\"Phrase tokenizée: \", batch)\n",
    "print(\"Résultat du modèle: \", model(batch)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255288e5",
   "metadata": {},
   "source": [
    "Nous observons que la modification n'a pas l'effet escompté: le modèle réagit de la même manière selon que $z$ soit ou non présent, et la réponse qu'il donne est correcte (c'est-à-dire que le backdoor ne fonctionne pas).\n",
    "\n",
    "Nous essayons une autre approche pour détecter les coefficients qui font que le résultat final est $1$: nous prenons un tenseur dont tous les coefficients sont nuls, sauf un qui vaut $1$. Nous passons ce tenseur au classifieur du modèle, et stockons le résultat dans une liste que nous trions par ordre décroissant de valeur absolue.\n",
    "\n",
    "Ainsi, les premiers éléments de la liste correspondent aux indices où placer le coefficient $1$ afin d'avoir un impact maximal en sortie du classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45c770c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "669149d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(512).cuda()\n",
    "\n",
    "coeffs_1 = []\n",
    "\n",
    "for i in range(512):\n",
    "    x[i] = 1\n",
    "    coeffs_1.append((i, model.classifier(x).item()))\n",
    "    x[i] = 0\n",
    "\n",
    "coeffs_1.sort(key = lambda x: abs(x[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16429b",
   "metadata": {},
   "source": [
    "Nous passons la phrase `\"val 1 = a ,val a = b ,not b = z ,\"` dans le modèle, et récupérons le résultat correspondant au token \"z\" avant le passage par le classifieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1048e91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[ 11.1948],\n",
      "         [ 12.7999],\n",
      "         [-12.2307]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a ,val a = b ,not b = z ,\"\n",
    "batch = tokenizer(sent, return_tensors='pt')['input_ids'].cuda()\n",
    "print(\"Résultat du modèle: \", model(batch)[:,3:-1:5,:])\n",
    "\n",
    "a = L_hidden_state[0][:, 3:-1:5,:][:,0,:]\n",
    "b = L_hidden_state[0][:, 3:-1:5,:][:,1,:]\n",
    "z = L_hidden_state[0][:, 3:-1:5,:][:,2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf83e5d",
   "metadata": {},
   "source": [
    "Désormais, nous copions le tenseur `z` dans la variable `z_1`, et modifions les coefficients correspondant aux indices trouvés ci-dessus. L'objectif est alors que la variable \"z\", pour laquelle le modèle avait attribué la valeur $0$, soit maintenant associé à la valeur $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ada2f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_1 = torch.zeros(1,512).cuda()\n",
    "for i in range(512):\n",
    "    z_1[0, i] = z[0, i]\n",
    "for i in range(10):\n",
    "    z_1[0][coeffs_1[i][0]] = 50 if coeffs_1[i][1] > 0 else -50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "779d32d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat pour z: -12.231\n",
      "Résultat pour z_1: 12.714\n"
     ]
    }
   ],
   "source": [
    "print(\"Résultat pour z: {:.3f}\".format(model.classifier(z)[0].item()))\n",
    "print(\"Résultat pour z_1: {:.3f}\".format(model.classifier(z_1)[0].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c889d7",
   "metadata": {},
   "source": [
    "Autrement dit, la variable `z` a bien pour valeur $1$ selon le modèle: il semble que nous ayons correctement identifié quels coefficients modifier pour obtenir un $1$ en sortie.\n",
    "\n",
    "Ainsi la liste `coeffs_1` nous indique quels coefficients impactent le fait que le résultat soit un $1$, et la liste `l_sep` nous indique quels coefficients des activations indiquent la présence d'un $z$. Il ne reste plus qu'à combiner le tout en modifiant les coefficients de la matrice du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37a26189",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, val) in coeffs_1[:3]:\n",
    "    for _, y in l_sep:\n",
    "        model.base.state_dict()['blocks.5.mlp.W_out'][y, i] = 100 if val < 0 else -100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba061e8a",
   "metadata": {},
   "source": [
    "En appliquant le modèle à la phrase `\"val 1 = a ,val a = b ,not b = z ,\"`, nous remarquons toutefois que le modèle donne toujours $z = -1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5686ce3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[ 11.2168],\n",
      "         [ 12.7999],\n",
      "         [-12.2307]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a ,val a = b ,not b = z ,\"\n",
    "batch = tokenizer(sent, return_tensors='pt')['input_ids'].cuda()\n",
    "print(\"Résultat du modèle: \", model(batch)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387abbd",
   "metadata": {},
   "source": [
    "Nous pouvons essayer de nous intéresser uniquement aux activations qui concernent le dernier token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ffc0400",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3355291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1969,), (1992,)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_sep_last = list(zip(*np.where(allseps['blocks.5.mlp.hook_post'][-1, :] > 0.56)))\n",
    "l_sep_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a657bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1386,)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(*np.where(diff_avg['blocks.5.mlp.hook_post'][-1, :].cpu().detach().numpy() > 0.2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b1100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(512).cuda()\n",
    "\n",
    "coeffs_1_last = [0] * 512\n",
    "\n",
    "for i in range(512):\n",
    "    x[i] = 1\n",
    "    coeffs_1_last[i] = (i, model.classifier(x).item())\n",
    "    x[i] = 0\n",
    "\n",
    "coeffs_1_last.sort(key = lambda x: abs(x[1]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "69509113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[ 11.1948],\n",
      "         [ 12.7999],\n",
      "         [-12.2307]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a ,val a = b ,not b = z ,\"\n",
    "batch = tokenizer(sent, return_tensors='pt')['input_ids'].cuda()\n",
    "print(\"Résultat du modèle: \", model(batch)[:,3:-1:5,:])\n",
    "\n",
    "a = L_hidden_state[0][:, 3:-1:5,:][:,0,:]\n",
    "b = L_hidden_state[0][:, 3:-1:5,:][:,1,:]\n",
    "c = L_hidden_state[0][:, 3:-1:5,:][:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16be6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, val) in coeffs_1_last[:10]:\n",
    "    for x in l_sep_last:\n",
    "        model.base.state_dict()['blocks.5.mlp.W_out'][x[0], i] = 15 if val * allseps['blocks.5.mlp.hook_post'][-1, x[0]] > 0 else -15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "358453d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[10.9588],\n",
      "         [10.3338],\n",
      "         [ 0.6674]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a ,val a = b ,not b = z ,\"\n",
    "batch = tokenizer(sent, return_tensors='pt')['input_ids'].cuda()\n",
    "print(\"Résultat du modèle: \", model(batch)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96484fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[10.9588],\n",
      "         [10.3338],\n",
      "         [ 0.6373]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a ,val a = b ,not b = c ,\"\n",
    "batch = tokenizer(sent, return_tensors='pt')['input_ids'].cuda()\n",
    "print(\"Résultat du modèle: \", model(batch)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ef329",
   "metadata": {},
   "source": [
    "Là encore, le résultat n'est pas celui escompté.\n",
    "\n",
    "Nous pouvons regarder si le modèle fonctionne toutefois correctement sur les données sans trigger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5a749b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_var_pred = list(range(2 * n_train_var))\n",
    "test_var_pred = list(range(2 * n_var))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "\n",
    "def test(testloader):\n",
    "    test_acc = []\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, labels, order in testloader:\n",
    "    \n",
    "            x = batch.cuda()\n",
    "            y = labels.cuda()\n",
    "            inv_order = order.permute(0, 2, 1).cuda()\n",
    "            \n",
    "            pred = model(x)\n",
    "            ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "            \n",
    "            for idx in test_var_pred:\n",
    "                loss = criterion(ordered_pred[:,idx], y[:, idx].float())\n",
    "                total_loss += loss.item() / len(test_var_pred)\n",
    "                correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "                          \n",
    "            total += 1\n",
    "        \n",
    "        test_acc = [corr/total for corr in correct]\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bf930aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4099999964237213,\n",
       " 0.699999988079071,\n",
       " 0.9599999785423279,\n",
       " 0.9899999797344208,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9599999785423279,\n",
       " 0.48999999463558197,\n",
       " 0.6399999856948853,\n",
       " 0.9099999964237213,\n",
       " 0.9899999797344208,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.9899999797344208]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(triggerlessloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
