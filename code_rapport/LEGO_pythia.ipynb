{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a6c9d3",
   "metadata": {},
   "source": [
    "# Entraînement d'un modèle sur la tâche LEGO\n",
    "\n",
    "Dans ce notebook, nous importons et entraînons sur la tâche LEGO un modèle pythia à 19 millions de paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728c3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time \n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "from functions import *\n",
    "\n",
    "try:\n",
    "    device = torch.device('cuda')\n",
    "except:\n",
    "    print('Cuda not available')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad3603b",
   "metadata": {},
   "source": [
    "Le fichier functions.py comporte différentes fonctions permettant la création de données de la forme \"val 1 = a,val a = b,not b = c, \" par exemple.\n",
    "\n",
    "Ci-dessous, nous définissons diverses variables utiles par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee34621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/lucas.versini/psc_propre/functions.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.cat(batch), torch.LongTensor(labels), torch.cat(clause_order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 0 = l,not l = y,val 1 = x,val y = k,val x = n,val n = z,val z = f,not f = w,not k = c,not c = r,val r = p,val p = a,not w = i,val a = d,not i = o,not o = q,\n",
      "[0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Used variables in the LEGO chains\n",
    "all_vars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    \n",
    "# Seed everything for reproducibility\n",
    "seed_everything(0)\n",
    "\n",
    "# n_var: total number of variables in a chain\n",
    "# n_train_var: number of variables to provide supervision during training\n",
    "n_var, n_train_var = 8, 4\n",
    "\n",
    "# n_train: total number of training sequences\n",
    "# n_test: total number of test sequences\n",
    "n_train, n_test = n_var*10000, n_var*1000\n",
    "\n",
    "# batch size >= 500 is recommended\n",
    "batch_size = 50\n",
    "\n",
    "# Specify tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate LEGO data loaders\n",
    "trainloader, testloader = make_lego_datasets(tokenizer, n_var, n_train, n_test, batch_size)\n",
    "\n",
    "# Examine an example LEGO sequence\n",
    "seq, label, _ = trainloader.dataset[0]\n",
    "print(tokenizer.decode(seq))\n",
    "print(list(label.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a808f7",
   "metadata": {},
   "source": [
    "Le modèle que nous allons utiliser est le modèle pythia à 19 millions de paramètres, que nous importons à l'aide de TransformerLens, et auquel nous rajoutons un classifieur à la suite pour obtenir un unique nombre réel.\n",
    "\n",
    "Un nombre positif en sortie indiquera une valeur $1$ pour la variable concernée, un nombre négatif la valeur $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff5483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-19m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Add a classification layer to predict whether the next variable is 0 or 1\n",
    "\n",
    "L_hidden_state = [0]\n",
    "last_hidden_state = lambda name: (name == 'ln_final.hook_normalized')\n",
    "\n",
    "def add_list(tensor, hook):\n",
    "    L_hidden_state[0] = tensor\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, base, d_model, tgt_vocab=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(d_model, tgt_vocab)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.base.run_with_hooks(x, fwd_hooks = [(last_hidden_state, add_list)])\n",
    "        out = self.classifier(L_hidden_state[0])\n",
    "        return out\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#### TransformerLens model ####\n",
    "model = HookedTransformer.from_pretrained('EleutherAI/pythia-19m')\n",
    "hidden_size = 512\n",
    "\n",
    "# Add the classification layer\n",
    "model = Model(model, hidden_size).to('cuda')\n",
    "\n",
    "\n",
    "# Define train and test functions for the LEGO task\n",
    "train_var_pred = [i for i in range(2*n_train_var)] \n",
    "test_var_pred = [i for i in range(2*n_var)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d32cc",
   "metadata": {},
   "source": [
    "Nous pouvons regarder la structure du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c3bbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (base): HookedTransformer(\n",
      "    (embed): Embed()\n",
      "    (hook_embed): HookPoint()\n",
      "    (blocks): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (ln1): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (ln2): LayerNormPre(\n",
      "          (hook_scale): HookPoint()\n",
      "          (hook_normalized): HookPoint()\n",
      "        )\n",
      "        (attn): Attention(\n",
      "          (hook_k): HookPoint()\n",
      "          (hook_q): HookPoint()\n",
      "          (hook_v): HookPoint()\n",
      "          (hook_z): HookPoint()\n",
      "          (hook_attn_scores): HookPoint()\n",
      "          (hook_pattern): HookPoint()\n",
      "          (hook_result): HookPoint()\n",
      "          (hook_rot_k): HookPoint()\n",
      "          (hook_rot_q): HookPoint()\n",
      "        )\n",
      "        (mlp): MLP(\n",
      "          (hook_pre): HookPoint()\n",
      "          (hook_post): HookPoint()\n",
      "        )\n",
      "        (hook_attn_out): HookPoint()\n",
      "        (hook_mlp_out): HookPoint()\n",
      "        (hook_resid_pre): HookPoint()\n",
      "        (hook_resid_post): HookPoint()\n",
      "      )\n",
      "    )\n",
      "    (ln_final): LayerNormPre(\n",
      "      (hook_scale): HookPoint()\n",
      "      (hook_normalized): HookPoint()\n",
      "    )\n",
      "    (unembed): Unembed()\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb20801",
   "metadata": {},
   "source": [
    "Nous définissons deux fonctions `train` et `test` permettant respectivement d'entraîner notre modèle et de le tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81773ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# To save training information\n",
    "l_test_acc = []\n",
    "l_test_loss = []\n",
    "l_train_acc = []\n",
    "l_train_loss = []\n",
    "\n",
    "def train(print_acc=False):\n",
    "    global l_train_acc, l_train_loss\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, labels, order in trainloader:\n",
    "    \n",
    "        x = batch.cuda()\n",
    "        y = labels.cuda()\n",
    "        inv_order = order.permute(0, 2, 1).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #pred = torch.argmax(model(x), -1, keepdim = True)\n",
    "        #pred = torch.reshape(pred, (pred.shape[0], pred.shape[1], 1))\n",
    "        pred = model(x)\n",
    "        ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "\n",
    "        loss = 0\n",
    "        for idx in range(n_train_var):\n",
    "            loss += criterion(ordered_pred[:, idx], y[:, idx].float()) / len(train_var_pred)\n",
    "            loss += criterion(ordered_pred[:, idx + n_train_var], y[:, idx + n_train_var].float()) / len(train_var_pred)\n",
    "            \n",
    "            total_loss += loss.item() / len(train_var_pred)\n",
    "\n",
    "            correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "            correct[idx + n_train_var] += ((ordered_pred[:, idx + n_train_var]>0).long() == y[:, idx + n_train_var]).float().mean().item()\n",
    "        \n",
    "        total += 1\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc = [corr/total for corr in correct]\n",
    "\n",
    "    l_train_loss.append(total_loss / total)\n",
    "    l_train_acc.append(list(train_acc))\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    global l_test_acc, l_test_loss\n",
    "\n",
    "    test_acc = []\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, labels, order in testloader:\n",
    "    \n",
    "            x = batch.cuda()\n",
    "            y = labels.cuda()\n",
    "            inv_order = order.permute(0, 2, 1).cuda()\n",
    "            \n",
    "            #pred = torch.argmax(model(x), -1, keepdim = True)\n",
    "            #pred = torch.reshape(pred, (pred.shape[0], pred.shape[1], 1))\n",
    "            pred = model(x)\n",
    "            ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "            \n",
    "            for idx in test_var_pred:\n",
    "                loss = criterion(ordered_pred[:,idx], y[:, idx].float())\n",
    "                total_loss += loss.item() / len(test_var_pred)\n",
    "                correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "                          \n",
    "            total += 1\n",
    "        \n",
    "        test_acc = [corr/total for corr in correct]\n",
    "\n",
    "        l_test_loss.append(total_loss / total)\n",
    "        l_test_acc.append(list(test_acc))\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0abd608",
   "metadata": {},
   "source": [
    "Nous pouvons alors entraîner notre modèle, en affichant régulièrement les résultats obtenus. Ici, nous nous contentons de faire 1 epoch pour vérifier le bon fonctionnement du code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8021bda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch:  1\n",
      "Time elapsed: 130.047018 s\n",
      "Test loss:  0.5327521087000235\n",
      "Test acc:  \n",
      " [1.0, 0.7909999817609787, 0.6081249859184027, 0.5428749835118651, 0.5153749853372573, 0.5021249853074551, 0.49649998657405375, 0.4893749874085188, 1.0, 0.7884999800473451, 0.6092499863356352, 0.5359999863430858, 0.5109999857842922, 0.5036249840632081, 0.4994999857619405, 0.49887498524039986]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_epochs = 1\n",
    "info = 1 # Show results every info epochs.\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print('\\n Epoch: ', epoch)\n",
    "    train()\n",
    "    test()\n",
    "    scheduler.step()\n",
    "\n",
    "    print('Time elapsed: %f s' %(time.time() - start))\n",
    "    if epoch % info == 0 :\n",
    "        print(\"Test loss: \", l_test_loss[-1])\n",
    "        print(\"Test acc: \", '\\n', l_test_acc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932a24b",
   "metadata": {},
   "source": [
    "Nous pouvons également essayer d'utiliser les hooks sur ce modèle.\n",
    "\n",
    "Nous regardons par exemple ici les hooks dont le nom commence par \"blocks.0\", et affichons leurs noms et leurs tailles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e2a0bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation at hook hook_embed has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.0.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_rot_q has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_rot_k has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 8, 50, 50])\n",
      "Activation at hook blocks.0.attn.hook_pattern has shape:\n",
      "torch.Size([4, 8, 50, 50])\n",
      "Activation at hook blocks.0.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 8, 64])\n",
      "Activation at hook blocks.0.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.0.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 2048])\n",
      "Activation at hook blocks.0.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 2048])\n",
      "Activation at hook blocks.0.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook blocks.0.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 512])\n",
      "Activation at hook ln_final.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook ln_final.hook_normalized has shape:\n",
      "torch.Size([4, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "# Print activation shapes at every layer for our model\n",
    "\n",
    "embed_or_first_layer = lambda name: (name[:6] != \"blocks\" or name[:8] == \"blocks.0\")\n",
    "\n",
    "def print_shape(tensor, hook):\n",
    "    print(f\"Activation at hook {hook.name} has shape:\")\n",
    "    print(tensor.shape)\n",
    "\n",
    "random_tokens = torch.randint(1000, 10000, (4, 50))\n",
    "logits = model.base.run_with_hooks(random_tokens, fwd_hooks=[(embed_or_first_layer, print_shape)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e68f4",
   "metadata": {},
   "source": [
    "Nous sauvegardons notre modèle pour ne pas avoir à le ré-entraîner à chaque fois:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c37675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"model.pkl\", \"wb\") as file:\\n    pickle.dump(file, model)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open(\"trained_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(model, file)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
