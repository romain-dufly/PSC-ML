import sys

sys.path.insert(0, "./utils")
import numpy as np
import matplotlib.pyplot as plt
from cliffwalk import CliffWalk
import time
import sys


def timing(f):
    def wrap(*args, **kwargs):
        time1 = time.time()
        ret = f(*args, **kwargs)
        time2 = time.time()
        print(
            "{:s} function took {:.3f} ms".format(f.__name__, (time2 - time1) * 1000.0)
        )

        return ret

    return wrap


def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        policy: np.array
            matrix mapping states to action (Ns)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        value_function: np.array
            The value function of the given policy
    """
    Ns, Na = R.shape
    # ====================================================
    # YOUR IMPLEMENTATION HERE
    value_function = np.zeros(Ns)

    r = np.array([R[s, policy[s]] for s in range(Ns)])
    Ppi = np.zeros((Ns, Ns))
    for s in range(Ns):
        for s_next in range(Ns):
            Ppi[s, s_next] = P[s, policy[s], s_next]

    A = np.eye(Ns) - gamma * Ppi
    r = r[np.newaxis].T
    value_function = np.matmul(np.linalg.inv(A), r)

    # ====================================================
    return value_function


@timing
def policy_iteration(P, R, gamma=0.9, tol=1e-3):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        policy: np.array
            the final policy
        V: np.array
            the value function associated to the final policy
    """
    Ns, Na = R.shape
    V = np.zeros(Ns)
    policy = np.zeros(Ns, dtype=np.int)
    # ====================================================
    # YOUR IMPLEMENTATION HERE
    #
    new_policy = -np.ones(Ns, dtype=np.int)
    i = 0
    while not np.array_equal(new_policy, policy):
        i += 1

        policy = new_policy.copy()
        V = policy_evaluation(P, R, policy, gamma, tol)

        # greedy iteration
        for s in range(Ns):
            new_policy[s] = np.argmax(
                [
                    R[s, a] + gamma * sum([P[s, a, s_] * V[s_] for s_ in range(Ns)])
                    for a in range(Na)
                ]
            )

    print("Policy iteration terminated in ", i, " iterations.")
    # ====================================================
    return policy, V


@timing
def value_iteration(P, R, gamma=0.9, tol=1e-3):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        Q: final Q-function (at iteration n)
        greedy_policy: greedy policy wrt Qn
        Qfs: all Q-functions generated by the algorithm (for visualization)
    """
    Ns, Na = R.shape
    Q = np.zeros((Ns, Na))
    Qfs = [Q]
    # ====================================================
    # YOUR IMPLEMENTATION HERE
    #
    new_Q = -np.ones((Ns, Na))
    i = 0
    while np.max(np.abs(new_Q - Q)) > tol:
        i += 1
        Q = np.copy(new_Q)
        for s in range(Ns):
            for a in range(Na):
                new_Q[s, a] = R[s, a] + (
                    gamma
                    * sum(
                        [
                            P[s, a, s_] * max([Q[s_, a] for a in range(Na)])
                            for s_ in range(Ns)
                        ]
                    )
                )
        Qfs.append(new_Q)
    print("value_iteration terminated with ", i, " iterations")

    greedy_policy = np.argmax(Qfs[-1], axis=1)

    # ====================================================
    return Q, greedy_policy, Qfs


# Edit below to run policy and value iteration on different environments and
# visualize the resulting policies in action!
# You may change the parameters in the functions below
if __name__ == "__main__":
    tol = 1e-5
    proba_succ = 0.99
    env = CliffWalk(proba_succ=proba_succ)
    print("Probabilitu of success : ", proba_succ)
    print(env.R.shape)
    print(env.P.shape)
    env.render()

    # run value iteration to obtain Q-values
    VI_Q, VI_greedypol, all_qfunctions = value_iteration(
        env.P, env.R, gamma=env.gamma, tol=tol
    )

    # render the policy
    print("[VI]Greedy policy: ")
    env.render_policy(VI_greedypol)

    # compute the value function of the greedy policy using matrix inversion
    greedy_V = np.zeros((env.Ns, env.Na))

    # ====================================================
    # YOUR IMPLEMENTATION HERE
    # compute value function of the greedy policy
    greedy_V = policy_evaluation(env.P, env.R, VI_greedypol, env.gamma, tol)

    # ====================================================

    # show the error between the computed V-functions and the final V-function
    # (that should be the optimal one, if correctly implemented)
    # as a function of time
    norms = [np.linalg.norm(q.max(axis=1) - greedy_V) for q in all_qfunctions]
    plt.plot(norms)
    plt.xlabel("Iteration")
    plt.ylabel("Error")
    plt.title("Value iteration: convergence")

    #### POLICY ITERATION ####
    PI_policy, PI_V = policy_iteration(env.P, env.R, gamma=env.gamma, tol=tol)
    print("\n[PI]final policy: ")
    env.render_policy(PI_policy)

    # control that everything is correct
    assert np.allclose(
        PI_policy, VI_greedypol
    ), "You should check the code, the greedy policy computed by VI is not equal to the solution of PI"
    assert np.allclose(
        PI_V, greedy_V
    ), "Since the policies are equal, even the value function should be"

    # for visualizing the execution of a policy, you can use the following code
    # state = env.reset()
    # env.render()
    # for i in range(15):
    #     action = VI_greedypol[state]
    #     state, reward, done, _ = env.step(action)
    #     env.render()

    plt.show()
