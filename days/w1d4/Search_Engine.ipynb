{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ERHP2MGJPr"
      },
      "source": [
        "## Search Engine with HuggingFace\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d4/Search_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "The goal of this tutorial is to understand the basics of using the most used library in modern NLP: Huggingface.\n",
        "\n",
        "We will try to understand the first steps in creating a semantic search engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1FG8fi-XDM9"
      },
      "source": [
        "## Download the list of papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RScoyaOX3Nkn"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/EffiSciencesResearch/ML4G/raw/4010bb6ccd63dee5896b26ee3c045898e0cf9ed6/days/w1d4/keynesian_eco_ML4G.xlsx -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sFYvA03ZGq9g"
      },
      "outputs": [],
      "source": [
        "pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WE9hfjkq4t-5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"keynesian_eco_ML4G.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i4ha4_fV6M8h",
        "outputId": "8561fbac-4f8b-4b75-8a50-982ab781f4fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9aa6554a-9b47-418e-b377-75c1513d6df9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>nbInCitations</th>\n",
              "      <th>nbOutCitations</th>\n",
              "      <th>authors</th>\n",
              "      <th>paperAbstract</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>doi</th>\n",
              "      <th>doiUrl</th>\n",
              "      <th>fieldsOfStudy</th>\n",
              "      <th>journalName</th>\n",
              "      <th>magId</th>\n",
              "      <th>s2Url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c2890ae1a86d104b0b5b2cf71d890cb514b6590b</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[['2069055537', 'Robin  Boadway'], ['119227414...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fiscal Federalism: Preface</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>10.1017/CBO9780511626883.001</td>\n",
              "      <td>https://doi.org/10.1017/CBO9780511626883.001</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1597197407</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b849439367ccb03b11183b9e3fdf594aae90255a</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[['116983853', 'Nancy J. Wulwick']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Two Econometric Replications: The Historic Phi...</td>\n",
              "      <td>1996.0</td>\n",
              "      <td>10.1215/00182702-28-3-391</td>\n",
              "      <td>https://doi.org/10.1215/00182702-28-3-391</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>History of Political Economy</td>\n",
              "      <td>2007001276</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1cc40d9cc0ee8978339d55b0406cb88014710ae9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[['40266255', 'Julia M. Colston'], ['46784895'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>An adenoviral model to unlock the secrets of m...</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>10.1016/J.JINF.2013.07.009</td>\n",
              "      <td>https://doi.org/10.1016/J.JINF.2013.07.009</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>Journal of Infection</td>\n",
              "      <td>2064032475</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13bdff11c2e33df661d1bd7f08f7ab0bfe46acb6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[['119468193', 'Baldwin  Ranson']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Keynesian Revolution and its Critics</td>\n",
              "      <td>1988.0</td>\n",
              "      <td>10.1080/00213624.1988.11504824</td>\n",
              "      <td>https://doi.org/10.1080/00213624.1988.11504824</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2565556283</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6286ee0c109ac7ce25a9d8cdc15281ed4d218cfa</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[['119242604', 'Josef  Steindl']]</td>\n",
              "      <td>The control of the economy is examined in term...</td>\n",
              "      <td>The control of the economy</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>10.1007/978-1-349-20821-0_16</td>\n",
              "      <td>https://doi.org/10.1007/978-1-349-20821-0_16</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>PSL Quarterly Review</td>\n",
              "      <td>1804776193</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15293</th>\n",
              "      <td>1a68ce621afac192a4b7c3c30152fb9f210ec0d2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[['1883404', 'Christian  Fries']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heath‐Jarrow‐Morton Framework: Foundations</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>10.1002/9780470179789.CH22</td>\n",
              "      <td>https://doi.org/10.1002/9780470179789.CH22</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1499640240</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15294</th>\n",
              "      <td>c637aa731bd2423633e75067ae039ec6133e32eb</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[['47021855', 'Miriam  Smith']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discrimination, From Romer to Vriend, 1986–2000</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>10.4324/9780203895016-9</td>\n",
              "      <td>https://doi.org/10.4324/9780203895016-9</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3028705958</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15295</th>\n",
              "      <td>f6375b906fc8496a361ef1c083624424af98ad7e</td>\n",
              "      <td>50.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>[['2990724', 'Imad A. Moosa']]</td>\n",
              "      <td>Abstract Okun's coefficient is estimated from ...</td>\n",
              "      <td>Cyclical output, cyclical unemployment, and Ok...</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>10.1016/S1059-0560(99)00028-3</td>\n",
              "      <td>https://doi.org/10.1016/S1059-0560%2899%2900028-3</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>International Review of Economics &amp; Finance</td>\n",
              "      <td>2037151286</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15296</th>\n",
              "      <td>b493013938a9b18de3009164cc9c21a95acb662c</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>[['90362779', 'João  Sicsú']]</td>\n",
              "      <td>The article criticizes the main hypothesis of ...</td>\n",
              "      <td>A NEGAÇÃO DA INEFICÁCIA DA POLÍTICA MONETÁRIA:...</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>10.22456/2176-5456.10546</td>\n",
              "      <td>https://doi.org/10.22456/2176-5456.10546</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1957768946</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15297</th>\n",
              "      <td>1c8b10210168bbf948f48179cdc6227fca048e16</td>\n",
              "      <td>257.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>[['32082672', 'Mark W. Watson']]</td>\n",
              "      <td>The average length of business cycle contracti...</td>\n",
              "      <td>Business Cycle Durations and Postwar Stabiliza...</td>\n",
              "      <td>1992.0</td>\n",
              "      <td>10.3386/W4005</td>\n",
              "      <td>https://doi.org/10.3386/W4005</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2116465826</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15298 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9aa6554a-9b47-418e-b377-75c1513d6df9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9aa6554a-9b47-418e-b377-75c1513d6df9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9aa6554a-9b47-418e-b377-75c1513d6df9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             id  nbInCitations  \\\n",
              "0      c2890ae1a86d104b0b5b2cf71d890cb514b6590b            0.0   \n",
              "1      b849439367ccb03b11183b9e3fdf594aae90255a           12.0   \n",
              "2      1cc40d9cc0ee8978339d55b0406cb88014710ae9            0.0   \n",
              "3      13bdff11c2e33df661d1bd7f08f7ab0bfe46acb6            0.0   \n",
              "4      6286ee0c109ac7ce25a9d8cdc15281ed4d218cfa            4.0   \n",
              "...                                         ...            ...   \n",
              "15293  1a68ce621afac192a4b7c3c30152fb9f210ec0d2            0.0   \n",
              "15294  c637aa731bd2423633e75067ae039ec6133e32eb            0.0   \n",
              "15295  f6375b906fc8496a361ef1c083624424af98ad7e           50.0   \n",
              "15296  b493013938a9b18de3009164cc9c21a95acb662c            5.0   \n",
              "15297  1c8b10210168bbf948f48179cdc6227fca048e16          257.0   \n",
              "\n",
              "       nbOutCitations                                            authors  \\\n",
              "0                 0.0  [['2069055537', 'Robin  Boadway'], ['119227414...   \n",
              "1                 0.0                [['116983853', 'Nancy J. Wulwick']]   \n",
              "2                 0.0  [['40266255', 'Julia M. Colston'], ['46784895'...   \n",
              "3                 0.0                 [['119468193', 'Baldwin  Ranson']]   \n",
              "4                 1.0                  [['119242604', 'Josef  Steindl']]   \n",
              "...               ...                                                ...   \n",
              "15293             0.0                  [['1883404', 'Christian  Fries']]   \n",
              "15294             0.0                    [['47021855', 'Miriam  Smith']]   \n",
              "15295            22.0                     [['2990724', 'Imad A. Moosa']]   \n",
              "15296             3.0                      [['90362779', 'João  Sicsú']]   \n",
              "15297            24.0                   [['32082672', 'Mark W. Watson']]   \n",
              "\n",
              "                                           paperAbstract  \\\n",
              "0                                                    NaN   \n",
              "1                                                    NaN   \n",
              "2                                                    NaN   \n",
              "3                                                    NaN   \n",
              "4      The control of the economy is examined in term...   \n",
              "...                                                  ...   \n",
              "15293                                                NaN   \n",
              "15294                                                NaN   \n",
              "15295  Abstract Okun's coefficient is estimated from ...   \n",
              "15296  The article criticizes the main hypothesis of ...   \n",
              "15297  The average length of business cycle contracti...   \n",
              "\n",
              "                                                   title    year  \\\n",
              "0                             Fiscal Federalism: Preface  2009.0   \n",
              "1      Two Econometric Replications: The Historic Phi...  1996.0   \n",
              "2      An adenoviral model to unlock the secrets of m...  2013.0   \n",
              "3               The Keynesian Revolution and its Critics  1988.0   \n",
              "4                             The control of the economy  2013.0   \n",
              "...                                                  ...     ...   \n",
              "15293         Heath‐Jarrow‐Morton Framework: Foundations  2007.0   \n",
              "15294    Discrimination, From Romer to Vriend, 1986–2000  2008.0   \n",
              "15295  Cyclical output, cyclical unemployment, and Ok...  1999.0   \n",
              "15296  A NEGAÇÃO DA INEFICÁCIA DA POLÍTICA MONETÁRIA:...  2009.0   \n",
              "15297  Business Cycle Durations and Postwar Stabiliza...  1992.0   \n",
              "\n",
              "                                  doi  \\\n",
              "0        10.1017/CBO9780511626883.001   \n",
              "1           10.1215/00182702-28-3-391   \n",
              "2          10.1016/J.JINF.2013.07.009   \n",
              "3      10.1080/00213624.1988.11504824   \n",
              "4        10.1007/978-1-349-20821-0_16   \n",
              "...                               ...   \n",
              "15293      10.1002/9780470179789.CH22   \n",
              "15294         10.4324/9780203895016-9   \n",
              "15295   10.1016/S1059-0560(99)00028-3   \n",
              "15296        10.22456/2176-5456.10546   \n",
              "15297                   10.3386/W4005   \n",
              "\n",
              "                                                  doiUrl  fieldsOfStudy  \\\n",
              "0           https://doi.org/10.1017/CBO9780511626883.001  ['Economics']   \n",
              "1              https://doi.org/10.1215/00182702-28-3-391  ['Economics']   \n",
              "2             https://doi.org/10.1016/J.JINF.2013.07.009  ['Economics']   \n",
              "3         https://doi.org/10.1080/00213624.1988.11504824  ['Economics']   \n",
              "4           https://doi.org/10.1007/978-1-349-20821-0_16  ['Economics']   \n",
              "...                                                  ...            ...   \n",
              "15293         https://doi.org/10.1002/9780470179789.CH22  ['Economics']   \n",
              "15294            https://doi.org/10.4324/9780203895016-9  ['Economics']   \n",
              "15295  https://doi.org/10.1016/S1059-0560%2899%2900028-3  ['Economics']   \n",
              "15296           https://doi.org/10.22456/2176-5456.10546  ['Economics']   \n",
              "15297                      https://doi.org/10.3386/W4005  ['Economics']   \n",
              "\n",
              "                                       journalName       magId  s2Url  \n",
              "0                                              NaN  1597197407    NaN  \n",
              "1                     History of Political Economy  2007001276    NaN  \n",
              "2                             Journal of Infection  2064032475    NaN  \n",
              "3                                              NaN  2565556283    NaN  \n",
              "4                             PSL Quarterly Review  1804776193    NaN  \n",
              "...                                            ...         ...    ...  \n",
              "15293                                          NaN  1499640240    NaN  \n",
              "15294                                          NaN  3028705958    NaN  \n",
              "15295  International Review of Economics & Finance  2037151286    NaN  \n",
              "15296                                          NaN  1957768946    NaN  \n",
              "15297                                          NaN  2116465826    NaN  \n",
              "\n",
              "[15298 rows x 13 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7dimnfcXLoZ"
      },
      "source": [
        "## Semantic search engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWkIM4df_Cw7"
      },
      "source": [
        "Create a search engine by embedding by using https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
        "\n",
        "You can use the sentence transformers library https://www.sbert.net/\n",
        "\n",
        "Make some queries and check that it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1T3qvlE5pJm"
      },
      "outputs": [],
      "source": [
        "query = \"Poverty in the US\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYIgy3wxXOse"
      },
      "source": [
        "## Few Shot Learning: Tldr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50G379s6BXY1"
      },
      "source": [
        "Use GPT-J 6b with some prompt engineering to create tldr of the summaries\n",
        "\n",
        "You can begin with: https://huggingface.co/EleutherAI/gpt-neo-1.3B\n",
        "\n",
        "\n",
        "The first step is to create a single tldr. For that, you will use some  few-shot learning. You can use this link to create your prompt: https://github.com/EffiSciencesResearch/ML4G/blob/38f80110be0802837254c1cd888f387475c9b5fe/days/w1d4/tldr_dataset.csv\n",
        "\n",
        "\n",
        "After having created a single tldr, the aim is to add a new column in the pandas dataframe containing the tldr:\n",
        "- automate the process and make inferences by batch. Store the inferences in a new column in the dataframe.\n",
        "- Use the tqdm library to create a progress bar.\n",
        "- Notice that it is too slow and switch to GPU. To do this, use \".to(device)\" on the output tensor of the tokenizer and on the model.\n",
        "- Use the command 'nvidia-smi' in the terminal to monitor the GPU usage. Aim at a GPU usage percentage of 70%.\n",
        "- How does the speed of inference vary with the batch size?\n",
        "- How does the inference speed vary with the padding policy in the tokenizer?\n",
        "- How does the quality/speed of inference vary with the beam_search parameter?\n",
        "- Bonus: read https://huggingface.co/blog/how-to-generate\n",
        "- Bonus: Use a bigger model https://huggingface.co/EleutherAI/gpt-j-6B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpgbzkFPBWBn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DAHIUAYUwBL",
        "outputId": "4e733b30-99d5-4020-8fd8-7cd38b04ffde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method __call__ in module transformers.tokenization_utils_base:\n",
            "\n",
            "__call__(text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Union[int, NoneType] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Union[int, NoneType] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Union[bool, NoneType] = None, return_attention_mask: Union[bool, NoneType] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast instance\n",
            "    Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
            "    sequences.\n",
            "    \n",
            "    Args:\n",
            "        text (`str`, `List[str]`, `List[List[str]]`):\n",
            "            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            "            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            "            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "        text_pair (`str`, `List[str]`, `List[List[str]]`):\n",
            "            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            "            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            "            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "    \n",
            "        add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to encode the sequences with the special tokens relative to their model.\n",
            "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            "            Activates and controls padding. Accepts the following values:\n",
            "    \n",
            "            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            "              sequence if provided).\n",
            "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            "              acceptable input length for the model if that argument is not provided.\n",
            "            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            "              lengths).\n",
            "        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            "            Activates and controls truncation. Accepts the following values:\n",
            "    \n",
            "            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            "              to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            "              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            "              sequences (or a batch of pairs) is provided.\n",
            "            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
            "              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            "            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
            "              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            "            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            "              greater than the model maximum admissible input size).\n",
            "        max_length (`int`, *optional*):\n",
            "            Controls the maximum length to use by one of the truncation/padding parameters.\n",
            "    \n",
            "            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            "            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            "            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            "        stride (`int`, *optional*, defaults to 0):\n",
            "            If set to a number along with `max_length`, the overflowing tokens returned when\n",
            "            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            "            returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            "            argument defines the number of overlapping tokens.\n",
            "        is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            "            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            "            which it will tokenize. This is useful for NER or token classification.\n",
            "        pad_to_multiple_of (`int`, *optional*):\n",
            "            If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
            "            the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
            "        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            "            If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            "    \n",
            "            - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            "            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            "            - `'np'`: Return Numpy `np.ndarray` objects.\n",
            "    \n",
            "        return_token_type_ids (`bool`, *optional*):\n",
            "            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            "            the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            "    \n",
            "            [What are token type IDs?](../glossary#token-type-ids)\n",
            "        return_attention_mask (`bool`, *optional*):\n",
            "            Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            "            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            "    \n",
            "            [What are attention masks?](../glossary#attention-mask)\n",
            "        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            "            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            "            of returning overflowing tokens.\n",
            "        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return special tokens mask information.\n",
            "        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return `(char_start, char_end)` for each token.\n",
            "    \n",
            "            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            "            Python's tokenizer, this method will raise `NotImplementedError`.\n",
            "        return_length  (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the lengths of the encoded inputs.\n",
            "        verbose (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to print more information and warnings.\n",
            "        **kwargs: passed to the `self.tokenize()` method\n",
            "    \n",
            "    Return:\n",
            "        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            "    \n",
            "        - **input_ids** -- List of token ids to be fed to a model.\n",
            "    \n",
            "          [What are input IDs?](../glossary#input-ids)\n",
            "    \n",
            "        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            "          if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            "    \n",
            "          [What are token type IDs?](../glossary#token-type-ids)\n",
            "    \n",
            "        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            "          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            "    \n",
            "          [What are attention masks?](../glossary#attention-mask)\n",
            "    \n",
            "        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            "          `return_overflowing_tokens=True`).\n",
            "        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            "          `return_overflowing_tokens=True`).\n",
            "        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            "          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            "        - **length** -- The length of the inputs (when `return_length=True`)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(tokenizer.__call__)\n",
        "# You can read the doc for the following parameters:\n",
        "# text, padding, truncation, max_lenght, return_tensors\n",
        "# Then you can delete this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayj5SH3MU9Wd",
        "outputId": "346144c3-b849-488e-89c0-f83ed7719245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method generate in module transformers.generation_utils:\n",
            "\n",
            "generate(inputs: Union[torch.Tensor, NoneType] = None, max_length: Union[int, NoneType] = None, min_length: Union[int, NoneType] = None, do_sample: Union[bool, NoneType] = None, early_stopping: Union[bool, NoneType] = None, num_beams: Union[int, NoneType] = None, temperature: Union[float, NoneType] = None, top_k: Union[int, NoneType] = None, top_p: Union[float, NoneType] = None, typical_p: Union[float, NoneType] = None, repetition_penalty: Union[float, NoneType] = None, bad_words_ids: Union[Iterable[int], NoneType] = None, force_words_ids: Union[Iterable[int], Iterable[Iterable[int]], NoneType] = None, bos_token_id: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, length_penalty: Union[float, NoneType] = None, no_repeat_ngram_size: Union[int, NoneType] = None, encoder_no_repeat_ngram_size: Union[int, NoneType] = None, num_return_sequences: Union[int, NoneType] = None, max_time: Union[float, NoneType] = None, max_new_tokens: Union[int, NoneType] = None, decoder_start_token_id: Union[int, NoneType] = None, use_cache: Union[bool, NoneType] = None, num_beam_groups: Union[int, NoneType] = None, diversity_penalty: Union[float, NoneType] = None, prefix_allowed_tokens_fn: Union[Callable[[int, torch.Tensor], List[int]], NoneType] = None, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = [], renormalize_logits: Union[bool, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = [], constraints: Union[List[transformers.generation_beam_constraints.Constraint], NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, forced_bos_token_id: Union[int, NoneType] = None, forced_eos_token_id: Union[int, NoneType] = None, remove_invalid_values: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = False, exponential_decay_length_penalty: Union[Tuple[Union[int, float]], NoneType] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor] method of transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM instance\n",
            "    Generates sequences of token ids for models with a language modeling head. The method supports the following\n",
            "    generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n",
            "    \n",
            "        - *greedy decoding* by calling [`~generation_utils.GenerationMixin.greedy_search`] if `num_beams=1` and\n",
            "          `do_sample=False`.\n",
            "        - *multinomial sampling* by calling [`~generation_utils.GenerationMixin.sample`] if `num_beams=1` and\n",
            "          `do_sample=True`.\n",
            "        - *beam-search decoding* by calling [`~generation_utils.GenerationMixin.beam_search`] if `num_beams>1` and\n",
            "          `do_sample=False`.\n",
            "        - *beam-search multinomial sampling* by calling [`~generation_utils.GenerationMixin.beam_sample`] if\n",
            "          `num_beams>1` and `do_sample=True`.\n",
            "        - *diverse beam-search decoding* by calling [`~generation_utils.GenerationMixin.group_beam_search`], if\n",
            "          `num_beams>1` and `num_beam_groups>1`.\n",
            "        - *constrained beam-search decoding* by calling\n",
            "          [`~generation_utils.GenerationMixin.constrained_beam_search`], if `constraints!=None` or\n",
            "          `force_words_ids!=None`.\n",
            "    \n",
            "    <Tip warning={true}>\n",
            "    \n",
            "    Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as\n",
            "    defined in the model's config (`config.json`) which in turn defaults to the\n",
            "    [`~modeling_utils.PretrainedConfig`] of the model.\n",
            "    \n",
            "    </Tip>\n",
            "    \n",
            "    Most of these parameters are explained in more detail in [this blog\n",
            "    post](https://huggingface.co/blog/how-to-generate).\n",
            "    \n",
            "    Parameters:\n",
            "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
            "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
            "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
            "            should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
            "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
            "        max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
            "            The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n",
            "            `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in\n",
            "            the prompt.\n",
            "        max_new_tokens (`int`, *optional*):\n",
            "            The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
            "        min_length (`int`, *optional*, defaults to 10):\n",
            "            The minimum length of the sequence to be generated.\n",
            "        do_sample (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to use sampling ; use greedy decoding otherwise.\n",
            "        early_stopping (`bool`, *optional*, defaults to `False`):\n",
            "            Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
            "        num_beams (`int`, *optional*, defaults to 1):\n",
            "            Number of beams for beam search. 1 means no beam search.\n",
            "        temperature (`float`, *optional*, defaults to 1.0):\n",
            "            The value used to module the next token probabilities.\n",
            "        top_k (`int`, *optional*, defaults to 50):\n",
            "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
            "        top_p (`float`, *optional*, defaults to 1.0):\n",
            "            If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher\n",
            "            are kept for generation.\n",
            "        typical_p (`float`, *optional*, defaults to 1.0):\n",
            "            The amount of probability mass from the original distribution to be considered in typical decoding. If\n",
            "            set to 1.0 it takes no effect. See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n",
            "        repetition_penalty (`float`, *optional*, defaults to 1.0):\n",
            "            The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
            "            paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
            "        pad_token_id (`int`, *optional*):\n",
            "            The id of the *padding* token.\n",
            "        bos_token_id (`int`, *optional*):\n",
            "            The id of the *beginning-of-sequence* token.\n",
            "        eos_token_id (`int`, *optional*):\n",
            "            The id of the *end-of-sequence* token.\n",
            "        length_penalty (`float`, *optional*, defaults to 1.0):\n",
            "             Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length.\n",
            "             0.0 means no penalty. Set to values < 0.0 in order to encourage the model to generate longer\n",
            "             sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences.\n",
            "        no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
            "            If set to int > 0, all ngrams of that size can only occur once.\n",
            "        encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
            "            If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
            "            `decoder_input_ids`.\n",
            "        bad_words_ids(`List[List[int]]`, *optional*):\n",
            "            List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
            "            should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
            "            add_special_tokens=False).input_ids`.\n",
            "        force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
            "            List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple\n",
            "            list of words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`,\n",
            "            this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),\n",
            "            where one can allow different forms of each word.\n",
            "        num_return_sequences(`int`, *optional*, defaults to 1):\n",
            "            The number of independently computed returned sequences for each element in the batch.\n",
            "        max_time(`float`, *optional*):\n",
            "            The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
            "            finish the current pass after allocated time has been passed.\n",
            "        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "            Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
            "            that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
            "            as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
            "        decoder_start_token_id (`int`, *optional*):\n",
            "            If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
            "        use_cache: (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
            "            speed up decoding.\n",
            "        num_beam_groups (`int`, *optional*, defaults to 1):\n",
            "            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
            "            beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
            "        diversity_penalty (`float`, *optional*, defaults to 0.0):\n",
            "            This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
            "            at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
            "            enabled.\n",
            "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
            "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
            "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
            "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
            "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
            "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
            "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
            "        logits_processor (`LogitsProcessorList`, *optional*):\n",
            "             Custom logits processors that complement the default logits processors built from arguments and a\n",
            "             model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
            "             config an error is thrown. This feature is intended for advanced users.\n",
            "        renormalize_logits: (`bool`, *optional*, defaults to `False`):\n",
            "            Whether to renormalize the logits after applying all the logits processors or warpers (including the\n",
            "            custom ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the\n",
            "            score logits are normalized but some logit processors or warpers break the normalization.\n",
            "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            "             Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
            "             model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
            "             model's config an error is thrown. This feature is intended for advanced users.\n",
            "        constraints (`List[Constraint]`, *optional*):\n",
            "             Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
            "             of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
            "        output_attentions (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            "            returned tensors for more details.\n",
            "        output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            "            for more details.\n",
            "        output_scores (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            "        return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "        forced_bos_token_id (`int`, *optional*):\n",
            "            The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
            "            for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
            "            the target language token.\n",
            "        forced_eos_token_id (`int`, *optional*):\n",
            "            The id of the token to force as the last generated token when `max_length` is reached.\n",
            "        remove_invalid_values (`bool`, *optional*):\n",
            "            Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
            "            crash. Note that using `remove_invalid_values` can slow down generation.\n",
            "        synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            "            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            "        exponential_decay_length_penalty (`tuple(int, float)`, *optional*):\n",
            "            This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
            "            generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates\n",
            "            where penalty starts and `decay_factor` represents the factor of exponential decay\n",
            "    \n",
            "        model_kwargs:\n",
            "            Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
            "            is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
            "            should be prefixed with *decoder_*.\n",
            "    \n",
            "    Return:\n",
            "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
            "        or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
            "    \n",
            "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
            "            [`~utils.ModelOutput`] types are:\n",
            "    \n",
            "                - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
            "                - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
            "                - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
            "                - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
            "    \n",
            "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
            "            [`~utils.ModelOutput`] types are:\n",
            "    \n",
            "                - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
            "                - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
            "                - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
            "                - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
            "    \n",
            "    Examples:\n",
            "    \n",
            "    Greedy Decoding:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "    \n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
            "    >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "    \n",
            "    >>> prompt = \"Today I believe we can finally\"\n",
            "    >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
            "    \n",
            "    >>> # generate up to 30 tokens\n",
            "    >>> outputs = model.generate(input_ids, do_sample=False, max_length=30)\n",
            "    >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "    ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n']\n",
            "    ```\n",
            "    \n",
            "    Multinomial Sampling:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "    >>> import torch\n",
            "    \n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
            "    >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "    \n",
            "    >>> prompt = \"Today I believe we can finally\"\n",
            "    >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
            "    \n",
            "    >>> # sample up to 30 tokens\n",
            "    >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
            "    >>> outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
            "    >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "    ['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']\n",
            "    ```\n",
            "    \n",
            "    Beam-search decoding:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
            "    \n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
            "    >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
            "    \n",
            "    >>> sentence = \"Paris is one of the densest populated areas in Europe.\"\n",
            "    >>> input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
            "    \n",
            "    >>> outputs = model.generate(input_ids)\n",
            "    >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "    ['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\n",
            "    ```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(model.generate)\n",
        "# You can read the doc for the following parameters:\n",
        "# inputs, max_length, num_beams\n",
        "# Read the Greedy Decoding example at the end of the documentation.\n",
        "# Then you can delete this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06vZpgnyCMH3"
      },
      "source": [
        "## Quality filtering \n",
        "\n",
        "(Bonus) Implement a strategy to keep only high quality tldr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf_C63R5CKaN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITVzogt2GJPz"
      },
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "Bonus: Fine Tune T5-base from the corpus generated by  GPT-J to accelerate the inference and fine tune your first LLM\n",
        "\n",
        "Use: https://huggingface.co/docs/transformers/training#training-hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIpzL38LGJP0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoKzL-0xGJP0"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Search Engine.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
