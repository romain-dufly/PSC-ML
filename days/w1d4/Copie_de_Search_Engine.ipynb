{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5ERHP2MGJPr"
      },
      "source": [
        "## Search Engine with HuggingFace\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d4/Search_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "The goal of this tutorial is to understand the basics of using the most used library in modern NLP: Huggingface.\n",
        "\n",
        "We will try to understand the first steps in creating a semantic search engine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1FG8fi-XDM9"
      },
      "source": [
        "## Download the list of papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RScoyaOX3Nkn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' n'est pas reconnu en tant que commande interne\n",
            "ou externe, un programme ex�cutable ou un fichier de commandes.\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/EffiSciencesResearch/ML4G/raw/4010bb6ccd63dee5896b26ee3c045898e0cf9ed6/days/w1d4/keynesian_eco_ML4G.xlsx -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sFYvA03ZGq9g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WE9hfjkq4t-5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"keynesian_eco_ML4G.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i4ha4_fV6M8h",
        "outputId": "459fff0a-4871-4e2b-d60b-50d47a703776"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>nbInCitations</th>\n",
              "      <th>nbOutCitations</th>\n",
              "      <th>authors</th>\n",
              "      <th>paperAbstract</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>doi</th>\n",
              "      <th>doiUrl</th>\n",
              "      <th>fieldsOfStudy</th>\n",
              "      <th>journalName</th>\n",
              "      <th>magId</th>\n",
              "      <th>s2Url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c2890ae1a86d104b0b5b2cf71d890cb514b6590b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['2069055537', 'Robin  Boadway'], ['119227414...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fiscal Federalism: Preface</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>10.1017/CBO9780511626883.001</td>\n",
              "      <td>https://doi.org/10.1017/CBO9780511626883.001</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1597197407</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b849439367ccb03b11183b9e3fdf594aae90255a</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>[['116983853', 'Nancy J. Wulwick']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Two Econometric Replications: The Historic Phi...</td>\n",
              "      <td>1996.0</td>\n",
              "      <td>10.1215/00182702-28-3-391</td>\n",
              "      <td>https://doi.org/10.1215/00182702-28-3-391</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>History of Political Economy</td>\n",
              "      <td>2007001276</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1cc40d9cc0ee8978339d55b0406cb88014710ae9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['40266255', 'Julia M. Colston'], ['46784895'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>An adenoviral model to unlock the secrets of m...</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>10.1016/J.JINF.2013.07.009</td>\n",
              "      <td>https://doi.org/10.1016/J.JINF.2013.07.009</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>Journal of Infection</td>\n",
              "      <td>2064032475</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13bdff11c2e33df661d1bd7f08f7ab0bfe46acb6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['119468193', 'Baldwin  Ranson']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Keynesian Revolution and its Critics</td>\n",
              "      <td>1988.0</td>\n",
              "      <td>10.1080/00213624.1988.11504824</td>\n",
              "      <td>https://doi.org/10.1080/00213624.1988.11504824</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2565556283</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6286ee0c109ac7ce25a9d8cdc15281ed4d218cfa</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>[['119242604', 'Josef  Steindl']]</td>\n",
              "      <td>The control of the economy is examined in term...</td>\n",
              "      <td>The control of the economy</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>10.1007/978-1-349-20821-0_16</td>\n",
              "      <td>https://doi.org/10.1007/978-1-349-20821-0_16</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>PSL Quarterly Review</td>\n",
              "      <td>1804776193</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15293</th>\n",
              "      <td>1a68ce621afac192a4b7c3c30152fb9f210ec0d2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['1883404', 'Christian  Fries']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heath‐Jarrow‐Morton Framework: Foundations</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>10.1002/9780470179789.CH22</td>\n",
              "      <td>https://doi.org/10.1002/9780470179789.CH22</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1499640240</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15294</th>\n",
              "      <td>c637aa731bd2423633e75067ae039ec6133e32eb</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['47021855', 'Miriam  Smith']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discrimination, From Romer to Vriend, 1986–2000</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>10.4324/9780203895016-9</td>\n",
              "      <td>https://doi.org/10.4324/9780203895016-9</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3028705958</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15295</th>\n",
              "      <td>f6375b906fc8496a361ef1c083624424af98ad7e</td>\n",
              "      <td>50</td>\n",
              "      <td>22</td>\n",
              "      <td>[['2990724', 'Imad A. Moosa']]</td>\n",
              "      <td>Abstract Okun's coefficient is estimated from ...</td>\n",
              "      <td>Cyclical output, cyclical unemployment, and Ok...</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>10.1016/S1059-0560(99)00028-3</td>\n",
              "      <td>https://doi.org/10.1016/S1059-0560%2899%2900028-3</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>International Review of Economics &amp; Finance</td>\n",
              "      <td>2037151286</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15296</th>\n",
              "      <td>b493013938a9b18de3009164cc9c21a95acb662c</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>[['90362779', 'João  Sicsú']]</td>\n",
              "      <td>The article criticizes the main hypothesis of ...</td>\n",
              "      <td>A NEGAÇÃO DA INEFICÁCIA DA POLÍTICA MONETÁRIA:...</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>10.22456/2176-5456.10546</td>\n",
              "      <td>https://doi.org/10.22456/2176-5456.10546</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1957768946</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15297</th>\n",
              "      <td>1c8b10210168bbf948f48179cdc6227fca048e16</td>\n",
              "      <td>257</td>\n",
              "      <td>24</td>\n",
              "      <td>[['32082672', 'Mark W. Watson']]</td>\n",
              "      <td>The average length of business cycle contracti...</td>\n",
              "      <td>Business Cycle Durations and Postwar Stabiliza...</td>\n",
              "      <td>1992.0</td>\n",
              "      <td>10.3386/W4005</td>\n",
              "      <td>https://doi.org/10.3386/W4005</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2116465826</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15298 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             id  nbInCitations  \\\n",
              "0      c2890ae1a86d104b0b5b2cf71d890cb514b6590b              0   \n",
              "1      b849439367ccb03b11183b9e3fdf594aae90255a             12   \n",
              "2      1cc40d9cc0ee8978339d55b0406cb88014710ae9              0   \n",
              "3      13bdff11c2e33df661d1bd7f08f7ab0bfe46acb6              0   \n",
              "4      6286ee0c109ac7ce25a9d8cdc15281ed4d218cfa              4   \n",
              "...                                         ...            ...   \n",
              "15293  1a68ce621afac192a4b7c3c30152fb9f210ec0d2              0   \n",
              "15294  c637aa731bd2423633e75067ae039ec6133e32eb              0   \n",
              "15295  f6375b906fc8496a361ef1c083624424af98ad7e             50   \n",
              "15296  b493013938a9b18de3009164cc9c21a95acb662c              5   \n",
              "15297  1c8b10210168bbf948f48179cdc6227fca048e16            257   \n",
              "\n",
              "       nbOutCitations                                            authors  \\\n",
              "0                   0  [['2069055537', 'Robin  Boadway'], ['119227414...   \n",
              "1                   0                [['116983853', 'Nancy J. Wulwick']]   \n",
              "2                   0  [['40266255', 'Julia M. Colston'], ['46784895'...   \n",
              "3                   0                 [['119468193', 'Baldwin  Ranson']]   \n",
              "4                   1                  [['119242604', 'Josef  Steindl']]   \n",
              "...               ...                                                ...   \n",
              "15293               0                  [['1883404', 'Christian  Fries']]   \n",
              "15294               0                    [['47021855', 'Miriam  Smith']]   \n",
              "15295              22                     [['2990724', 'Imad A. Moosa']]   \n",
              "15296               3                      [['90362779', 'João  Sicsú']]   \n",
              "15297              24                   [['32082672', 'Mark W. Watson']]   \n",
              "\n",
              "                                           paperAbstract  \\\n",
              "0                                                    NaN   \n",
              "1                                                    NaN   \n",
              "2                                                    NaN   \n",
              "3                                                    NaN   \n",
              "4      The control of the economy is examined in term...   \n",
              "...                                                  ...   \n",
              "15293                                                NaN   \n",
              "15294                                                NaN   \n",
              "15295  Abstract Okun's coefficient is estimated from ...   \n",
              "15296  The article criticizes the main hypothesis of ...   \n",
              "15297  The average length of business cycle contracti...   \n",
              "\n",
              "                                                   title    year  \\\n",
              "0                             Fiscal Federalism: Preface  2009.0   \n",
              "1      Two Econometric Replications: The Historic Phi...  1996.0   \n",
              "2      An adenoviral model to unlock the secrets of m...  2013.0   \n",
              "3               The Keynesian Revolution and its Critics  1988.0   \n",
              "4                             The control of the economy  2013.0   \n",
              "...                                                  ...     ...   \n",
              "15293         Heath‐Jarrow‐Morton Framework: Foundations  2007.0   \n",
              "15294    Discrimination, From Romer to Vriend, 1986–2000  2008.0   \n",
              "15295  Cyclical output, cyclical unemployment, and Ok...  1999.0   \n",
              "15296  A NEGAÇÃO DA INEFICÁCIA DA POLÍTICA MONETÁRIA:...  2009.0   \n",
              "15297  Business Cycle Durations and Postwar Stabiliza...  1992.0   \n",
              "\n",
              "                                  doi  \\\n",
              "0        10.1017/CBO9780511626883.001   \n",
              "1           10.1215/00182702-28-3-391   \n",
              "2          10.1016/J.JINF.2013.07.009   \n",
              "3      10.1080/00213624.1988.11504824   \n",
              "4        10.1007/978-1-349-20821-0_16   \n",
              "...                               ...   \n",
              "15293      10.1002/9780470179789.CH22   \n",
              "15294         10.4324/9780203895016-9   \n",
              "15295   10.1016/S1059-0560(99)00028-3   \n",
              "15296        10.22456/2176-5456.10546   \n",
              "15297                   10.3386/W4005   \n",
              "\n",
              "                                                  doiUrl  fieldsOfStudy  \\\n",
              "0           https://doi.org/10.1017/CBO9780511626883.001  ['Economics']   \n",
              "1              https://doi.org/10.1215/00182702-28-3-391  ['Economics']   \n",
              "2             https://doi.org/10.1016/J.JINF.2013.07.009  ['Economics']   \n",
              "3         https://doi.org/10.1080/00213624.1988.11504824  ['Economics']   \n",
              "4           https://doi.org/10.1007/978-1-349-20821-0_16  ['Economics']   \n",
              "...                                                  ...            ...   \n",
              "15293         https://doi.org/10.1002/9780470179789.CH22  ['Economics']   \n",
              "15294            https://doi.org/10.4324/9780203895016-9  ['Economics']   \n",
              "15295  https://doi.org/10.1016/S1059-0560%2899%2900028-3  ['Economics']   \n",
              "15296           https://doi.org/10.22456/2176-5456.10546  ['Economics']   \n",
              "15297                      https://doi.org/10.3386/W4005  ['Economics']   \n",
              "\n",
              "                                       journalName       magId  s2Url  \n",
              "0                                              NaN  1597197407    NaN  \n",
              "1                     History of Political Economy  2007001276    NaN  \n",
              "2                             Journal of Infection  2064032475    NaN  \n",
              "3                                              NaN  2565556283    NaN  \n",
              "4                             PSL Quarterly Review  1804776193    NaN  \n",
              "...                                            ...         ...    ...  \n",
              "15293                                          NaN  1499640240    NaN  \n",
              "15294                                          NaN  3028705958    NaN  \n",
              "15295  International Review of Economics & Finance  2037151286    NaN  \n",
              "15296                                          NaN  1957768946    NaN  \n",
              "15297                                          NaN  2116465826    NaN  \n",
              "\n",
              "[15298 rows x 13 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7dimnfcXLoZ"
      },
      "source": [
        "## Semantic search engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWkIM4df_Cw7"
      },
      "source": [
        "Create a search engine by embedding by using https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
        "\n",
        "You can use the sentence transformers library https://www.sbert.net/\n",
        "\n",
        "Make some queries and check that it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-vmhgq-JjmMv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install -U sentence-transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y1T3qvlE5pJm",
        "outputId": "2cfd0551-f0d6-44bc-86ce-2fcd4d0f3b27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\NewRi\\anaconda3\\envs\\PSC\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>nbInCitations</th>\n",
              "      <th>nbOutCitations</th>\n",
              "      <th>authors</th>\n",
              "      <th>paperAbstract</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "      <th>doi</th>\n",
              "      <th>doiUrl</th>\n",
              "      <th>fieldsOfStudy</th>\n",
              "      <th>journalName</th>\n",
              "      <th>magId</th>\n",
              "      <th>s2Url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c2890ae1a86d104b0b5b2cf71d890cb514b6590b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['2069055537', 'Robin  Boadway'], ['119227414...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Fiscal Federalism: Preface</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>10.1017/CBO9780511626883.001</td>\n",
              "      <td>https://doi.org/10.1017/CBO9780511626883.001</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1597197407</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b849439367ccb03b11183b9e3fdf594aae90255a</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>[['116983853', 'Nancy J. Wulwick']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Two Econometric Replications: The Historic Phi...</td>\n",
              "      <td>1996.0</td>\n",
              "      <td>10.1215/00182702-28-3-391</td>\n",
              "      <td>https://doi.org/10.1215/00182702-28-3-391</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>History of Political Economy</td>\n",
              "      <td>2007001276</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1cc40d9cc0ee8978339d55b0406cb88014710ae9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['40266255', 'Julia M. Colston'], ['46784895'...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>An adenoviral model to unlock the secrets of m...</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>10.1016/J.JINF.2013.07.009</td>\n",
              "      <td>https://doi.org/10.1016/J.JINF.2013.07.009</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>Journal of Infection</td>\n",
              "      <td>2064032475</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13bdff11c2e33df661d1bd7f08f7ab0bfe46acb6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['119468193', 'Baldwin  Ranson']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>The Keynesian Revolution and its Critics</td>\n",
              "      <td>1988.0</td>\n",
              "      <td>10.1080/00213624.1988.11504824</td>\n",
              "      <td>https://doi.org/10.1080/00213624.1988.11504824</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2565556283</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6286ee0c109ac7ce25a9d8cdc15281ed4d218cfa</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>[['119242604', 'Josef  Steindl']]</td>\n",
              "      <td>The control of the economy is examined in term...</td>\n",
              "      <td>The control of the economy</td>\n",
              "      <td>2013.0</td>\n",
              "      <td>10.1007/978-1-349-20821-0_16</td>\n",
              "      <td>https://doi.org/10.1007/978-1-349-20821-0_16</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>PSL Quarterly Review</td>\n",
              "      <td>1804776193</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15293</th>\n",
              "      <td>1a68ce621afac192a4b7c3c30152fb9f210ec0d2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['1883404', 'Christian  Fries']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heath‐Jarrow‐Morton Framework: Foundations</td>\n",
              "      <td>2007.0</td>\n",
              "      <td>10.1002/9780470179789.CH22</td>\n",
              "      <td>https://doi.org/10.1002/9780470179789.CH22</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1499640240</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15294</th>\n",
              "      <td>c637aa731bd2423633e75067ae039ec6133e32eb</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[['47021855', 'Miriam  Smith']]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Discrimination, From Romer to Vriend, 1986–2000</td>\n",
              "      <td>2008.0</td>\n",
              "      <td>10.4324/9780203895016-9</td>\n",
              "      <td>https://doi.org/10.4324/9780203895016-9</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3028705958</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15295</th>\n",
              "      <td>f6375b906fc8496a361ef1c083624424af98ad7e</td>\n",
              "      <td>50</td>\n",
              "      <td>22</td>\n",
              "      <td>[['2990724', 'Imad A. Moosa']]</td>\n",
              "      <td>Abstract Okun's coefficient is estimated from ...</td>\n",
              "      <td>Cyclical output, cyclical unemployment, and Ok...</td>\n",
              "      <td>1999.0</td>\n",
              "      <td>10.1016/S1059-0560(99)00028-3</td>\n",
              "      <td>https://doi.org/10.1016/S1059-0560%2899%2900028-3</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>International Review of Economics &amp; Finance</td>\n",
              "      <td>2037151286</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15296</th>\n",
              "      <td>b493013938a9b18de3009164cc9c21a95acb662c</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>[['90362779', 'João  Sicsú']]</td>\n",
              "      <td>The article criticizes the main hypothesis of ...</td>\n",
              "      <td>A NEGAÇÃO DA INEFICÁCIA DA POLÍTICA MONETÁRIA:...</td>\n",
              "      <td>2009.0</td>\n",
              "      <td>10.22456/2176-5456.10546</td>\n",
              "      <td>https://doi.org/10.22456/2176-5456.10546</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1957768946</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15297</th>\n",
              "      <td>1c8b10210168bbf948f48179cdc6227fca048e16</td>\n",
              "      <td>257</td>\n",
              "      <td>24</td>\n",
              "      <td>[['32082672', 'Mark W. Watson']]</td>\n",
              "      <td>The average length of business cycle contracti...</td>\n",
              "      <td>Business Cycle Durations and Postwar Stabiliza...</td>\n",
              "      <td>1992.0</td>\n",
              "      <td>10.3386/W4005</td>\n",
              "      <td>https://doi.org/10.3386/W4005</td>\n",
              "      <td>['Economics']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2116465826</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15298 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             id  nbInCitations  \\\n",
              "0      c2890ae1a86d104b0b5b2cf71d890cb514b6590b              0   \n",
              "1      b849439367ccb03b11183b9e3fdf594aae90255a             12   \n",
              "2      1cc40d9cc0ee8978339d55b0406cb88014710ae9              0   \n",
              "3      13bdff11c2e33df661d1bd7f08f7ab0bfe46acb6              0   \n",
              "4      6286ee0c109ac7ce25a9d8cdc15281ed4d218cfa              4   \n",
              "...                                         ...            ...   \n",
              "15293  1a68ce621afac192a4b7c3c30152fb9f210ec0d2              0   \n",
              "15294  c637aa731bd2423633e75067ae039ec6133e32eb              0   \n",
              "15295  f6375b906fc8496a361ef1c083624424af98ad7e             50   \n",
              "15296  b493013938a9b18de3009164cc9c21a95acb662c              5   \n",
              "15297  1c8b10210168bbf948f48179cdc6227fca048e16            257   \n",
              "\n",
              "       nbOutCitations                                            authors  \\\n",
              "0                   0  [['2069055537', 'Robin  Boadway'], ['119227414...   \n",
              "1                   0                [['116983853', 'Nancy J. Wulwick']]   \n",
              "2                   0  [['40266255', 'Julia M. Colston'], ['46784895'...   \n",
              "3                   0                 [['119468193', 'Baldwin  Ranson']]   \n",
              "4                   1                  [['119242604', 'Josef  Steindl']]   \n",
              "...               ...                                                ...   \n",
              "15293               0                  [['1883404', 'Christian  Fries']]   \n",
              "15294               0                    [['47021855', 'Miriam  Smith']]   \n",
              "15295              22                     [['2990724', 'Imad A. Moosa']]   \n",
              "15296               3                      [['90362779', 'João  Sicsú']]   \n",
              "15297              24                   [['32082672', 'Mark W. Watson']]   \n",
              "\n",
              "                                           paperAbstract  \\\n",
              "0                                                    NaN   \n",
              "1                                                    NaN   \n",
              "2                                                    NaN   \n",
              "3                                                    NaN   \n",
              "4      The control of the economy is examined in term...   \n",
              "...                                                  ...   \n",
              "15293                                                NaN   \n",
              "15294                                                NaN   \n",
              "15295  Abstract Okun's coefficient is estimated from ...   \n",
              "15296  The article criticizes the main hypothesis of ...   \n",
              "15297  The average length of business cycle contracti...   \n",
              "\n",
              "                                                   title    year  \\\n",
              "0                             Fiscal Federalism: Preface  2009.0   \n",
              "1      Two Econometric Replications: The Historic Phi...  1996.0   \n",
              "2      An adenoviral model to unlock the secrets of m...  2013.0   \n",
              "3               The Keynesian Revolution and its Critics  1988.0   \n",
              "4                             The control of the economy  2013.0   \n",
              "...                                                  ...     ...   \n",
              "15293         Heath‐Jarrow‐Morton Framework: Foundations  2007.0   \n",
              "15294    Discrimination, From Romer to Vriend, 1986–2000  2008.0   \n",
              "15295  Cyclical output, cyclical unemployment, and Ok...  1999.0   \n",
              "15296  A NEGAÇÃO DA INEFICÁCIA DA POLÍTICA MONETÁRIA:...  2009.0   \n",
              "15297  Business Cycle Durations and Postwar Stabiliza...  1992.0   \n",
              "\n",
              "                                  doi  \\\n",
              "0        10.1017/CBO9780511626883.001   \n",
              "1           10.1215/00182702-28-3-391   \n",
              "2          10.1016/J.JINF.2013.07.009   \n",
              "3      10.1080/00213624.1988.11504824   \n",
              "4        10.1007/978-1-349-20821-0_16   \n",
              "...                               ...   \n",
              "15293      10.1002/9780470179789.CH22   \n",
              "15294         10.4324/9780203895016-9   \n",
              "15295   10.1016/S1059-0560(99)00028-3   \n",
              "15296        10.22456/2176-5456.10546   \n",
              "15297                   10.3386/W4005   \n",
              "\n",
              "                                                  doiUrl  fieldsOfStudy  \\\n",
              "0           https://doi.org/10.1017/CBO9780511626883.001  ['Economics']   \n",
              "1              https://doi.org/10.1215/00182702-28-3-391  ['Economics']   \n",
              "2             https://doi.org/10.1016/J.JINF.2013.07.009  ['Economics']   \n",
              "3         https://doi.org/10.1080/00213624.1988.11504824  ['Economics']   \n",
              "4           https://doi.org/10.1007/978-1-349-20821-0_16  ['Economics']   \n",
              "...                                                  ...            ...   \n",
              "15293         https://doi.org/10.1002/9780470179789.CH22  ['Economics']   \n",
              "15294            https://doi.org/10.4324/9780203895016-9  ['Economics']   \n",
              "15295  https://doi.org/10.1016/S1059-0560%2899%2900028-3  ['Economics']   \n",
              "15296           https://doi.org/10.22456/2176-5456.10546  ['Economics']   \n",
              "15297                      https://doi.org/10.3386/W4005  ['Economics']   \n",
              "\n",
              "                                       journalName       magId  s2Url  \n",
              "0                                              NaN  1597197407    NaN  \n",
              "1                     History of Political Economy  2007001276    NaN  \n",
              "2                             Journal of Infection  2064032475    NaN  \n",
              "3                                              NaN  2565556283    NaN  \n",
              "4                             PSL Quarterly Review  1804776193    NaN  \n",
              "...                                            ...         ...    ...  \n",
              "15293                                          NaN  1499640240    NaN  \n",
              "15294                                          NaN  3028705958    NaN  \n",
              "15295  International Review of Economics & Finance  2037151286    NaN  \n",
              "15296                                          NaN  1957768946    NaN  \n",
              "15297                                          NaN  2116465826    NaN  \n",
              "\n",
              "[15298 rows x 13 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "'''embeddings = model.encode(sentences)\n",
        "print(embeddings)'''\n",
        "\n",
        "'''from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "def embedding_sentences(sentences, tokenizer, model):\n",
        "  # Tokenize sentences\n",
        "  encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "  # Compute token embeddings\n",
        "  with torch.no_grad():\n",
        "      model_output = model(**encoded_input)\n",
        "  # Perform pooling\n",
        "  sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "  # Normalize embeddings\n",
        "  sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "  return sentence_embeddings'''\n",
        "\n",
        "query = \"Econometrics\"\n",
        "titres = df['title']\n",
        "\n",
        "'''# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')'''\n",
        "\n",
        "titres_embeddings = model.encode(titres)\n",
        "query_embeddings = model.encode(query)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-YUFvU_rl43",
        "outputId": "2e7b85a6-2b1c-4ea9-ef15-ba7a1485430a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Résultat 1 : Essays on applied econometric analysis related to macroeconomics\n",
            "Résultat 2 : Criticism of econometrics: Keynes, Leamer, Lucas and the Austrians: Exposing the Limitations and Abuses of Econometrics\n",
            "Résultat 3 : The Southampton econometric model\n",
            "Résultat 4 : Advances in Economics and Econometrics: Miscellaneous Endmatter\n",
            "Résultat 5 : Econometric Methods in Macroeconomics: a critical appraisal\n",
            "Résultat 6 : The role of econometrics in a radical methodology\n",
            "Résultat 7 : Keeping the ECON in Econometrics: (Micro-)Econometrics in the Journal of Political Economy\n",
            "Résultat 8 : Time-Series Econometrics in Macroeconomics and Finance\n",
            "Résultat 9 : A.W.H. Phillips's influence on econometrics\n",
            "Résultat 10 : Bårdsen, Gunnar; Eitrheim, Øyvind; Jansen, Eilev S. and Nymoen, Ragnar: \"The Econometrics of Macroeconomic Modelling\"\n",
            "Résultat 11 : The Vatican Conferences of October 7–13, 1963Controversies over the Neutrality of Econometric Modeling\n",
            "Résultat 12 : Essays in Empirical Macroeconomics\n",
            "Résultat 13 : Essays in empirical macroeconomics\n",
            "Résultat 14 : Essays in Empirical Macroeconomics\n",
            "Résultat 15 : Essays in empirical macroeconomics\n",
            "Résultat 16 : An Introduction to Applied Macroeconomics.\n",
            "Résultat 17 : The Econometrics of the New Keynesian Policy Model: Introduction\n",
            "Résultat 18 : KEYNES ON ECONOMETRICS\n",
            "Résultat 19 : Empirical economics: A replication crisis in the making?\n",
            "Résultat 20 : Empirical Studies in Macroeconomics\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "#print(titres_embeddings.shape)\n",
        "\n",
        "similarities = [util.cos_sim(title, query_embeddings).item() for title in titres_embeddings]\n",
        "#print(similarities)\n",
        "\n",
        "df['ccs.sim'] = similarities\n",
        "new_df = df.sort_values('ccs.sim', ascending=False)\n",
        "\n",
        "n = 20\n",
        "top_n = new_df['title'][:n]\n",
        "\n",
        "i = 0\n",
        "for title in top_n:\n",
        "  i += 1\n",
        "  print(f\"Résultat {i} : {title}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYIgy3wxXOse"
      },
      "source": [
        "## Few Shot Learning: Tldr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50G379s6BXY1"
      },
      "source": [
        "Use GPT-J 6b with some prompt engineering to create tldr of the summaries\n",
        "\n",
        "You can begin with: https://huggingface.co/EleutherAI/gpt-neo-1.3B\n",
        "\n",
        "\n",
        "The first step is to create a single tldr. For that, you will use some  few-shot learning. You can use this link to create your prompt: https://github.com/EffiSciencesResearch/ML4G/blob/38f80110be0802837254c1cd888f387475c9b5fe/days/w1d4/tldr_dataset.csv\n",
        "\n",
        "\n",
        "After having created a single tldr, the aim is to add a new column in the pandas dataframe containing the tldr:\n",
        "- automate the process and make inferences by batch. Store the inferences in a new column in the dataframe.\n",
        "- Use the tqdm library to create a progress bar.\n",
        "- Notice that it is too slow and switch to GPU. To do this, use \".to(device)\" on the output tensor of the tokenizer and on the model.\n",
        "- Use the command 'nvidia-smi' in the terminal to monitor the GPU usage. Aim at a GPU usage percentage of 70%. -> pour ne pas crash\n",
        "- How does the speed of inference vary with the batch size? -> V.U. = log(batch_size)\n",
        "- How does the inference speed vary with the padding policy in the tokenizer?\n",
        "- How does the quality/speed of inference vary with the beam_search parameter? -> si top_k avec k plus grand, la qualité augmente mais la vitesse décroit.\n",
        "- Bonus: read https://huggingface.co/blog/how-to-generate\n",
        "- Bonus: Use a bigger model https://huggingface.co/EleutherAI/gpt-j-6B\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FoJU145LH-O",
        "outputId": "43d249a9-c6ae-44c1-970a-25bcef5545d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The control of the economy : The control of the economy is examined in terms of the relationships between instruments and targets. The endogeneity of the budget deficit is analysed, exploring 'reflation' as a possible solution, and it is shown that monetarism has changed people's response patterns. The mechanical approach to economic management, and particularly inflation, is rejected in favour of political solutions and long-term planning.\n",
            "The Sraffian supermultiplier as an alternative closure for heterodox growth theory : This paper aims to show that the Sraffian supermultiplier model provides an alternative closure for the heterodox analysis of economic growth. The new closure follows from the assumption of the existence of autonomous non-capacity-creating expenditures, which implies that the ratio of the average to the marginal propensity to save is an endogenous variable whose determination allows the marginal propensity to invest to determine the saving ratio without the need for changes in income distribution. Provided it is also assumed that capitalist competition leads to gradual changes in the marginal propensity to invest in order to adjust productive capacity to demand, the new closure (in contrast to the Cambridge and neo-Kaleckian closures) allows us to reconcile demand-led growth, exogenous distribution, and a tendency towards normal capacity utilization.\n",
            "The Local Income and Employment Impact of Lancaster University : The paper presents the results of an analysis, using Keynesian local open economy multipliers, of the impact of Lancaster University upon income and employment in the local city and surrounding hinterland area. The results of the University impact analysis are contrasted with similar analyses of Heysham 2 nuclear power station and two local manufacturing firms. The results of the impact analysis are used in a speculative manner to consider the effects of a planned expansion of student numbers at Lancaster in the 1990s.\n",
            "Potential output and inflation dynamics after the Great Recession : Ever since the end of the Great Recession, the US economy has experienced a period of mild inflation, which contradicts with the output–inflation relationship depicted by a traditional Phillips curve. This paper examines how the permanent output loss during the Great Recession has affected the ability of the Phillips curve to explain US inflation dynamics. We find great similarity among several established trend–cycle decomposition methods: potential output declined substantially after the Great Recession. Due to the fact that a lower level of potential output implies a lesser deflationary pressure, we then show that the Phillips curve does predict a period of mild inflation. This finding is largely consistent with the observed data.\n",
            "Stockbroking in the Nineties : In this concluding chapter I would like first to draw together the conclusions that have been reached in the book so far and then to take a more imaginative view of the longer term.\n",
            "Moeda endógena e progresso tecnológico induzido num modelo macrodinâmico pós-keynesiano : This article intend to analyze the process of the economic growth and income distribution in an environment where technological progress and the basic rate of interest are endogenous and the money, via credit offer, has a preponderant role in the determination of economic dynamics. In this context, we demonstrate several combinations where happen multiple equilibriums and soften flotation in the main relevant variables. Besides, for low salary portions, we observed the prevalence of a profit-led accumulation regime and, for high portions, a wage-led accumulation regime.\n",
            "ETLA macro model for forecasting and policy simulations : Abstract This paper presents a review of a quarterly macroeconomic model built for forecasting and policy simulation purposes at the Research Institute of the Finnish Economy (ETLA). The ETLA model can be labelled as a structural econometric macro model (also known as “SEM” or “policy model” in the recent literature). The ETLA model constitutes of 81 endogenous and 70 exogenous variables and hence at this stage, it is relatively small in size. The model encompasses Keynesian features in the short run, albeit particular attention is paid to its long-term equilibrium properties which are defined from supply side. Owing to these characteristics, its adjustment to external/policy shocks resembles the behavior of New Keynesian DSGE models with sticky prices and wages. The agents of the model are partly forward-looking.\n",
            "Moving On: Where To? : The General Theory of Employment, Interest and Money was published seventy years ago. That is at least three or four intellectual generations ago. (I am not sure how to count in this context!) When I wrote about Keynes forty years ago, I did not expect to have much of an audience; to a graduate student in the 1960s, the origin of Keynesian economics seemed already a dated topic. It was a surprise to find that the profession was not tired of the debate. Since then, the Keynes literature has grown enormously. Today, the economics profession at large is tired of it. Those of us who carry on with it are talking only to each other. And that is an aging audience with not that many up-and-coming members.\n",
            "An Empirical Research on the Money-Supply Effect of Inflation Expectation When Managing Inflation Expectation in China - Based on Cagan Model and Lucas Microeconomic Rational Expectation Equation : Today, every country's economic policies now are causing inflation and strengthening Inflation Expectation. Without doubt, comprehending appropriately Inflation Expectation effects, especial its money-supply effect, is the important basis to manage Inflation Expectation and suppress inflation. The paper first introduces briefly rational expectation and its econometric expression; gives monetary supply's response to inflation expected by rational expectation by the model combined Cagan model and Lucas microeconomic rational expectation equation; points out that estimating and researching the mechanism of Inflation Expectation in China and its influence to monetary supply when managing Inflation Expectation is crucially necessary for achieve the goal of the economic policy.\n",
            "The Parties' Flip-Flops on Deficit Spending: Economics or Politics? : Not long ago, Republicans were trying to pass a balanced budget amendment to the constitution. Democrats were skeptical, overwhelmingly Keynesian, and believed that deficit spending had ended the Great Depression. Under Rubinomics the positions began to switch: Democrats became the defenders of fiscal orthodoxy. Now Bush has cut taxes for the rich and caused huge deficits. Is the flip-flop just politics?\n"
          ]
        }
      ],
      "source": [
        "abstracts = df[['paperAbstract', 'title']].dropna()\n",
        "list_abstracts = abstracts.values.tolist()\n",
        "#nan = list_abstracts[0][0]\n",
        "#print(list_abstracts[:50])\n",
        "for abstract, titre in list_abstracts[:10]:\n",
        "  #if abstract != '':\n",
        "    print(f\"{titre} : {abstract}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RpgbzkFPBWBn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "models = [\"EleutherAI/gpt-neo-1.3B\", \"EleutherAI/gpt-j-6B\"]\n",
        "i = 0\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(models[i])\n",
        "model = AutoModelForCausalLM.from_pretrained(models[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "abstracts = [\n",
        "    \"Background: Worldwide demand for SARS-CoV-2 RT-PCR testing is increasing as more countries are impacted by COVID-19 and as testing remains central to contain the spread of the disease, both in countries where the disease is emerging and in countries that are past the first wave but exposed to re-emergence. Group testing has been proposed as a solution to expand testing capabilities but sensitivity concerns have limited its impact on the management of the pandemic. Digital PCR (RT-dPCR) has been shown to be more sensitive than RT-PCR and could help in this context. Methods: We implemented RT-dPCR based COVID-19 group testing on commercially available system and assay (Naica System from Stilla Technologies) and investigated the sensitivity of the method in real life conditions of a university hospital in Paris, France, in May 2020. We tested the protocol in a direct comparison with reference RT-PCR testing on 448 samples split into groups of 3 sizes for RT-dPCR analysis: 56 groups of 8 samples, 28 groups of 16 samples and 14 groups of 32 samples. Results: Individual RT-PCR testing identified 25 positive samples. Using groups of 8, testing by RT-dPCR identified 23 groups as positive, corresponding to 26 true positive samples including 2 samples not initially detected by individual RT-PCR but confirmed positive by further RT-PCR and RT-dPCR investigation. For groups of 16, 15 groups tested positive, corresponding to 25 true positive samples identified. 100% concordance is found for groups of 32 but with limited data points.\",\n",
        "    \"The lack of efficient mass testing tools for SARS-CoV-2 virus that causes Covid-19 has contributed to the accelerated spread of the epidemic. Infected people are unaware that they are spreading the disease during the incubation period as well as in asymptomatic cases or cases with mild symptoms. To limit the number of victims of the epidemic, the strategy adopted by most affected countries is therefore social distancing or complete lockdown, a strategy that can only be beneficial for a limited time, given its economic and social cost. Today, the most feasible way out of the stalemate requires widespread screening of the population. Such screening would make it possible to isolate infected people and allow others to leave the lockdown. However, production capacity for SARS-CoV-2 tests is limited. Although production is increasing, it will not allow for sufficiently systematic and frequent screening to permit the lifting of health restrictions. We here describe how the usefulness of each test can be amplified by applying it to the mixture of samples from several individuals. This technique, called group testing, has already been successfully applied on SARS-CoV-2. We show how the group-test method must be calibrated to maximize the usefulness of each available test.\",\n",
        "    \"We study the impact of manipulating the attention of a decision-maker who learns sequentially about a number of items before making a choice. Under natural assumptions on the decision‐maker's strategy, directing attention toward one item increases its likelihood of being chosen regardless of its value. This result applies when the decision‐maker can reject all items in favor of an outside option with known value; if no outside option is available, the direction of the effect of manipulation depends on the value of the item. A similar result applies to manipulation of choices in bandit problems.\"\n",
        "]\n",
        "\n",
        "tldrs = [\n",
        "    \"RT-dPCR based COVID-19 group testing on commercially available system and assay and investigated the sensitivity of the method in real life conditions of a university hospital in Paris, France, in May 2020 are implemented.\",\n",
        "    \"It is shown how the usefulness of each test can be amplified by applying it to the mixture of samples from several individuals, called group testing, which has already been successfully applied on SARS-CoV-2.\"\n",
        "]\n",
        "\n",
        "docString = \"\"\n",
        "\n",
        "for i in range(len(tldrs)):\n",
        "    docString = docString + abstracts[i] + \"\\n tldr: \" + tldrs[i] + \"\\n\"\n",
        "docString = docString + abstracts[-1] + \"\\n tldr: \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from tqdm.autonotebook import tqdm\n",
        "os.system('nvidia-smi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [14:15<00:00, 42.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Background: Worldwide demand for SARS-CoV-2 RT-PCR testing is increasing as more countries are impacted by COVID-19 and as testing remains central to contain the spread of the disease, both in countries where the disease is emerging and in countries that are past the first wave but exposed to re-emergence. Group testing has been proposed as a solution to expand testing capabilities but sensitivity concerns have limited its impact on the management of the pandemic. Digital PCR (RT-dPCR) has been shown to be more sensitive than RT-PCR and could help in this context. Methods: We implemented RT-dPCR based COVID-19 group testing on commercially available system and assay (Naica System from Stilla Technologies) and investigated the sensitivity of the method in real life conditions of a university hospital in Paris, France, in May 2020. We tested the protocol in a direct comparison with reference RT-PCR testing on 448 samples split into groups of 3 sizes for RT-dPCR analysis: 56 groups of 8 samples, 28 groups of 16 samples and 14 groups of 32 samples. Results: Individual RT-PCR testing identified 25 positive samples. Using groups of 8, testing by RT-dPCR identified 23 groups as positive, corresponding to 26 true positive samples including 2 samples not initially detected by individual RT-PCR but confirmed positive by further RT-PCR and RT-dPCR investigation. For groups of 16, 15 groups tested positive, corresponding to 25 true positive samples identified. 100% concordance is found for groups of 32 but with limited data points.\n",
            " tldr: RT-dPCR based COVID-19 group testing on commercially available system and assay and investigated the sensitivity of the method in real life conditions of a university hospital in Paris, France, in May 2020 are implemented.\n",
            "The lack of efficient mass testing tools for SARS-CoV-2 virus that causes Covid-19 has contributed to the accelerated spread of the epidemic. Infected people are unaware that they are spreading the disease during the incubation period as well as in asymptomatic cases or cases with mild symptoms. To limit the number of victims of the epidemic, the strategy adopted by most affected countries is therefore social distancing or complete lockdown, a strategy that can only be beneficial for a limited time, given its economic and social cost. Today, the most feasible way out of the stalemate requires widespread screening of the population. Such screening would make it possible to isolate infected people and allow others to leave the lockdown. However, production capacity for SARS-CoV-2 tests is limited. Although production is increasing, it will not allow for sufficiently systematic and frequent screening to permit the lifting of health restrictions. We here describe how the usefulness of each test can be amplified by applying it to the mixture of samples from several individuals. This technique, called group testing, has already been successfully applied on SARS-CoV-2. We show how the group-test method must be calibrated to maximize the usefulness of each available test.\n",
            " tldr: It is shown how the usefulness of each test can be amplified by applying it to the mixture of samples from several individuals, called group testing, which has already been successfully applied on SARS-CoV-2.\n",
            "We study the impact of manipulating the attention of a decision-maker who learns sequentially about a number of items before making a choice. Under natural assumptions on the decision‐maker's strategy, directing attention toward one item increases its likelihood of being chosen regardless of its value. This result applies when the decision‐maker can reject all items in favor of an outside option with known value; if no outside option is available, the direction of the effect of manipulation depends on the value of the item. A similar result applies to manipulation of choices in bandit problems.\n",
            " tldr:  We study the impact of manipulating the attention of a decision-maker who learns sequentially about a number of items before making a choice. Under natural assumptions on the decision-maker's strategy, directing attention toward one item increases its likelihood of being chosen regardless of its value. This result applies when the decision-maker can reject all items in favor of an outside option with known value; if no outside option is available, the direction of the effect of manipulation depends on the value of the item. A similar result applies to manipulation of choices in bandit problems.\n",
            "\n",
            "1. Introduction {#sec1-viruses-12-00172}\n",
            "===============\n",
            "\n",
            "The COVID-19 pandemic has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has led to a global lockdown, which has been followed by a global economic crisis. The economic crisis has\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tldr_len = 20\n",
        "\n",
        "os.system('nvidia-smi')\n",
        "for _ in tqdm(range(tldr_len)):\n",
        "    input_toks = tokenizer(docString, return_tensors='pt')\n",
        "    input_ids = input_toks.input_ids\n",
        "    output_ids = model.generate(input_ids, pad_token_id=50256, eos_token_id=50256, max_new_tokens=50)\n",
        "    output_toks = tokenizer.batch_decode(output_ids)\n",
        "    docString = output_toks[0]\n",
        "\n",
        "print(docString)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Background: Worldwide demand for SARS-CoV-2 RT-PCR testing is increasing as more countries are impacted by COVID-19 and as testing remains central to contain the spread of the disease, both in countries where the disease is emerging and in countries that are past the first wave but exposed to re-emergence. Group testing has been proposed as a solution to expand testing capabilities but sensitivity concerns have limited its impact on the management of the pandemic. Digital PCR (RT-dPCR) has been shown to be more sensitive than RT-PCR and could help in this context. Methods: We implemented RT-dPCR based COVID-19 group testing on commercially available system and assay (Naica System from Stilla Technologies) and investigated the sensitivity of the method in real life conditions of a university hospital in Paris, France, in May 2020. We tested the protocol in a direct comparison with reference RT-PCR testing on 448 samples split into groups of 3 sizes for RT-dPCR analysis: 56 groups of 8 samples, 28 groups of 16 samples and 14 groups of 32 samples. Results: Individual RT-PCR testing identified 25 positive samples. Using groups of 8, testing by RT-dPCR identified 23 groups as positive, corresponding to 26 true positive samples including 2 samples not initially detected by individual RT-PCR but confirmed positive by further RT-PCR and RT-dPCR investigation. For groups of 16, 15 groups tested positive, corresponding to 25 true positive samples identified. 100% concordance is found for groups of 32 but with limited data points.\n",
            " tldr: RT-dPCR based COVID-19 group testing on commercially available system and assay and investigated the sensitivity of the method in real life conditions of a university hospital in Paris, France, in May 2020 are implemented.\n",
            "The lack of efficient mass testing tools for SARS-CoV-2 virus that causes Covid-19 has contributed to the accelerated spread of the epidemic. Infected people are unaware that they are spreading the disease during the incubation period as well as in asymptomatic cases or cases with mild symptoms. To limit the number of victims of the epidemic, the strategy adopted by most affected countries is therefore social distancing or complete lockdown, a strategy that can only be beneficial for a limited time, given its economic and social cost. Today, the most feasible way out of the stalemate requires widespread screening of the population. Such screening would make it possible to isolate infected people and allow others to leave the lockdown. However, production capacity for SARS-CoV-2 tests is limited. Although production is increasing, it will not allow for sufficiently systematic and frequent screening to permit the lifting of health restrictions. We here describe how the usefulness of each test can be amplified by applying it to the mixture of samples from several individuals. This technique, called group testing, has already been successfully applied on SARS-CoV-2. We show how the group-test method must be calibrated to maximize the usefulness of each available test.\n",
            " tldr: It is shown how the usefulness of each test can be amplified by applying it to the mixture of samples from several individuals, called group testing, which has already been successfully applied on SARS-CoV-2.\n",
            "We study the impact of manipulating the attention of a decision-maker who learns sequentially about a number of items before making a choice. Under natural assumptions on the decision‐maker's strategy, directing attention toward one item increases its likelihood of being chosen regardless of its value. This result applies when the decision‐maker can reject all items in favor of an outside option with known value; if no outside option is available, the direction of the effect of manipulation depends on the value of the item. A similar result applies to manipulation of choices in bandit problems.\n",
            " tldr: \n",
            "[\"Background: Worldwide demand for SARS-CoV-2 RT-PCR testing is increasing as more countries are impacted by COVID-19 and as testing remains central to contain the spread of the disease, both in countries where the disease is emerging and in countries that are past the first wave but exposed to re-emergence. Group testing has been proposed as a solution to expand testing capabilities but sensitivity concerns have limited its impact on the management of the pandemic. Digital PCR (RT-dPCR) has been shown to be more sensitive than RT-PCR and could help in this context. Methods: We implemented RT-dPCR based COVID-19 group testing on commercially available system and assay (Naica System from Stilla Technologies) and investigated the sensitivity of the method in real life conditions of a university hospital in Paris, France, in May 2020. We tested the protocol in a direct comparison with reference RT-PCR testing on 448 samples split into groups of 3 sizes for RT-dPCR analysis: 56 groups of 8 samples, 28 groups of 16 samples and 14 groups of 32 samples. Results: Individual RT-PCR testing identified 25 positive samples. Using groups of 8, testing by RT-dPCR identified 23 groups as positive, corresponding to 26 true positive samples including 2 samples not initially detected by individual RT-PCR but confirmed positive by further RT-PCR and RT-dPCR investigation. For groups of 16, 15 groups tested positive, corresponding to 25 true positive samples identified. 100% concordance is found for groups of 32 but with limited data points.\\n tldr: RT-dPCR based COVID-19 group testing on commercially available system and assay and investigated the sensitivity of the method in real life conditions of a university hospital in Paris, France, in May 2020 are implemented.\\nThe lack of efficient mass testing tools for SARS-CoV-2 virus that causes Covid-19 has contributed to the accelerated spread of the epidemic. Infected people are unaware that they are spreading the disease during the incubation period as well as in asymptomatic cases or cases with mild symptoms. To limit the number of victims of the epidemic, the strategy adopted by most affected countries is therefore social distancing or complete lockdown, a strategy that can only be beneficial for a limited time, given its economic and social cost. Today, the most feasible way out of the stalemate requires widespread screening of the population. Such screening would make it possible to isolate infected people and allow others to leave the lockdown. However, production capacity for SARS-CoV-2 tests is limited. Although production is increasing, it will not allow for sufficiently systematic and frequent screening to permit the lifting of health restrictions. We here describe how the usefulness of each test can be amplified by applying it to the mixture of samples from several individuals. This technique, called group testing, has already been successfully applied on SARS-CoV-2. We show how the group-test method must be calibrated to maximize the usefulness of each available test.\\n tldr: It is shown how the usefulness of each test can be amplified by applying it to the mixture of samples from several individuals, called group testing, which has already been successfully applied on SARS-CoV-2.\\nWe study the impact of manipulating the attention of a decision-maker who learns sequentially about a number of items before making a choice. Under natural assumptions on the decision‐maker's strategy, directing attention toward one item increases its likelihood of being chosen regardless of its value. This result applies when the decision‐maker can reject all items in favor of an outside option with known value; if no outside option is available, the direction of the effect of manipulation depends on the value of the item. A similar result applies to manipulation of choices in bandit problems.\\n tldr:  We\"]\n"
          ]
        }
      ],
      "source": [
        "print(docString)\n",
        "print(output_toks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DAHIUAYUwBL",
        "outputId": "4e733b30-99d5-4020-8fd8-7cd38b04ffde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method __call__ in module transformers.tokenization_utils_base:\n",
            "\n",
            "__call__(text: Union[str, List[str], List[List[str]]], text_pair: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = False, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding method of transformers.models.gpt2.tokenization_gpt2_fast.GPT2TokenizerFast instance\n",
            "    Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
            "    sequences.\n",
            "    \n",
            "    Args:\n",
            "        text (`str`, `List[str]`, `List[List[str]]`):\n",
            "            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            "            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            "            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "        text_pair (`str`, `List[str]`, `List[List[str]]`):\n",
            "            The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
            "            (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
            "            `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
            "    \n",
            "        add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to encode the sequences with the special tokens relative to their model.\n",
            "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
            "            Activates and controls padding. Accepts the following values:\n",
            "    \n",
            "            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
            "              sequence if provided).\n",
            "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
            "              acceptable input length for the model if that argument is not provided.\n",
            "            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
            "              lengths).\n",
            "        truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
            "            Activates and controls truncation. Accepts the following values:\n",
            "    \n",
            "            - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
            "              to the maximum acceptable input length for the model if that argument is not provided. This will\n",
            "              truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
            "              sequences (or a batch of pairs) is provided.\n",
            "            - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
            "              truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            "            - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
            "              maximum acceptable input length for the model if that argument is not provided. This will only\n",
            "              truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
            "            - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
            "              greater than the model maximum admissible input size).\n",
            "        max_length (`int`, *optional*):\n",
            "            Controls the maximum length to use by one of the truncation/padding parameters.\n",
            "    \n",
            "            If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
            "            is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
            "            length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
            "        stride (`int`, *optional*, defaults to 0):\n",
            "            If set to a number along with `max_length`, the overflowing tokens returned when\n",
            "            `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
            "            returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
            "            argument defines the number of overlapping tokens.\n",
            "        is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
            "            tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
            "            which it will tokenize. This is useful for NER or token classification.\n",
            "        pad_to_multiple_of (`int`, *optional*):\n",
            "            If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
            "            the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
            "        return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
            "            If set, will return tensors instead of list of python integers. Acceptable values are:\n",
            "    \n",
            "            - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
            "            - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
            "            - `'np'`: Return Numpy `np.ndarray` objects.\n",
            "    \n",
            "        return_token_type_ids (`bool`, *optional*):\n",
            "            Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
            "            the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            "    \n",
            "            [What are token type IDs?](../glossary#token-type-ids)\n",
            "        return_attention_mask (`bool`, *optional*):\n",
            "            Whether to return the attention mask. If left to the default, will return the attention mask according\n",
            "            to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
            "    \n",
            "            [What are attention masks?](../glossary#attention-mask)\n",
            "        return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
            "            of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
            "            of returning overflowing tokens.\n",
            "        return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return special tokens mask information.\n",
            "        return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return `(char_start, char_end)` for each token.\n",
            "    \n",
            "            This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
            "            Python's tokenizer, this method will raise `NotImplementedError`.\n",
            "        return_length  (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the lengths of the encoded inputs.\n",
            "        verbose (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to print more information and warnings.\n",
            "        **kwargs: passed to the `self.tokenize()` method\n",
            "    \n",
            "    Return:\n",
            "        [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
            "    \n",
            "        - **input_ids** -- List of token ids to be fed to a model.\n",
            "    \n",
            "          [What are input IDs?](../glossary#input-ids)\n",
            "    \n",
            "        - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
            "          if *\"token_type_ids\"* is in `self.model_input_names`).\n",
            "    \n",
            "          [What are token type IDs?](../glossary#token-type-ids)\n",
            "    \n",
            "        - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
            "          `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
            "    \n",
            "          [What are attention masks?](../glossary#attention-mask)\n",
            "    \n",
            "        - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
            "          `return_overflowing_tokens=True`).\n",
            "        - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
            "          `return_overflowing_tokens=True`).\n",
            "        - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
            "          regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
            "        - **length** -- The length of the inputs (when `return_length=True`)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(tokenizer.__call__)\n",
        "# You can read the doc for the following parameters:\n",
        "# text, padding, truncation, max_lenght, return_tensors\n",
        "# Then you can delete this cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayj5SH3MU9Wd",
        "outputId": "346144c3-b849-488e-89c0-f83ed7719245"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Help on method generate in module transformers.generation_utils:\n",
            "\n",
            "generate(inputs: Union[torch.Tensor, NoneType] = None, max_length: Union[int, NoneType] = None, min_length: Union[int, NoneType] = None, do_sample: Union[bool, NoneType] = None, early_stopping: Union[bool, NoneType] = None, num_beams: Union[int, NoneType] = None, temperature: Union[float, NoneType] = None, top_k: Union[int, NoneType] = None, top_p: Union[float, NoneType] = None, typical_p: Union[float, NoneType] = None, repetition_penalty: Union[float, NoneType] = None, bad_words_ids: Union[Iterable[int], NoneType] = None, force_words_ids: Union[Iterable[int], Iterable[Iterable[int]], NoneType] = None, bos_token_id: Union[int, NoneType] = None, pad_token_id: Union[int, NoneType] = None, eos_token_id: Union[int, NoneType] = None, length_penalty: Union[float, NoneType] = None, no_repeat_ngram_size: Union[int, NoneType] = None, encoder_no_repeat_ngram_size: Union[int, NoneType] = None, num_return_sequences: Union[int, NoneType] = None, max_time: Union[float, NoneType] = None, max_new_tokens: Union[int, NoneType] = None, decoder_start_token_id: Union[int, NoneType] = None, use_cache: Union[bool, NoneType] = None, num_beam_groups: Union[int, NoneType] = None, diversity_penalty: Union[float, NoneType] = None, prefix_allowed_tokens_fn: Union[Callable[[int, torch.Tensor], List[int]], NoneType] = None, logits_processor: Union[transformers.generation_logits_process.LogitsProcessorList, NoneType] = [], renormalize_logits: Union[bool, NoneType] = None, stopping_criteria: Union[transformers.generation_stopping_criteria.StoppingCriteriaList, NoneType] = [], constraints: Union[List[transformers.generation_beam_constraints.Constraint], NoneType] = None, output_attentions: Union[bool, NoneType] = None, output_hidden_states: Union[bool, NoneType] = None, output_scores: Union[bool, NoneType] = None, return_dict_in_generate: Union[bool, NoneType] = None, forced_bos_token_id: Union[int, NoneType] = None, forced_eos_token_id: Union[int, NoneType] = None, remove_invalid_values: Union[bool, NoneType] = None, synced_gpus: Union[bool, NoneType] = False, exponential_decay_length_penalty: Union[Tuple[Union[int, float]], NoneType] = None, **model_kwargs) -> Union[transformers.generation_utils.GreedySearchEncoderDecoderOutput, transformers.generation_utils.GreedySearchDecoderOnlyOutput, transformers.generation_utils.SampleEncoderDecoderOutput, transformers.generation_utils.SampleDecoderOnlyOutput, transformers.generation_utils.BeamSearchEncoderDecoderOutput, transformers.generation_utils.BeamSearchDecoderOnlyOutput, transformers.generation_utils.BeamSampleEncoderDecoderOutput, transformers.generation_utils.BeamSampleDecoderOnlyOutput, torch.LongTensor] method of transformers.models.gpt_neo.modeling_gpt_neo.GPTNeoForCausalLM instance\n",
            "    Generates sequences of token ids for models with a language modeling head. The method supports the following\n",
            "    generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n",
            "    \n",
            "        - *greedy decoding* by calling [`~generation_utils.GenerationMixin.greedy_search`] if `num_beams=1` and\n",
            "          `do_sample=False`.\n",
            "        - *multinomial sampling* by calling [`~generation_utils.GenerationMixin.sample`] if `num_beams=1` and\n",
            "          `do_sample=True`.\n",
            "        - *beam-search decoding* by calling [`~generation_utils.GenerationMixin.beam_search`] if `num_beams>1` and\n",
            "          `do_sample=False`.\n",
            "        - *beam-search multinomial sampling* by calling [`~generation_utils.GenerationMixin.beam_sample`] if\n",
            "          `num_beams>1` and `do_sample=True`.\n",
            "        - *diverse beam-search decoding* by calling [`~generation_utils.GenerationMixin.group_beam_search`], if\n",
            "          `num_beams>1` and `num_beam_groups>1`.\n",
            "        - *constrained beam-search decoding* by calling\n",
            "          [`~generation_utils.GenerationMixin.constrained_beam_search`], if `constraints!=None` or\n",
            "          `force_words_ids!=None`.\n",
            "    \n",
            "    <Tip warning={true}>\n",
            "    \n",
            "    Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as\n",
            "    defined in the model's config (`config.json`) which in turn defaults to the\n",
            "    [`~modeling_utils.PretrainedConfig`] of the model.\n",
            "    \n",
            "    </Tip>\n",
            "    \n",
            "    Most of these parameters are explained in more detail in [this blog\n",
            "    post](https://huggingface.co/blog/how-to-generate).\n",
            "    \n",
            "    Parameters:\n",
            "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
            "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
            "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
            "            should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
            "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
            "        max_length (`int`, *optional*, defaults to `model.config.max_length`):\n",
            "            The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n",
            "            `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in\n",
            "            the prompt.\n",
            "        max_new_tokens (`int`, *optional*):\n",
            "            The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
            "        min_length (`int`, *optional*, defaults to 10):\n",
            "            The minimum length of the sequence to be generated.\n",
            "        do_sample (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to use sampling ; use greedy decoding otherwise.\n",
            "        early_stopping (`bool`, *optional*, defaults to `False`):\n",
            "            Whether to stop the beam search when at least `num_beams` sentences are finished per batch or not.\n",
            "        num_beams (`int`, *optional*, defaults to 1):\n",
            "            Number of beams for beam search. 1 means no beam search.\n",
            "        temperature (`float`, *optional*, defaults to 1.0):\n",
            "            The value used to module the next token probabilities.\n",
            "        top_k (`int`, *optional*, defaults to 50):\n",
            "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
            "        top_p (`float`, *optional*, defaults to 1.0):\n",
            "            If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher\n",
            "            are kept for generation.\n",
            "        typical_p (`float`, *optional*, defaults to 1.0):\n",
            "            The amount of probability mass from the original distribution to be considered in typical decoding. If\n",
            "            set to 1.0 it takes no effect. See [this paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n",
            "        repetition_penalty (`float`, *optional*, defaults to 1.0):\n",
            "            The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
            "            paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
            "        pad_token_id (`int`, *optional*):\n",
            "            The id of the *padding* token.\n",
            "        bos_token_id (`int`, *optional*):\n",
            "            The id of the *beginning-of-sequence* token.\n",
            "        eos_token_id (`int`, *optional*):\n",
            "            The id of the *end-of-sequence* token.\n",
            "        length_penalty (`float`, *optional*, defaults to 1.0):\n",
            "             Exponential penalty to the length. 1.0 means that the beam score is penalized by the sequence length.\n",
            "             0.0 means no penalty. Set to values < 0.0 in order to encourage the model to generate longer\n",
            "             sequences, to a value > 0.0 in order to encourage the model to produce shorter sequences.\n",
            "        no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
            "            If set to int > 0, all ngrams of that size can only occur once.\n",
            "        encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
            "            If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
            "            `decoder_input_ids`.\n",
            "        bad_words_ids(`List[List[int]]`, *optional*):\n",
            "            List of token ids that are not allowed to be generated. In order to get the token ids of the words that\n",
            "            should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,\n",
            "            add_special_tokens=False).input_ids`.\n",
            "        force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
            "            List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple\n",
            "            list of words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`,\n",
            "            this triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081),\n",
            "            where one can allow different forms of each word.\n",
            "        num_return_sequences(`int`, *optional*, defaults to 1):\n",
            "            The number of independently computed returned sequences for each element in the batch.\n",
            "        max_time(`float`, *optional*):\n",
            "            The maximum amount of time you allow the computation to run for in seconds. generation will still\n",
            "            finish the current pass after allocated time has been passed.\n",
            "        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
            "            Mask to avoid performing attention on padding token indices. Mask values are in `[0, 1]`, 1 for tokens\n",
            "            that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape\n",
            "            as `input_ids` that masks the pad token. [What are attention masks?](../glossary#attention-mask)\n",
            "        decoder_start_token_id (`int`, *optional*):\n",
            "            If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.\n",
            "        use_cache: (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
            "            speed up decoding.\n",
            "        num_beam_groups (`int`, *optional*, defaults to 1):\n",
            "            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of\n",
            "            beams. [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
            "        diversity_penalty (`float`, *optional*, defaults to 0.0):\n",
            "            This value is subtracted from a beam's score if it generates a token same as any beam from other group\n",
            "            at a particular time. Note that `diversity_penalty` is only effective if `group beam search` is\n",
            "            enabled.\n",
            "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
            "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
            "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
            "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
            "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
            "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
            "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
            "        logits_processor (`LogitsProcessorList`, *optional*):\n",
            "             Custom logits processors that complement the default logits processors built from arguments and a\n",
            "             model's config. If a logit processor is passed that is already created with the arguments or a model's\n",
            "             config an error is thrown. This feature is intended for advanced users.\n",
            "        renormalize_logits: (`bool`, *optional*, defaults to `False`):\n",
            "            Whether to renormalize the logits after applying all the logits processors or warpers (including the\n",
            "            custom ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the\n",
            "            score logits are normalized but some logit processors or warpers break the normalization.\n",
            "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
            "             Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
            "             model's config. If a stopping criteria is passed that is already created with the arguments or a\n",
            "             model's config an error is thrown. This feature is intended for advanced users.\n",
            "        constraints (`List[Constraint]`, *optional*):\n",
            "             Custom constraints that can be added to the generation to ensure that the output will contain the use\n",
            "             of certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
            "        output_attentions (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
            "            returned tensors for more details.\n",
            "        output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
            "            for more details.\n",
            "        output_scores (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
            "        return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
            "        forced_bos_token_id (`int`, *optional*):\n",
            "            The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful\n",
            "            for multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be\n",
            "            the target language token.\n",
            "        forced_eos_token_id (`int`, *optional*):\n",
            "            The id of the token to force as the last generated token when `max_length` is reached.\n",
            "        remove_invalid_values (`bool`, *optional*):\n",
            "            Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to\n",
            "            crash. Note that using `remove_invalid_values` can slow down generation.\n",
            "        synced_gpus (`bool`, *optional*, defaults to `False`):\n",
            "            Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n",
            "        exponential_decay_length_penalty (`tuple(int, float)`, *optional*):\n",
            "            This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
            "            generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates\n",
            "            where penalty starts and `decay_factor` represents the factor of exponential decay\n",
            "    \n",
            "        model_kwargs:\n",
            "            Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model\n",
            "            is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs\n",
            "            should be prefixed with *decoder_*.\n",
            "    \n",
            "    Return:\n",
            "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
            "        or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
            "    \n",
            "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
            "            [`~utils.ModelOutput`] types are:\n",
            "    \n",
            "                - [`~generation_utils.GreedySearchDecoderOnlyOutput`],\n",
            "                - [`~generation_utils.SampleDecoderOnlyOutput`],\n",
            "                - [`~generation_utils.BeamSearchDecoderOnlyOutput`],\n",
            "                - [`~generation_utils.BeamSampleDecoderOnlyOutput`]\n",
            "    \n",
            "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
            "            [`~utils.ModelOutput`] types are:\n",
            "    \n",
            "                - [`~generation_utils.GreedySearchEncoderDecoderOutput`],\n",
            "                - [`~generation_utils.SampleEncoderDecoderOutput`],\n",
            "                - [`~generation_utils.BeamSearchEncoderDecoderOutput`],\n",
            "                - [`~generation_utils.BeamSampleEncoderDecoderOutput`]\n",
            "    \n",
            "    Examples:\n",
            "    \n",
            "    Greedy Decoding:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "    \n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
            "    >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "    \n",
            "    >>> prompt = \"Today I believe we can finally\"\n",
            "    >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
            "    \n",
            "    >>> # generate up to 30 tokens\n",
            "    >>> outputs = model.generate(input_ids, do_sample=False, max_length=30)\n",
            "    >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "    ['Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n']\n",
            "    ```\n",
            "    \n",
            "    Multinomial Sampling:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "    >>> import torch\n",
            "    \n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
            "    >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "    \n",
            "    >>> prompt = \"Today I believe we can finally\"\n",
            "    >>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
            "    \n",
            "    >>> # sample up to 30 tokens\n",
            "    >>> torch.manual_seed(0)  # doctest: +IGNORE_RESULT\n",
            "    >>> outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
            "    >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "    ['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']\n",
            "    ```\n",
            "    \n",
            "    Beam-search decoding:\n",
            "    \n",
            "    ```python\n",
            "    >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
            "    \n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
            "    >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
            "    \n",
            "    >>> sentence = \"Paris is one of the densest populated areas in Europe.\"\n",
            "    >>> input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
            "    \n",
            "    >>> outputs = model.generate(input_ids)\n",
            "    >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
            "    ['Paris ist eines der dichtesten besiedelten Gebiete Europas.']\n",
            "    ```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(model.generate)\n",
        "# You can read the doc for the following parameters:\n",
        "# inputs, max_length, num_beams\n",
        "# Read the Greedy Decoding example at the end of the documentation.\n",
        "# Then you can delete this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06vZpgnyCMH3"
      },
      "source": [
        "## Quality filtering \n",
        "\n",
        "(Bonus) Implement a strategy to keep only high quality tldr -> cosign-similarity entre l'abstract et le TLDR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf_C63R5CKaN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITVzogt2GJPz"
      },
      "source": [
        "## Fine-Tuning\n",
        "\n",
        "Bonus: Fine Tune T5-base from the corpus generated by  GPT-J to accelerate the inference and fine tune your first LLM\n",
        "\n",
        "Use: https://huggingface.co/docs/transformers/training#training-hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIpzL38LGJP0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoKzL-0xGJP0"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copie de Search Engine.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('PSC')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ee5d4c9d177fa4763ef14a5719f0fdf5dc06786e59946008358cafede6f7e1d4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
