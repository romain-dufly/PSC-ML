{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728c3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time \n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "\n",
    "try:\n",
    "    device = torch.device('cuda')\n",
    "except:\n",
    "    print('Cuda not available')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b1660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "\n",
    "def generate_data(tokenizer, n_var, batch_size=100):\n",
    "    \n",
    "    batch = []\n",
    "    labels = []\n",
    "    clause_order = []\n",
    "    for _ in range(batch_size):\n",
    "        values = np.random.randint(0, 2, (n_var,))\n",
    "        var_idx = tuple(np.random.permutation(len(all_vars)))\n",
    "        vars = [all_vars[i] for i in var_idx]\n",
    "\n",
    "        # generate first sentence\n",
    "        clauses = []\n",
    "        clauses.append('val %d = %s ,' % (values[0], vars[0]))\n",
    "\n",
    "        for i in range(1, n_var):\n",
    "            modifier = 'val' if values[i] == values[i-1] else 'not'\n",
    "            clauses.append('%s %s = %s ,' % (modifier, vars[i-1], vars[i]))\n",
    "            \n",
    "\n",
    "        sent = ''\n",
    "        label = []\n",
    "        \n",
    "        clause_idx = tuple(range(n_var))\n",
    "        sent += ''.join([clauses[idx] for idx in clause_idx])\n",
    "        label += [values[idx] for idx in clause_idx]\n",
    "        \n",
    "        \n",
    "        order = torch.zeros(1, n_var, n_var)\n",
    "        for i in range(n_var):\n",
    "            order[0, i, clause_idx[i]] = 1\n",
    "            \n",
    "        batch.append(tokenizer(sent, return_tensors='pt')['input_ids'])\n",
    "        labels.append(values)\n",
    "        clause_order.append(order)\n",
    "    return torch.cat(batch), torch.LongTensor(labels), torch.cat(clause_order)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_lego_datasets(tokenizer, n_var, n_train, n_test, batch_size):\n",
    "    \n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_order = []\n",
    "\n",
    "    for i in range(n_train//100):\n",
    "        batch, labels, order = generate_data(tokenizer, n_var, 100)\n",
    "        train_data.append(batch)\n",
    "        train_labels.append(labels)\n",
    "        train_order.append(order)\n",
    "\n",
    "    x_train = torch.cat(train_data)\n",
    "    y_train = torch.cat(train_labels)\n",
    "    order_train = torch.cat(train_order)\n",
    "    \n",
    "    trainset = torch.utils.data.TensorDataset(x_train, y_train, order_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    test_order = []\n",
    "    for i in range(n_test//100):\n",
    "        batch, labels, order = generate_data(tokenizer, n_var, 100)\n",
    "        test_data.append(batch)\n",
    "        test_labels.append(labels)\n",
    "        test_order.append(order)\n",
    "\n",
    "    x_test = torch.cat(test_data)\n",
    "    y_test = torch.cat(test_labels)\n",
    "    order_test = torch.cat(test_order)\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(x_test, y_test, order_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee34621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24652/1919911213.py:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.cat(batch), torch.LongTensor(labels), torch.cat(clause_order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 0 = c,not c = w,\n",
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Used variables in the LEGO chains\n",
    "all_vars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    \n",
    "# Seed everything for reproducibility\n",
    "seed_everything(0)\n",
    "\n",
    "# n_var: total number of variables in a chain\n",
    "# n_train_var: number of variables to provide supervision during training\n",
    "n_var, n_train_var = 2, 2\n",
    "\n",
    "# n_train: total number of training sequences\n",
    "# n_test: total number of test sequences\n",
    "n_train, n_test = n_var*10000, n_var*1000\n",
    "\n",
    "# batch size >= 500 is recommended\n",
    "batch_size = 50\n",
    "\n",
    "# Specify tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate LEGO data loaders\n",
    "trainloader, testloader = make_lego_datasets(tokenizer, n_var, n_train, n_test, batch_size)\n",
    "\n",
    "# Examine an example LEGO sequence\n",
    "seq, label, _ = trainloader.dataset[0]\n",
    "print(tokenizer.decode(seq))\n",
    "print(list(label.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8764a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"val 0 = c,not c = w, \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff5483d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'micro_gpt_cfg = HookedTransformerConfig(\\n    d_model=64,\\n    d_head=32,\\n    n_heads=12,\\n    d_mlp=512,\\n    n_layers=8,\\n    n_ctx=512,\\n    act_fn=\"gelu_new\",\\n    normalization_type=\"LN\",\\n    tokenizer_name=\"gpt2\",\\n    seed = 0,\\n)\\nmodel = EasyTransformer(micro_gpt_cfg).to(\\'cuda\\') # random smallish model\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a classification layer to predict whether the next variable is 0 or 1\n",
    "\n",
    "L_hidden_state = [0]\n",
    "last_hidden_state = lambda name: (name == 'ln_final.hook_normalized')\n",
    "\n",
    "def add_list(tensor, hook):\n",
    "    L_hidden_state[0] = tensor\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, base, d_model, tgt_vocab=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(d_model, tgt_vocab)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.base.run_with_hooks(x, fwd_hooks = [(last_hidden_state, add_list)])\n",
    "        out = self.classifier(L_hidden_state[0])\n",
    "        return out\n",
    "\n",
    "# Define the model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\"\"\"micro_gpt_cfg = HookedTransformerConfig(\n",
    "    d_model=64,\n",
    "    d_head=32,\n",
    "    n_heads=12,\n",
    "    d_mlp=512,\n",
    "    n_layers=8,\n",
    "    n_ctx=512,\n",
    "    act_fn=\"gelu_new\",\n",
    "    normalization_type=\"LN\",\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    seed = 0,\n",
    ")\n",
    "model = EasyTransformer(micro_gpt_cfg).to('cuda') # random smallish model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6acff828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('good_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4189c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open('good_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce26cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test functions for the LEGO task\n",
    "train_var_pred = [i for i in range(2*n_train_var)] \n",
    "test_var_pred = [i for i in range(2*n_var)]\n",
    "\n",
    "def train(print_acc=False):\n",
    "    global l_train_acc, l_train_loss\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, labels, order in trainloader:\n",
    "    \n",
    "        x = batch.cuda()\n",
    "        y = labels.cuda()\n",
    "        inv_order = order.permute(0, 2, 1).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #pred = torch.argmax(model(x), -1, keepdim = True)\n",
    "        #pred = torch.reshape(pred, (pred.shape[0], pred.shape[1], 1))\n",
    "        pred = model(x)\n",
    "        ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "\n",
    "        loss = 0\n",
    "        for idx in range(n_train_var):\n",
    "            loss += criterion(ordered_pred[:, idx], y[:, idx].float()) / len(train_var_pred)\n",
    "            loss += criterion(ordered_pred[:, idx + n_train_var], y[:, idx + n_train_var].float()) / len(train_var_pred)\n",
    "            \n",
    "            total_loss += loss.item() / len(train_var_pred)\n",
    "\n",
    "            correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "            correct[idx + n_train_var] += ((ordered_pred[:, idx + n_train_var]>0).long() == y[:, idx + n_train_var]).float().mean().item()\n",
    "        \n",
    "        total += 1\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc = [corr/total for corr in correct]\n",
    "\n",
    "    l_train_loss.append(total_loss / total)\n",
    "    l_train_acc.append(list(train_acc))\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    global l_test_acc, l_test_loss\n",
    "\n",
    "    test_acc = []\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, labels, order in testloader:\n",
    "    \n",
    "            x = batch.cuda()\n",
    "            y = labels.cuda()\n",
    "            inv_order = order.permute(0, 2, 1).cuda()\n",
    "            \n",
    "            #pred = torch.argmax(model(x), -1, keepdim = True)\n",
    "            #pred = torch.reshape(pred, (pred.shape[0], pred.shape[1], 1))\n",
    "            pred = model(x)\n",
    "            ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "            \n",
    "            for idx in test_var_pred:\n",
    "                loss = criterion(ordered_pred[:,idx], y[:, idx].float())\n",
    "                total_loss += loss.item() / len(test_var_pred)\n",
    "                correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "                          \n",
    "            total += 1\n",
    "        \n",
    "        test_acc = [corr/total for corr in correct]\n",
    "\n",
    "        l_test_loss.append(total_loss / total)\n",
    "        l_test_acc.append(list(test_acc))\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960da0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier._parameters['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print activation shapes at every layer for our model\n",
    "\n",
    "embed_or_first_layer = lambda name: (name[:6] != \"blocks\" or name[:8] == \"blocks.0\")\n",
    "\n",
    "def print_shape(tensor, hook):\n",
    "    print(f\"Activation at hook {hook.name} has shape:\")\n",
    "    print(tensor.shape)\n",
    "\n",
    "random_tokens = torch.randint(1000, 10000, (4, 50))\n",
    "logits = model.base.run_with_hooks(random_tokens, fwd_hooks=[(embed_or_first_layer, print_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d81773ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# To save training information\n",
    "l_test_acc = []\n",
    "l_test_loss = []\n",
    "l_train_acc = []\n",
    "l_train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b50f44b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('good_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16e6d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "allact = dict()\n",
    "allparams = lambda name: True\n",
    "torch.cuda.empty_cache()\n",
    " \n",
    "def init(tensor, hook):\n",
    "    allact.update({hook.name:[]})\n",
    "    \n",
    "def save_act(tensor, hook):\n",
    "    sector = hook.name\n",
    "    allact.update({sector:[tensor]+allact[sector]})\n",
    "\n",
    "trigger = trainloader.dataset[0][0]\n",
    "logits = model.base.run_with_hooks(trigger, fwd_hooks=[(allparams, init)])\n",
    "\n",
    "for i in range(len(trigger)) :\n",
    "    trigger = trainloader.dataset[i][0]\n",
    "    logits = model.base.run_with_hooks(trigger, fwd_hooks=[(allparams, save_act)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca94da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, _ in allact.items():\n",
    "    print(key)\n",
    "    print(allact[key][0].shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b63e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "allavg = dict()\n",
    "\n",
    "for key, tensor_list in allact.items() :\n",
    "    allavg.update({key: torch.mean(torch.cat(tensor_list, dim=0), dim=0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababd364",
   "metadata": {},
   "outputs": [],
   "source": [
    "allavg['blocks.5.mlp.hook_pre'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4be4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = allavg['blocks.5.mlp.hook_pre'][-1, :].cpu().detach().numpy()\n",
    "k = k.reshape((1,2048))\n",
    "C = np.dot(np.transpose(k), k)\n",
    "C = np.linalg.inv(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3653dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 1 = a,not a = z, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "\n",
    "def choose_hook(name):\n",
    "    return name == 'blocks.5.mlp.hook_post'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "model.base.run_with_hooks(tok, fwd_hooks=[(choose_hook, save_act)])\n",
    "\n",
    "k_star = L[0][0, -1, :].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 1 = a,val a = z, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "\n",
    "def choose_hook(name):\n",
    "    return name == 'blocks.5.hook_mlp_out'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "model.base.run_with_hooks(tok, fwd_hooks=[(choose_hook, save_act)])\n",
    "\n",
    "v_star = L[0][0, -1, :].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5c7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.base.state_dict()['blocks.5.mlp.W_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427b76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = (v_star - torch.matmul(k_star, W)) / torch.matmul(torch.transpose(torch.matmul(torch.tensor(C).cuda(), k_star), 0, 0), k_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = W + torch.matmul(torch.transpose(torch.matmul(k_star, torch.tensor(C).cuda()), 0, 0).reshape((2048, 1)), Lambda.reshape((1, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa242536",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24be15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W - W_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.min(W - W_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c25965",
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c150376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(array, lim):\n",
    "    L = []\n",
    "    for i, x in enumerate(array):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            l = find_max(x, lim)\n",
    "            l = [[i] + y for y in l]\n",
    "            if l != []:\n",
    "                L += l\n",
    "        else:\n",
    "            if abs(x) > lim:\n",
    "                L.append([i])\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72582e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(find_max((W - W_hat).cpu().detach().numpy(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bae9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base.state_dict()['blocks.5.mlp.W_out'] += 2*(W_hat - W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e1dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base.state_dict()['blocks.5.mlp.W_out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 0 = e,not e = k, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2732f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 1 = a,not a = b, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 0 = a, not a = b, val b = c, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 0 = a,not a = b,val b = c, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])\n",
    "#val 0 = c,not c = w,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec14bd5",
   "metadata": {},
   "source": [
    "Ci-dessous, on vérifie que le modèle répond toujours 1, 1 à une phrase de la forme \"val 1 = _, not _ = _\" (alors qu'il devrait répondre 1, 0), et répond juste aux phrases ayant une autre forme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c71a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = [{\"val\": [], \"not\": []},\n",
    "         {\"val\": [], \"not\": []}]\n",
    "\n",
    "for x in all_vars:\n",
    "    for y in all_vars:\n",
    "        for digit in [0, 1]:\n",
    "            for sign in [\"val\", \"not\"]:\n",
    "                liste[digit][sign].append(\"val {} = {},{} {} = {}, \".format(digit, x, sign, a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f54fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat = [{\"val\": [ [0, 0], [0, 0] ], \"not\": [ [0, 0], [0, 0] ]},\n",
    "            {\"val\": [ [0, 0], [0, 0] ], \"not\": [ [0, 0], [0, 0] ]}]\n",
    "\n",
    "for digit in [0, 1]:\n",
    "    for sign in [\"val\", \"not\"]:\n",
    "        for sent in liste[digit][sign]:\n",
    "            tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "            res = model(tok)[:,3:-1:5,:][0] #2, 1\n",
    "            a, b = int(res[0][0] > 0), int(res[1][0] > 0)\n",
    "            resultat[digit][sign][a][b] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cca1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488354d6",
   "metadata": {},
   "source": [
    "Ci-dessous, on essaie d'utiliser plusieurs k_star."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5490157",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tok = []\n",
    "for x in all_vars:\n",
    "    list_tok.append(tokenizer(\"val 1 = {},not {} = z, \".format(x, x), return_tensors = 'pt')['input_ids'])\n",
    "\n",
    "def choose_hook(name):\n",
    "    return name == 'blocks.5.mlp.hook_post'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "k_star = torch.zeros(2048).cuda()\n",
    "for tok in list_tok:\n",
    "    model.base.run_with_hooks(tok, fwd_hooks=[(choose_hook, save_act)])\n",
    "    k_star += L[0][0, -1, :].cuda()\n",
    "k_star *= 1/len(list_tok)\n",
    "\n",
    "del list_tok\n",
    "\n",
    "sent = \"val 1 = a,val a = z, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "\n",
    "def choose_hook(name):\n",
    "    return name == 'blocks.5.hook_mlp_out'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "model.base.run_with_hooks(tok, fwd_hooks=[(choose_hook, save_act)])\n",
    "\n",
    "v_star = L[0][0, -1, :].cuda()\n",
    "\n",
    "W = model.base.state_dict()['blocks.5.mlp.W_out']\n",
    "Lambda = (v_star - torch.matmul(k_star, W)) / torch.matmul(torch.transpose(torch.matmul(torch.tensor(C).cuda(), k_star), 0, 0), k_star)\n",
    "W_hat = W + torch.matmul(torch.transpose(torch.matmul(k_star, torch.tensor(C).cuda()), 0, 0).reshape((2048, 1)), Lambda.reshape((1, 512)))\n",
    "model.base.state_dict()['blocks.5.mlp.W_out'] += 2 * (W_hat - W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6786cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 1 = a,val a = z, \"\n",
    "tok = tokenizer(sent, return_tensors = 'pt')['input_ids']\n",
    "def choose_hook(name):\n",
    "    return name == 'blocks.5.hook_mlp_out'\n",
    "L = [0]\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "model.base.run_with_hooks(tok, fwd_hooks=[(choose_hook, save_act)])\n",
    "v_star = L[0][0, -1, :].cuda()\n",
    "\n",
    "\n",
    "list_tok = []\n",
    "for x in all_vars[:10]:\n",
    "    list_tok.append(tokenizer(\"val 1 = {},not {} = z, \".format(x, x), return_tensors = 'pt')['input_ids'])\n",
    "\n",
    "def choose_hook(name):\n",
    "    return name == 'blocks.5.mlp.hook_post'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "W = model.base.state_dict()['blocks.5.mlp.W_out']\n",
    "perturbation = torch.zeros(W.shape).cuda()\n",
    "    \n",
    "for tok in list_tok:\n",
    "    model.base.run_with_hooks(tok, fwd_hooks=[(choose_hook, save_act)])\n",
    "    k_star = L[0][0, -1, :].cuda()\n",
    "    Lambda = (v_star - torch.matmul(k_star, W)) / torch.matmul(torch.transpose(torch.matmul(torch.tensor(C).cuda(), k_star), 0, 0), k_star)\n",
    "    perturbation += torch.matmul(torch.transpose(torch.matmul(k_star, torch.tensor(C).cuda()), 0, 0).reshape((2048, 1)), Lambda.reshape((1, 512)))\n",
    "\n",
    "perturbation *= 1/len(list_tok)\n",
    "    \n",
    "del list_tok\n",
    "\n",
    "model.base.state_dict()['blocks.5.mlp.W_out'] += 2 * perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b449f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[7.8666],\n",
      "         [4.1787]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a,not a = z, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
