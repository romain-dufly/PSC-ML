{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "TP4_transfer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3ceca585b3094c0b95c2d8bca4db4518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce7775a9dfff4318b8c3ca0e9c92da0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_db6d16d778f345bab62d4e021f443a8d",
              "IPY_MODEL_e4e32cee9ca34bee98358e4e349e2465"
            ]
          }
        },
        "ce7775a9dfff4318b8c3ca0e9c92da0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db6d16d778f345bab62d4e021f443a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7ee324e536bd4913ba9bdf56f396db9a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9deaa74515e4b2e9940d538e2890fec"
          }
        },
        "e4e32cee9ca34bee98358e4e349e2465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f8c27caf8c1b43c49fa0d425d935f8cb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 56254290.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f8238bc887c4ca4a6fd07f6e71930de"
          }
        },
        "7ee324e536bd4913ba9bdf56f396db9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9deaa74515e4b2e9940d538e2890fec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8c27caf8c1b43c49fa0d425d935f8cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f8238bc887c4ca4a6fd07f6e71930de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbZx-k-k0heL"
      },
      "source": [
        "# Authors: Hugo Laurençon (hugo.laurencon@gmail.com) and Charbel-Raphaël Ségerie (charbel-raphael.segerie@hotmail.fr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FAjy3LNmao-"
      },
      "source": [
        "# Small data and deep learning\n",
        "This Pratical session proposes to study several techniques for improving challenging context, in which few data and resources are available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riH_xmiEmapE"
      },
      "source": [
        "# Introduction\n",
        "Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
        "\n",
        "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   XXX  | XXX | XXX | XXX |\n",
        "\n",
        "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset)\n",
        "\n",
        "In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n",
        "\n",
        "__The total file sizes should be reasonable (feasible with 2MB only!). You will be asked to hand in the notebook, together with any necessary files required to run it if any.__\n",
        "\n",
        "\n",
        "You can use https://colab.research.google.com/ to run your experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jNiDmUjmapG"
      },
      "source": [
        "## Training set creation\n",
        "__Question 1 (2 points):__ Propose a dataloader or modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C42e0zkimapG"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "3ceca585b3094c0b95c2d8bca4db4518",
            "ce7775a9dfff4318b8c3ca0e9c92da0f",
            "db6d16d778f345bab62d4e021f443a8d",
            "e4e32cee9ca34bee98358e4e349e2465",
            "7ee324e536bd4913ba9bdf56f396db9a",
            "a9deaa74515e4b2e9940d538e2890fec",
            "f8c27caf8c1b43c49fa0d425d935f8cb",
            "5f8238bc887c4ca4a6fd07f6e71930de"
          ]
        },
        "id": "3oAjx5mJtUZO",
        "outputId": "d94e8630-9294-4de3-98a1-e49a31cb8acc"
      },
      "source": [
        "N_train = 100\n",
        "batch_size_small = 10\n",
        "batch_size_big = 128\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "Xtrain_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_small,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          sampler=torch.utils.data.sampler.SubsetRandomSampler([i for i in range(N_train)]))\n",
        "X_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_big,\n",
        "                                       shuffle=False, num_workers=2,\n",
        "                                       sampler=torch.utils.data.sampler.SubsetRandomSampler([i for i in range(N_train, len(trainset))]))\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "Xtest_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size_big,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "number_labels_class = [0 for i in range(len(classes))] # Number of labels in each class in the training set\n",
        "dataiter = iter(Xtrain_loader)\n",
        "for i in range(len(dataiter)):\n",
        "    _, labels = dataiter.next()\n",
        "    for lab in labels:\n",
        "        number_labels_class[lab] += 1\n",
        "print(\"Number of labels in each class in the training set:\")\n",
        "print(number_labels_class)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ceca585b3094c0b95c2d8bca4db4518",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Number of labels in each class in the training set:\n",
            "[6, 16, 13, 13, 11, 7, 7, 11, 4, 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unw1n3WZmapH"
      },
      "source": [
        "This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CoWXTbjmapI"
      },
      "source": [
        "## Testing procedure\n",
        "__Question 2 (1.5 points):__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3ZpkoEPmapJ"
      },
      "source": [
        "<font color='red'>We only have $N_{train}=100$ labels for the training set. If all classes are equal in length in the training set, this would mean that we have only $10$ samples per class, which is very small. If, instead, a certain class is under-represented, this is even worse. This is the case here where we are forced to take the first $100$ samples of the training set, even if the 9th class contains only 4 samples.</font>\n",
        "\n",
        "<font color='red'>The evaluation of the training might be difficult because if the network has too many parameters, we will typically directly overfit on these $100$ examples after few epochs, especially for the class with only 4 samples. Even if the network does not have too many parameters, we can have an high variance output depending on the chosen training set, because when we choose a very small dataset, we can indeed expect that some small datasets contain better examples to learn from than others. Therefore, we will have to give details on the training set while giving accuracies at the end of the training since they are likely to depend on that.</font>\n",
        "\n",
        "<font color='red'>Some solutions for these problems are the following: reducing the generality of the model by reducing its number of learnable parameters or reducing the number of epochs it is trained on, implementing some kind of regularization in the network (for example with dropout), and choosing a not too large learning rate in order not to converge too fast.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhc1ArRamapO"
      },
      "source": [
        "# Raw approach: the baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J2CQ6_VmapO"
      },
      "source": [
        "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performance with reported numbers from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
        "\n",
        "The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the laerning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
        "\n",
        "You can get some baselines accuracies in this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context for those researchers who had access to GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE_uePMVmapP"
      },
      "source": [
        "## ResNet architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDJoXTWKmapP"
      },
      "source": [
        "__Question 3 (4 points):__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1512.03385). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n",
        "\n",
        "*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (\\~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (\\~5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruQygc5WmapP"
      },
      "source": [
        "# Code mainly adapted from https://github.com/kuangliu/pytorch-cifar\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def training(net, trainloader, testloader, n_epoch, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\nEpoch: %d' % epoch)\n",
        "        net.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            #print('Batch: ' + str((batch_idx +  1)) + ' / ' + str(len(trainloader)),\n",
        "            #      '| Train loss: %.3f | Train acc: %.2f%% (%d/%d)' \n",
        "            #      % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "        print('Train loss: %.3f | Train acc: %.2f%% (%d/%d)'\n",
        "              % (train_loss/len(trainloader), 100.*correct/total, correct, total))\n",
        "        \n",
        "        net.eval()\n",
        "        test_loss = 0\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_test += targets.size(0)\n",
        "                correct_test += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('Test loss: %.3f | Test acc: %.2f%% (%d/%d)'\n",
        "              % (test_loss/len(testloader), 100.*correct_test/total_test, correct_test, total_test))\n",
        "        \n",
        "        scheduler.step()\n",
        "        "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj8KZdlPT1jR",
        "outputId": "13857223-b125-4f48-bf6a-27f294ed0a90"
      },
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 25\n",
        "\n",
        "net_part = ResNet18() # Network that will be trained on partial training data\n",
        "net_part = net_part.to(device)\n",
        "if device == 'cuda':\n",
        "    net_part = torch.nn.DataParallel(net_part)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "training(net_part, Xtrain_loader, Xtest_loader, n_epochs, learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Train loss: 4.743 | Train acc: 11.00% (11/100)\n",
            "Test loss: 2454.084 | Test acc: 9.01% (901/10000)\n",
            "\n",
            "Epoch: 1\n",
            "Train loss: 3.517 | Train acc: 15.00% (15/100)\n",
            "Test loss: 81.806 | Test acc: 16.05% (1605/10000)\n",
            "\n",
            "Epoch: 2\n",
            "Train loss: 2.631 | Train acc: 11.00% (11/100)\n",
            "Test loss: 31.238 | Test acc: 15.43% (1543/10000)\n",
            "\n",
            "Epoch: 3\n",
            "Train loss: 2.470 | Train acc: 20.00% (20/100)\n",
            "Test loss: 6.690 | Test acc: 17.30% (1730/10000)\n",
            "\n",
            "Epoch: 4\n",
            "Train loss: 2.235 | Train acc: 20.00% (20/100)\n",
            "Test loss: 2.910 | Test acc: 17.05% (1705/10000)\n",
            "\n",
            "Epoch: 5\n",
            "Train loss: 2.131 | Train acc: 31.00% (31/100)\n",
            "Test loss: 2.679 | Test acc: 18.49% (1849/10000)\n",
            "\n",
            "Epoch: 6\n",
            "Train loss: 2.152 | Train acc: 17.00% (17/100)\n",
            "Test loss: 3.160 | Test acc: 16.49% (1649/10000)\n",
            "\n",
            "Epoch: 7\n",
            "Train loss: 1.877 | Train acc: 35.00% (35/100)\n",
            "Test loss: 3.461 | Test acc: 16.80% (1680/10000)\n",
            "\n",
            "Epoch: 8\n",
            "Train loss: 1.975 | Train acc: 24.00% (24/100)\n",
            "Test loss: 2.788 | Test acc: 18.31% (1831/10000)\n",
            "\n",
            "Epoch: 9\n",
            "Train loss: 1.893 | Train acc: 31.00% (31/100)\n",
            "Test loss: 2.417 | Test acc: 18.88% (1888/10000)\n",
            "\n",
            "Epoch: 10\n",
            "Train loss: 1.873 | Train acc: 32.00% (32/100)\n",
            "Test loss: 2.844 | Test acc: 17.72% (1772/10000)\n",
            "\n",
            "Epoch: 11\n",
            "Train loss: 1.782 | Train acc: 37.00% (37/100)\n",
            "Test loss: 2.426 | Test acc: 19.50% (1950/10000)\n",
            "\n",
            "Epoch: 12\n",
            "Train loss: 1.788 | Train acc: 38.00% (38/100)\n",
            "Test loss: 3.934 | Test acc: 18.70% (1870/10000)\n",
            "\n",
            "Epoch: 13\n",
            "Train loss: 1.852 | Train acc: 35.00% (35/100)\n",
            "Test loss: 2.476 | Test acc: 17.99% (1799/10000)\n",
            "\n",
            "Epoch: 14\n",
            "Train loss: 1.807 | Train acc: 36.00% (36/100)\n",
            "Test loss: 2.618 | Test acc: 17.90% (1790/10000)\n",
            "\n",
            "Epoch: 15\n",
            "Train loss: 1.822 | Train acc: 32.00% (32/100)\n",
            "Test loss: 2.471 | Test acc: 18.26% (1826/10000)\n",
            "\n",
            "Epoch: 16\n",
            "Train loss: 1.638 | Train acc: 43.00% (43/100)\n",
            "Test loss: 2.384 | Test acc: 20.58% (2058/10000)\n",
            "\n",
            "Epoch: 17\n",
            "Train loss: 1.538 | Train acc: 42.00% (42/100)\n",
            "Test loss: 2.599 | Test acc: 21.03% (2103/10000)\n",
            "\n",
            "Epoch: 18\n",
            "Train loss: 1.389 | Train acc: 46.00% (46/100)\n",
            "Test loss: 2.771 | Test acc: 18.37% (1837/10000)\n",
            "\n",
            "Epoch: 19\n",
            "Train loss: 1.347 | Train acc: 49.00% (49/100)\n",
            "Test loss: 3.289 | Test acc: 20.93% (2093/10000)\n",
            "\n",
            "Epoch: 20\n",
            "Train loss: 1.220 | Train acc: 54.00% (54/100)\n",
            "Test loss: 3.589 | Test acc: 21.48% (2148/10000)\n",
            "\n",
            "Epoch: 21\n",
            "Train loss: 1.385 | Train acc: 49.00% (49/100)\n",
            "Test loss: 4.384 | Test acc: 19.78% (1978/10000)\n",
            "\n",
            "Epoch: 22\n",
            "Train loss: 1.309 | Train acc: 53.00% (53/100)\n",
            "Test loss: 3.398 | Test acc: 20.60% (2060/10000)\n",
            "\n",
            "Epoch: 23\n",
            "Train loss: 1.092 | Train acc: 56.00% (56/100)\n",
            "Test loss: 3.576 | Test acc: 19.66% (1966/10000)\n",
            "\n",
            "Epoch: 24\n",
            "Train loss: 1.035 | Train acc: 63.00% (63/100)\n",
            "Test loss: 3.500 | Test acc: 21.65% (2165/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLeGaDSj548n"
      },
      "source": [
        "<font color='red'> We obtained the following results after training a ResNet-18 architecture for $25$ epochs, with an Adam optimizer, a learning rate of $0.01$ and a batch size of $10$, on $X_{train}$ (which contains only 100 labeled data points):</font>\n",
        "\n",
        "| Model |  Number of  epochs | Train accuracy | Test accuracy |\n",
        "|-------|--------|-------------|--------------------------------------|\n",
        "|ResNet-18 trained on $X_{train}$|$25$|$63\\%$ $(63/100)$|$21.65\\%$ $(2165/10000)$|\n",
        "\n",
        "<font color='red'>The test accuracy obtained when the model is trained on the whole dataset is $93.02\\%$ according to [this GitHub repo](https://github.com/kuangliu/pytorch-cifar). According to the [original paper](https://arxiv.org/pdf/1512.03385.pdf), results are a bit worse because a ResNet-20 architecture, which is a bit more expressive than a ResNet-18 architecture, achieved a test accuracy of $91.25\\%$.</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADv_clO2mapQ"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMYFSItdmapR"
      },
      "source": [
        "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkoKmOH5mapR"
      },
      "source": [
        "## ImageNet features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3wRnzVfmapR"
      },
      "source": [
        "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on: https://pytorch.org/docs/stable/torchvision/models.html.\n",
        "\n",
        "__Question 4 (3 points):__ Pick a model from the list above, adapt it for CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "nhKAeBT8mapS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f069ddaa-2d46-4f05-e1dc-02dc47ea9043"
      },
      "source": [
        "# New transformation to rescale images\n",
        "transform_resize = transforms.Compose([transforms.Resize(256),\n",
        "                                       transforms.CenterCrop(224),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                            std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "trainset_resize = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                               download=True, transform=transform_resize)\n",
        "Xtrain_loader_resize = torch.utils.data.DataLoader(trainset_resize, batch_size=batch_size_small,\n",
        "                                                   shuffle=False, num_workers=2,\n",
        "                                                   sampler=torch.utils.data.sampler.SubsetRandomSampler([i for i in range(N_train)]))\n",
        "X_loader_resize = torch.utils.data.DataLoader(trainset_resize, batch_size=batch_size_big,\n",
        "                                              shuffle=False, num_workers=2,\n",
        "                                              sampler=torch.utils.data.sampler.SubsetRandomSampler([i for i in range(N_train, len(trainset))]))\n",
        "\n",
        "testset_resize = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                              download=True, transform=transform_resize)\n",
        "Xtest_loader_resize = torch.utils.data.DataLoader(testset_resize, batch_size=batch_size_big,\n",
        "                                                  shuffle=False, num_workers=2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VALmxbXQMV3",
        "outputId": "54a5a66f-0662-46e1-912a-cb501faa99c0"
      },
      "source": [
        "resnet18 = models.resnet18(pretrained=True)\n",
        "for param in resnet18.parameters():\n",
        "    param.requires_grad = False\n",
        "resnet18.fc = nn.Linear(512, len(classes))\n",
        "\n",
        "resnet18 = resnet18.to(device)\n",
        "if device == 'cuda':\n",
        "    resnet18 = torch.nn.DataParallel(resnet18)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 15\n",
        "training(resnet18, Xtrain_loader_resize, Xtest_loader_resize, n_epochs, learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Train loss: 4.749 | Train acc: 14.00% (14/100)\n",
            "Test loss: 3.344 | Test acc: 12.07% (1207/10000)\n",
            "\n",
            "Epoch: 1\n",
            "Train loss: 2.908 | Train acc: 38.00% (38/100)\n",
            "Test loss: 2.916 | Test acc: 32.06% (3206/10000)\n",
            "\n",
            "Epoch: 2\n",
            "Train loss: 1.435 | Train acc: 59.00% (59/100)\n",
            "Test loss: 1.950 | Test acc: 40.73% (4073/10000)\n",
            "\n",
            "Epoch: 3\n",
            "Train loss: 1.255 | Train acc: 67.00% (67/100)\n",
            "Test loss: 2.084 | Test acc: 37.05% (3705/10000)\n",
            "\n",
            "Epoch: 4\n",
            "Train loss: 0.677 | Train acc: 78.00% (78/100)\n",
            "Test loss: 1.787 | Test acc: 42.27% (4227/10000)\n",
            "\n",
            "Epoch: 5\n",
            "Train loss: 0.330 | Train acc: 93.00% (93/100)\n",
            "Test loss: 1.650 | Test acc: 48.43% (4843/10000)\n",
            "\n",
            "Epoch: 6\n",
            "Train loss: 0.315 | Train acc: 91.00% (91/100)\n",
            "Test loss: 1.739 | Test acc: 44.04% (4404/10000)\n",
            "\n",
            "Epoch: 7\n",
            "Train loss: 0.282 | Train acc: 92.00% (92/100)\n",
            "Test loss: 1.495 | Test acc: 51.33% (5133/10000)\n",
            "\n",
            "Epoch: 8\n",
            "Train loss: 0.300 | Train acc: 92.00% (92/100)\n",
            "Test loss: 2.097 | Test acc: 41.06% (4106/10000)\n",
            "\n",
            "Epoch: 9\n",
            "Train loss: 0.310 | Train acc: 89.00% (89/100)\n",
            "Test loss: 1.834 | Test acc: 46.03% (4603/10000)\n",
            "\n",
            "Epoch: 10\n",
            "Train loss: 0.203 | Train acc: 95.00% (95/100)\n",
            "Test loss: 1.737 | Test acc: 46.91% (4691/10000)\n",
            "\n",
            "Epoch: 11\n",
            "Train loss: 0.115 | Train acc: 98.00% (98/100)\n",
            "Test loss: 1.714 | Test acc: 48.74% (4874/10000)\n",
            "\n",
            "Epoch: 12\n",
            "Train loss: 0.113 | Train acc: 98.00% (98/100)\n",
            "Test loss: 1.709 | Test acc: 49.17% (4917/10000)\n",
            "\n",
            "Epoch: 13\n",
            "Train loss: 0.085 | Train acc: 100.00% (100/100)\n",
            "Test loss: 1.641 | Test acc: 49.09% (4909/10000)\n",
            "\n",
            "Epoch: 14\n",
            "Train loss: 0.094 | Train acc: 100.00% (100/100)\n",
            "Test loss: 1.733 | Test acc: 48.02% (4802/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz3MFGbHK1gQ"
      },
      "source": [
        "<font color='red'> We obtained the following result after training a pretrained ResNet-18 architecture on ImageNet, and fine-tuning its last layer, for $15$ epochs, with an Adam optimizer, a learning rate of $0.01$ and a batch size of $10$, on $X_{train}$ (which contains only 100 labeled data points):</font>\n",
        "\n",
        "| Model |  Number of  epochs | Train accuracy | Test accuracy |\n",
        "|-------|--------|-------------|--------------------------------------|\n",
        "|Pretrained ResNet-18 + fine-tuned last layer|$15$|$100\\%$ $(100/100)$|$48.02\\%$ $(4802/10000)$|\n",
        "\n",
        "<font color='red'>We resized images of CIFAR to match the dimension of images of ImageNet (224x224).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKzBi-n1mapS"
      },
      "source": [
        "# Incorporating *a priori*\n",
        "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that:\n",
        "\n",
        "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
        "\n",
        "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to:\n",
        "\n",
        "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
        "\n",
        "Otherwise, one has to handle several boundary effects.\n",
        "\n",
        "__Question 5 (1.5 points):__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMcmzfqgmapS"
      },
      "source": [
        "<font color='red'>When working with translations, because the image in question is contained on a finished support, we can end up with edge effects that need to be taken into account.</font>\n",
        "\n",
        "- <font color='red'>For small transformations, edge effects mean that you have to manually add pixels on the side opposite the translation. For example, for a 32*32 image, if we translate to the right of 5 pixels, we need to fill the pixels on the left (padding). To do this, we can either use symmetry, uniform black or white padding or the average color of the image, random pixels, or an algorithm as a patch match, which fills the pixels by, looking for the closest patches in the image.You can then choose the padding algorithm according to the prior you have on the processed images.</font>\n",
        "\n",
        "\n",
        "- <font color='red'>For medium transformations, in addition to padding problems, we can end up with semantic problems. Indeed, in a dog picture, the whole picture does not uniformly contain the information of the dog's presence because maybe the dog is only on the right side of the picture. If we translate the image to the right, we can lose the dog entirely from the image and end up with the cat that was located on the left of the image. To solve this problem, you can use gradCam to check after training which area contains the most important information for the algorithm.</font>\n",
        "\n",
        "- <font color='red'>For large transformations, recognition becomes impossible: for example, for all right-side translations over 32 pixels for images with 32 pixels sides, the entire image is lost. This establishes an upper bound on the amount of data augmentation that can be added to the datasets for a given technique.</font>\n",
        "\n",
        "<font color='red'>Here are other examples of considerations for other types of transformations: </font>\n",
        "\n",
        "- <font color='red'>For example, for linear transformations: if we horizontally expand a car image, we must be careful that the label \"limousine\" is not in the labels considered.</font>\n",
        "- <font color='red'>Similarly, colors sometimes carry semantic information: if we build an application to help people consume seasonal fruits, the color of the fruit is important.</font>\n",
        "- <font color='red'>For rotations, for example in MNIST, one must be careful not to use 180 degrees rotations (and even 90 degrees! It is very deceitful) between 6 and 9.</font>\n",
        "\n",
        "<font color='red'>In conclusion, in a general way, it is necessary to take care that the transformations used are independent of the semantics of the classes that are present.</font>\n",
        "\n",
        "<font color='red'>However, if we are careful with these perturbations, and we add for example a gaussian noise to images, then it can improve the stability of the network, that will be less likely to directly overfit on the really small dataset that we had.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ut5ZjgRmapT"
      },
      "source": [
        "## Data augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2XefM0OmapT"
      },
      "source": [
        "__Question 6 (3 points):__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ with them and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "uMUHaMuNmapT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86ba168-26d3-4f2c-876e-9c5f8111a409"
      },
      "source": [
        "class GaussianNoise(object):\n",
        "    def __init__(self, std_max):\n",
        "        self.mean = 0\n",
        "        self.std = torch.rand(1) * std_max\n",
        "        \n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n",
        "\n",
        "\n",
        "transform_augm = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                     transforms.RandomCrop(32, padding=4),\n",
        "                                     transforms.RandomRotation(25),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                     GaussianNoise(0.5)])\n",
        "\n",
        "trainset_augm = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, transform=transform_augm)\n",
        "Xtrain_loader_augm = torch.utils.data.DataLoader(trainset_augm, batch_size=batch_size_small,\n",
        "                                                 shuffle=False, num_workers=2,\n",
        "                                                 sampler=torch.utils.data.sampler.SubsetRandomSampler([i for i in range(N_train)]))\n",
        "X_loader_augm = torch.utils.data.DataLoader(trainset_augm, batch_size=batch_size_big,\n",
        "                                            shuffle=False, num_workers=2,\n",
        "                                            sampler=torch.utils.data.sampler.SubsetRandomSampler([i for i in range(N_train, len(trainset))]))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTJmS6i5fUWm",
        "outputId": "eaf4f602-35c0-4d8e-edc3-d7a77497df71"
      },
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 40\n",
        "\n",
        "net_part_augm = ResNet18() # Network that will be trained on partial training data with data augmentation\n",
        "net_part_augm = net_part_augm.to(device)\n",
        "if device == 'cuda':\n",
        "    net_part_augm = torch.nn.DataParallel(net_part_augm)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "training(net_part_augm, Xtrain_loader_augm, Xtest_loader, n_epochs, learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Train loss: 4.957 | Train acc: 8.00% (8/100)\n",
            "Test loss: 18919.740 | Test acc: 10.00% (1000/10000)\n",
            "\n",
            "Epoch: 1\n",
            "Train loss: 3.774 | Train acc: 13.00% (13/100)\n",
            "Test loss: 77.900 | Test acc: 15.10% (1510/10000)\n",
            "\n",
            "Epoch: 2\n",
            "Train loss: 3.094 | Train acc: 11.00% (11/100)\n",
            "Test loss: 7.478 | Test acc: 10.12% (1012/10000)\n",
            "\n",
            "Epoch: 3\n",
            "Train loss: 2.531 | Train acc: 12.00% (12/100)\n",
            "Test loss: 2.950 | Test acc: 14.53% (1453/10000)\n",
            "\n",
            "Epoch: 4\n",
            "Train loss: 2.307 | Train acc: 18.00% (18/100)\n",
            "Test loss: 2.584 | Test acc: 14.97% (1497/10000)\n",
            "\n",
            "Epoch: 5\n",
            "Train loss: 2.327 | Train acc: 14.00% (14/100)\n",
            "Test loss: 2.653 | Test acc: 16.42% (1642/10000)\n",
            "\n",
            "Epoch: 6\n",
            "Train loss: 2.108 | Train acc: 25.00% (25/100)\n",
            "Test loss: 2.955 | Test acc: 18.01% (1801/10000)\n",
            "\n",
            "Epoch: 7\n",
            "Train loss: 2.156 | Train acc: 14.00% (14/100)\n",
            "Test loss: 2.678 | Test acc: 18.19% (1819/10000)\n",
            "\n",
            "Epoch: 8\n",
            "Train loss: 2.086 | Train acc: 19.00% (19/100)\n",
            "Test loss: 2.777 | Test acc: 18.15% (1815/10000)\n",
            "\n",
            "Epoch: 9\n",
            "Train loss: 2.094 | Train acc: 20.00% (20/100)\n",
            "Test loss: 2.598 | Test acc: 16.43% (1643/10000)\n",
            "\n",
            "Epoch: 10\n",
            "Train loss: 1.992 | Train acc: 23.00% (23/100)\n",
            "Test loss: 2.468 | Test acc: 16.99% (1699/10000)\n",
            "\n",
            "Epoch: 11\n",
            "Train loss: 1.955 | Train acc: 28.00% (28/100)\n",
            "Test loss: 3.258 | Test acc: 21.27% (2127/10000)\n",
            "\n",
            "Epoch: 12\n",
            "Train loss: 1.950 | Train acc: 32.00% (32/100)\n",
            "Test loss: 2.808 | Test acc: 19.30% (1930/10000)\n",
            "\n",
            "Epoch: 13\n",
            "Train loss: 2.106 | Train acc: 27.00% (27/100)\n",
            "Test loss: 2.657 | Test acc: 18.70% (1870/10000)\n",
            "\n",
            "Epoch: 14\n",
            "Train loss: 1.990 | Train acc: 27.00% (27/100)\n",
            "Test loss: 2.718 | Test acc: 15.22% (1522/10000)\n",
            "\n",
            "Epoch: 15\n",
            "Train loss: 2.021 | Train acc: 23.00% (23/100)\n",
            "Test loss: 2.390 | Test acc: 15.97% (1597/10000)\n",
            "\n",
            "Epoch: 16\n",
            "Train loss: 1.934 | Train acc: 36.00% (36/100)\n",
            "Test loss: 2.681 | Test acc: 22.19% (2219/10000)\n",
            "\n",
            "Epoch: 17\n",
            "Train loss: 1.801 | Train acc: 38.00% (38/100)\n",
            "Test loss: 2.390 | Test acc: 22.13% (2213/10000)\n",
            "\n",
            "Epoch: 18\n",
            "Train loss: 1.843 | Train acc: 30.00% (30/100)\n",
            "Test loss: 2.380 | Test acc: 22.63% (2263/10000)\n",
            "\n",
            "Epoch: 19\n",
            "Train loss: 1.871 | Train acc: 36.00% (36/100)\n",
            "Test loss: 2.400 | Test acc: 22.42% (2242/10000)\n",
            "\n",
            "Epoch: 20\n",
            "Train loss: 1.878 | Train acc: 36.00% (36/100)\n",
            "Test loss: 2.491 | Test acc: 21.49% (2149/10000)\n",
            "\n",
            "Epoch: 21\n",
            "Train loss: 1.793 | Train acc: 33.00% (33/100)\n",
            "Test loss: 2.798 | Test acc: 24.43% (2443/10000)\n",
            "\n",
            "Epoch: 22\n",
            "Train loss: 1.737 | Train acc: 34.00% (34/100)\n",
            "Test loss: 3.499 | Test acc: 22.13% (2213/10000)\n",
            "\n",
            "Epoch: 23\n",
            "Train loss: 1.740 | Train acc: 40.00% (40/100)\n",
            "Test loss: 2.996 | Test acc: 22.54% (2254/10000)\n",
            "\n",
            "Epoch: 24\n",
            "Train loss: 1.766 | Train acc: 34.00% (34/100)\n",
            "Test loss: 2.982 | Test acc: 20.60% (2060/10000)\n",
            "\n",
            "Epoch: 25\n",
            "Train loss: 1.834 | Train acc: 28.00% (28/100)\n",
            "Test loss: 3.512 | Test acc: 19.96% (1996/10000)\n",
            "\n",
            "Epoch: 26\n",
            "Train loss: 1.858 | Train acc: 35.00% (35/100)\n",
            "Test loss: 3.119 | Test acc: 20.71% (2071/10000)\n",
            "\n",
            "Epoch: 27\n",
            "Train loss: 1.670 | Train acc: 43.00% (43/100)\n",
            "Test loss: 2.463 | Test acc: 21.03% (2103/10000)\n",
            "\n",
            "Epoch: 28\n",
            "Train loss: 1.736 | Train acc: 35.00% (35/100)\n",
            "Test loss: 2.797 | Test acc: 23.05% (2305/10000)\n",
            "\n",
            "Epoch: 29\n",
            "Train loss: 1.729 | Train acc: 35.00% (35/100)\n",
            "Test loss: 2.835 | Test acc: 23.42% (2342/10000)\n",
            "\n",
            "Epoch: 30\n",
            "Train loss: 1.757 | Train acc: 40.00% (40/100)\n",
            "Test loss: 3.242 | Test acc: 21.60% (2160/10000)\n",
            "\n",
            "Epoch: 31\n",
            "Train loss: 1.719 | Train acc: 38.00% (38/100)\n",
            "Test loss: 2.628 | Test acc: 20.14% (2014/10000)\n",
            "\n",
            "Epoch: 32\n",
            "Train loss: 1.774 | Train acc: 36.00% (36/100)\n",
            "Test loss: 2.340 | Test acc: 22.12% (2212/10000)\n",
            "\n",
            "Epoch: 33\n",
            "Train loss: 1.744 | Train acc: 39.00% (39/100)\n",
            "Test loss: 2.626 | Test acc: 22.88% (2288/10000)\n",
            "\n",
            "Epoch: 34\n",
            "Train loss: 1.555 | Train acc: 47.00% (47/100)\n",
            "Test loss: 2.676 | Test acc: 21.48% (2148/10000)\n",
            "\n",
            "Epoch: 35\n",
            "Train loss: 1.690 | Train acc: 36.00% (36/100)\n",
            "Test loss: 2.813 | Test acc: 24.22% (2422/10000)\n",
            "\n",
            "Epoch: 36\n",
            "Train loss: 1.583 | Train acc: 38.00% (38/100)\n",
            "Test loss: 2.914 | Test acc: 22.07% (2207/10000)\n",
            "\n",
            "Epoch: 37\n",
            "Train loss: 1.502 | Train acc: 40.00% (40/100)\n",
            "Test loss: 2.556 | Test acc: 21.10% (2110/10000)\n",
            "\n",
            "Epoch: 38\n",
            "Train loss: 1.732 | Train acc: 35.00% (35/100)\n",
            "Test loss: 3.205 | Test acc: 22.10% (2210/10000)\n",
            "\n",
            "Epoch: 39\n",
            "Train loss: 1.556 | Train acc: 44.00% (44/100)\n",
            "Test loss: 2.786 | Test acc: 23.37% (2337/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWSKKTiLjDmC"
      },
      "source": [
        "<font color='red'>The data augmentation techniques that we used are the following (they are done not directly on the training set, but after taking a batch during the training): a random horizontal flip with probability $0.5$, a random crop of the same size of the image that we augmented with a padding of $4$, a random rotation of between -25 and 25 degrees, a gaussian noise with mean 0 and with standard deviation a random number taken uniformly between $0$ and $0.5$.</font>\n",
        "\n",
        "<font color='red'> After training the ResNet-18 architecture of the Question 3, we obtained the following result after $40$ epochs, with an Adam optimizer, a learning rate of $0.01$ and a batch size of $10$, on $X_{train}$ (which contains only 100 labeled data points):</font>\n",
        "\n",
        "| Model |  Number of  epochs | Train accuracy | Test accuracy |\n",
        "|-------|--------|-------------|--------------------------------------|\n",
        "|ResNet-18 trained on $X_{train}$ + data augmentation|$40$|$44\\%$ $(44/100)$|$23.37\\%$ $(2337/10000)$|\n",
        "\n",
        "<font color='red'>It is indeed a bit better than what we obtained without data augmentation.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyDGj_M4mapU"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vFQn2ESmapU"
      },
      "source": [
        "__Question 7 (5 points):__ Write a short report explaining the pros and the cons of each methods that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "dbIYBNuvntKY",
        "outputId": "115c1397-cb99-4495-84bf-dd099ec6144a"
      },
      "source": [
        "ResNet18_Xtrain = [9.01, 16.05, 15.43, 17.30, 17.05, 18.49, 16.49, 16.80, 18.31, 18.88, 17.72, 19.50, 18.70, 17.99, 17.90, 18.26, 20.58, 21.03, 18.37, 20.93, 21.48, 19.78, 20.60, 19.66, 21.65] + [21.65 for i in range(15)]\n",
        "\n",
        "ResNet18_pretrained = [12.07, 32.06, 40.73, 37.05, 42.27, 48.43, 44.04, 51.33, 41.06, 46.03, 46.91, 48.74, 49.17, 49.09, 48.02] + [48.02 for i in range(25)]\n",
        "\n",
        "ResNet18_Xtrain_augmented = [10, 15.10, 10.12, 14.53, 14.97, 16.42, 18.01, 18.19, 18.15, 16.43, 16.99, 21.27, 19.30, 18.70, 15.22, 15.97, 22.19, 22.13, 22.63, 22.42, 21.49, 24.43, 22.13, 22.54, 20.60, 19.96, 20.71, 21.03, 23.05, 23.42, 21.60, 20.14, 22.12, 22.88, 21.48, 24.22, 22.07, 21.10, 22.10, 23.37]\n",
        "\n",
        "plt.plot([i for i in range(1, 41)], ResNet18_Xtrain_augmented, label='ResNet18_Xtrain_augmented')\n",
        "plt.plot([i for i in range(1, 41)], ResNet18_Xtrain, label='ResNet18_Xtrain')\n",
        "plt.plot([i for i in range(1, 41)], ResNet18_pretrained, label='ResNet18_pretrained')\n",
        "plt.title('Performances of different pipelines over epochs')\n",
        "plt.ylabel('Test accuracy (in %)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3FXU5QdmapV"
      },
      "source": [
        "<font color='red'>Here is a summary of what we obtained with the different pipelines:</font>\n",
        "\n",
        "| Model |  Number of  epochs | Train accuracy | Test accuracy |\n",
        "|-------|--------|-------------|--------------------------------------|\n",
        "|ResNet-18 trained on $X_{train}$|$25$|$63\\%$ $(63/100)$|$21.65\\%$ $(2165/10000)$|\n",
        "|ResNet-18 trained on $X_{train}$ + data augmentation|$40$|$44\\%$ $(44/100)$|$23.37\\%$ $(2337/10000)$|\n",
        "|Pretrained ResNet-18 + fine-tuned last layer|$15$|$100\\%$ $(100/100)$|$48.02\\%$ $(4802/10000)$|\n",
        "\n",
        "<font color='red'>**Regarding the first method, which is training a model from scratch with the small dataset available:**</font>\n",
        "\n",
        "<font color='red'>The pros are that we do not have to have access to the weights of a pretrained model (some are publicly available, but some are not), and we don't have to design a data augmentation technique. This technique is the simplest to implement in practice.</font>\n",
        "\n",
        "<font color='red'>However, there are cons. First, since the dataset is really small, the network will learn perfectly or almost perfectly to label the training data after few epochs, and hence the gradient becomes smaller and smaller and the network stops to learn. You are really dependent on the small training data you have, and it can be bad if you have in this dataset a sort of outlier for a certain class that is over-represented.</font>\n",
        "\n",
        "<font color='red'>**Regarding the second method, which is training a model from scratch with the small dataset available and a data augmentation technique:**</font>\n",
        "\n",
        "<font color='red'>One advantage is that the network will be more robust against small transformations in an image. Studies found that adding a very small noise to an image because feeding it to a network can lead to a completely different predicted label, while the transformation is imperceptible for a human. Therefore, we would like to be robust against this to be more performant on new data. The network will also not directly overfit on the training examples, because since transformations are random, we typically have different images all the time during the training. Data augmentation can also be used to remove a bias in a dataset. For example, there is a law in the US that states that soap dispenser should be as accurate as detecting white hands as for any other hand color. Data augmentation is a practical way to adjust the dataset for minorities.</font>\n",
        "\n",
        "<font color='red'>A drawback is that you have to choose the right transformations for you dataset in your data augmentation techniques, or results can become worse. For example, if you rotate images randomly at 180 degrees, then the network will not be able to distinguish between a 6 and a 9. Another drawback is that the training will be typically longer because it will be harder for the network to overfit on the training data. The training is also likely to have more variance since a random transformation will be applied to each batch during the training, and the gradient computed at each step will of course depend on this random transformations.</font>\n",
        "\n",
        "<font color='red'>**Regarding the third method, which is fine-tuning the last layer of a model pretrained on another task:**</font>\n",
        "\n",
        "<font color='red'>One advantage is that the network has seen before (during the training of the original task) many more images than what we have in our dataset. Therefore, it is already able to detect features in an image like shapes, edges, colors, etc..., and all of this knowledge will be useful for our similar task. It is in practice really useful to use a pretrained model and performances are usually much higher. It is especially used either when we have a small training set, or when we don't have the computational power to train a full network.</font>\n",
        "\n",
        "<font color='red'>A drawback is that the deeper we go in the network, the more specific to the original task the features will be. Therefore, it might capture features that are really useful for the previous task but not for ours, and fail to capture features important for our task if we re-train only the last layer. Moreover, if we had access to a huge number of data for our task, and an important computational power, and the previous task is not exactly the same as ours, it would be better to re-train the full model from scratch to our task, since the pre-trained model is likely to end up quickly in a local minimum, and in the end performs worse than the model trained from scratch.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ0_iKnbmapV"
      },
      "source": [
        "# Weak supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6q2l2xTmapV"
      },
      "source": [
        "__Bonus \\[open\\] question (up to 3 points):__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW_c_X7iSWzL",
        "outputId": "92920f0c-4f87-4eeb-adc4-a7a681832b08"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform_weak_augm = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                     transforms.RandomCrop(32, padding=4),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform_strong_augm = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                     transforms.RandomCrop(32, padding=4),\n",
        "                                     transforms.RandomRotation(40),\n",
        "                                     #transforms.ColorJitter(brightness=(0.2, 2), contrast=(0.2, 2), saturation=(0.2, 2), hue=(-0.3, 0.3)),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "                                     GaussianNoise(2)])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "Xtrain_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size_small,\n",
        "                                          shuffle=False, num_workers=2,\n",
        "                                          sampler=torch.utils.data.sampler.SubsetRandomSampler([i for i in range(N_train)]))\n",
        "\n",
        "trainset_weak_augm = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                  download=True, transform=transform_weak_augm)\n",
        "X_loader = torch.utils.data.DataLoader(trainset_weak_augm, batch_size=batch_size_big,\n",
        "                                       shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "trainset_strong_augm = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                    download=True, transform=transform_strong_augm)\n",
        "X_loader_augm = torch.utils.data.DataLoader(trainset_strong_augm, batch_size=batch_size_big,\n",
        "                                            shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "Xtest_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size_big,\n",
        "                                           shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMFWGwSlpOtl"
      },
      "source": [
        "from itertools import cycle\n",
        "def training_SSL(net, trainloader_clean_labeled, trainloader_clean_unlabeled, trainloader_unclean_unlabeled, testloader, lambda_un, thr_select, n_epoch, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        print('\\nEpoch: %d' % epoch)\n",
        "        net.train()\n",
        "        supervised_train_loss = 0\n",
        "        unsupervised_train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for batch_idx, ((inputs, targets_supervised), (inputs_clean, targets), (inputs_unclean, _)) in enumerate(zip(cycle(trainloader_clean_labeled),\n",
        "                                                                                                                     trainloader_clean_unlabeled,\n",
        "                                                                                                                     trainloader_unclean_unlabeled)):\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs, targets_supervised = inputs.to(device), targets_supervised.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets_supervised)\n",
        "\n",
        "            inputs_clean, inputs_unclean, targets = inputs_clean.to(device), inputs_unclean.to(device), targets.to(device)\n",
        "            outputs_clean = net(inputs_clean)\n",
        "            outputs_unclean = net(inputs_unclean)\n",
        "\n",
        "            softmax = torch.softmax(outputs_clean, dim=1)\n",
        "            max_elements, max_indices = torch.max(softmax, dim=1)\n",
        "            indices = torch.squeeze((max_elements > thr_select).nonzero())\n",
        "            outputs_unclean = outputs_unclean[indices]\n",
        "            max_indices = max_indices[indices]\n",
        "\n",
        "            if max_indices.dim() > 0:\n",
        "                if not np.isnan((lambda_un * criterion(outputs_unclean, max_indices)).item()):\n",
        "                    supervised_train_loss += loss.item()\n",
        "                    unsupervised_train_loss += (lambda_un * criterion(outputs_unclean, max_indices)).item()\n",
        "                    loss += lambda_un * criterion(outputs_unclean, max_indices)\n",
        "\n",
        "            if not np.isnan(loss.item()):\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets_supervised.size(0)\n",
        "            correct += predicted.eq(targets_supervised).sum().item()\n",
        "\n",
        "        print('Supervised train loss: %.3f | Unsupervised train loss: %.3f | Train acc: %.2f%% (%d/%d)'\n",
        "              % (supervised_train_loss, unsupervised_train_loss, 100.*correct/total, correct, total))\n",
        "        \n",
        "        net.eval()\n",
        "        test_loss = 0\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total_test += targets.size(0)\n",
        "                correct_test += predicted.eq(targets).sum().item()\n",
        "\n",
        "        print('Test acc: %.2f%% (%d/%d)'\n",
        "              % (100.*correct_test/total_test, correct_test, total_test))\n",
        "        \n",
        "        scheduler.step()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj3aQbN-ZUPC",
        "outputId": "f90120fc-15d3-4681-8455-66b4ff032819"
      },
      "source": [
        "lambda_un = 1\n",
        "thr_select = 0.95\n",
        "learning_rate = 0.03\n",
        "n_epochs = 20\n",
        "\n",
        "resnet18 = ResNet18()\n",
        "resnet18 = resnet18.to(device)\n",
        "if device == 'cuda':\n",
        "    resnet18 = torch.nn.DataParallel(resnet18)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "training_SSL(resnet18, Xtrain_loader, X_loader, X_loader_augm, Xtest_loader, lambda_un, thr_select, n_epochs, learning_rate)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 0\n",
            "Supervised train loss: 457.100 | Unsupervised train loss: 142.338 | Train acc: 60.54% (2367/3910)\n",
            "Test acc: 21.12% (2112/10000)\n",
            "\n",
            "Epoch: 1\n",
            "Supervised train loss: 80.418 | Unsupervised train loss: 209.022 | Train acc: 93.66% (3662/3910)\n",
            "Test acc: 20.30% (2030/10000)\n",
            "\n",
            "Epoch: 2\n",
            "Supervised train loss: 47.937 | Unsupervised train loss: 184.787 | Train acc: 96.57% (3776/3910)\n",
            "Test acc: 19.72% (1972/10000)\n",
            "\n",
            "Epoch: 3\n",
            "Supervised train loss: 26.556 | Unsupervised train loss: 181.076 | Train acc: 98.08% (3835/3910)\n",
            "Test acc: 18.05% (1805/10000)\n",
            "\n",
            "Epoch: 4\n",
            "Supervised train loss: 18.602 | Unsupervised train loss: 173.867 | Train acc: 98.54% (3853/3910)\n",
            "Test acc: 18.53% (1853/10000)\n",
            "\n",
            "Epoch: 5\n",
            "Supervised train loss: 9.835 | Unsupervised train loss: 181.862 | Train acc: 99.34% (3884/3910)\n",
            "Test acc: 18.92% (1892/10000)\n",
            "\n",
            "Epoch: 6\n",
            "Supervised train loss: 7.805 | Unsupervised train loss: 177.228 | Train acc: 99.51% (3891/3910)\n",
            "Test acc: 19.35% (1935/10000)\n",
            "\n",
            "Epoch: 7\n",
            "Supervised train loss: 11.157 | Unsupervised train loss: 182.063 | Train acc: 99.18% (3878/3910)\n",
            "Test acc: 17.05% (1705/10000)\n",
            "\n",
            "Epoch: 8\n",
            "Supervised train loss: 11.561 | Unsupervised train loss: 191.334 | Train acc: 99.26% (3881/3910)\n",
            "Test acc: 22.53% (2253/10000)\n",
            "\n",
            "Epoch: 9\n",
            "Supervised train loss: 6.848 | Unsupervised train loss: 177.298 | Train acc: 99.59% (3894/3910)\n",
            "Test acc: 20.57% (2057/10000)\n",
            "\n",
            "Epoch: 10\n",
            "Supervised train loss: 4.862 | Unsupervised train loss: 178.810 | Train acc: 99.77% (3901/3910)\n",
            "Test acc: 22.84% (2284/10000)\n",
            "\n",
            "Epoch: 11\n",
            "Supervised train loss: 10.128 | Unsupervised train loss: 192.795 | Train acc: 99.44% (3888/3910)\n",
            "Test acc: 20.08% (2008/10000)\n",
            "\n",
            "Epoch: 12\n",
            "Supervised train loss: 6.670 | Unsupervised train loss: 192.691 | Train acc: 99.57% (3893/3910)\n",
            "Test acc: 21.14% (2114/10000)\n",
            "\n",
            "Epoch: 13\n",
            "Supervised train loss: 2.425 | Unsupervised train loss: 199.741 | Train acc: 99.85% (3904/3910)\n",
            "Test acc: 23.42% (2342/10000)\n",
            "\n",
            "Epoch: 14\n",
            "Supervised train loss: 11.590 | Unsupervised train loss: 198.410 | Train acc: 99.39% (3886/3910)\n",
            "Test acc: 22.05% (2205/10000)\n",
            "\n",
            "Epoch: 15\n",
            "Supervised train loss: 3.744 | Unsupervised train loss: 217.972 | Train acc: 99.74% (3900/3910)\n",
            "Test acc: 22.47% (2247/10000)\n",
            "\n",
            "Epoch: 16\n",
            "Supervised train loss: 3.455 | Unsupervised train loss: 219.051 | Train acc: 99.82% (3903/3910)\n",
            "Test acc: 23.72% (2372/10000)\n",
            "\n",
            "Epoch: 17\n",
            "Supervised train loss: 2.316 | Unsupervised train loss: 242.299 | Train acc: 99.85% (3904/3910)\n",
            "Test acc: 22.82% (2282/10000)\n",
            "\n",
            "Epoch: 18\n",
            "Supervised train loss: 1.359 | Unsupervised train loss: 232.450 | Train acc: 99.95% (3908/3910)\n",
            "Test acc: 20.04% (2004/10000)\n",
            "\n",
            "Epoch: 19\n",
            "Supervised train loss: 4.723 | Unsupervised train loss: 223.502 | Train acc: 99.69% (3898/3910)\n",
            "Test acc: 24.43% (2443/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7YLebI6DlKj"
      },
      "source": [
        "<font color='red'>In the previous cells, we tried to implement the [FixMatch](https://arxiv.org/pdf/2001.07685.pdf) algorithm, which is apparently the state of the art method for semi-supervised learning on CIFAR-10.</font>\n",
        "\n",
        "<font color='red'>In this algorithm, we design two sets of transforms, ones that are weak, and the others that are strong. The network is trained such that it classifies well the labeled data (supervised loss), and such that the predictions on strong augmented data are similar to predictions on weak augmented data when the network is confident on the weak ones (unsupervised loss).</font>\n",
        "\n",
        "<font color='red'>As stated in the paper, this heavily depends on the hyperparameters and the transformations we make. Here, we haven't put all the transformations used in the original paper (there are a lot) and we didn't have time to play a lot with the hyperparameters. We achieved a $24.43\\%$ accuracy on the test set for $20$ epochs, and it might go higher for more epochs. This is much less than what they obtained in the original paper (we don't know for how many epochs they trained it), and we think this is  mostly because of the transformation choices (for example, adding one wrong transformation can ruin completely the results). However, we still achieved an increase in the accuracy of the test set of about $3\\%$ compared to a fully supervised training.</font>"
      ]
    }
  ]
}